{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262a8cc4-c595-4c4f-82d5-8f5cda5b87e2",
   "metadata": {},
   "source": [
    "# Organising the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aaa012a-5525-4472-a944-df1bd2f11806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SERM_P1_WOD_1660_LeichPredigt.xml → 1650-1700/Sermons\n",
      "✓ SERM_P3_OOD_1792_Sonntagen.xml → 1750-1800/Sermons\n",
      "✓ LEGA_P1_WOD_1654_HoffgerichtsOrdnung.xml → 1650-1700/Legal\n",
      "✓ SCIE_P2_WOD_1741_Erden.xml → 1700-1750/Scientific\n",
      "✓ HUMA_P1_NoD_1667_Ratseburg.xml → 1650-1700/Humanities\n",
      "✓ LEGA_P3_NoD_1751_FeuerOrdnung.xml → 1750-1800/Legal\n",
      "✓ DRAM_P1_OOD_1675_Pirrus.xml → 1650-1700/Drama\n",
      "✓ SCIE_P1_OOD_1681_CometenGespoetts.xml → 1650-1700/Scientific\n",
      "✓ NARR_P1_OMD_1671_Ruebezahl.xml → 1650-1700/Narrative\n",
      "✓ HUMA_P1_WMD_1692_Christus.xml → 1650-1700/Humanities\n",
      "✓ HUMA_P3_WMD_1772_Baukunst.xml → 1750-1800/Humanities\n",
      "✓ SERM_P2_OMD_1715_Beerdigung.xml → 1700-1750/Sermons\n",
      "✓ SCIE_P3_NoD_1799_Gasarten.xml → 1750-1800/Scientific\n",
      "✓ SCIE_P2_WMD_1744_SelbstArtzt.xml → 1700-1750/Scientific\n",
      "✓ SERM_P3_OMD_1760_Folgen.xml → 1750-1800/Sermons\n",
      "✓ SERM_P3_WOD_1792_Hegel.xml → 1750-1800/Sermons\n",
      "✓ NEWS_P2_OOD_1702_muenchen2.xml → 1700-1750/Newspapers\n",
      "✓ DRAM_P2_WOD_1748_Hoelle.xml → 1700-1750/Drama\n",
      "✓ NARR_P2_NoD_1715_Africa.xml → 1700-1750/Narrative\n",
      "✓ SERM_P2_WMD_1702_Leben.xml → 1700-1750/Sermons\n",
      "✓ SERM_P1_OMD_1680_Balcken.xml → 1650-1700/Sermons\n",
      "✓ HUMA_P1_OOD_1680_MercksWienn.xml → 1650-1700/Humanities\n",
      "✓ HUMA_P1_NoD_1663_HaubtSprache.xml → 1650-1700/Humanities\n",
      "✓ SERM_P1_WMD_1674_Trost.xml → 1650-1700/Sermons\n",
      "✓ NEWS_P3_OMD_1769_erfurt.xml → 1750-1800/Newspapers\n",
      "✓ LEGA_P2_WOD_1738_Constantz.xml → 1700-1750/Legal\n",
      "✓ NEWS_P1_OOD_1684_muenchmerc.xml → 1650-1700/Newspapers\n",
      "✓ LEGA_P2_NoD_1707_Reglement.xml → 1700-1750/Legal\n",
      "✓ NARR_P3_OOD_1789_PeterProsch.xml → 1750-1800/Narrative\n",
      "✓ DRAM_P3_NoD_1764_Salomo.xml → 1750-1800/Drama\n",
      "✓ HUMA_P1_OMD_1654_Rosetum.xml → 1650-1700/Humanities\n",
      "✓ DRAM_P1_OMD_1657_Cardenio.xml → 1650-1700/Drama\n",
      "✓ DRAM_P3_OOD_1764_Maegera.xml → 1750-1800/Drama\n",
      "✓ DRAM_P1_WOD_1687_Joseph.xml → 1650-1700/Drama\n",
      "✓ NEWS_P2_NoD_1735_berlin.xml → 1700-1750/Newspapers\n",
      "✓ LEGA_P2_OOD_1704_OrdnungNuernberg.xml → 1700-1750/Legal\n",
      "✓ NEWS_P2_OOD_1702_muenchen1.xml → 1700-1750/Newspapers\n",
      "✓ SCIE_P3_WOD_1774_Hygrometrie.xml → 1750-1800/Scientific\n",
      "✓ SCIE_P1_NoD_1680_ConsiliumMedicum.xml → 1650-1700/Scientific\n",
      "✓ NARR_P1_NoD_1659_Herkules.xml → 1650-1700/Narrative\n",
      "✓ NARR_P2_OMD_1731_Seefahrer.xml → 1700-1750/Narrative\n",
      "✓ LEGA_P2_OMD_1709_WaeysenOrdnung.xml → 1700-1750/Legal\n",
      "✓ HUMA_P1_OMD_1685_ChristenStat.xml → 1650-1700/Humanities\n",
      "✓ DRAM_P3_NoD_1767_Minna.xml → 1750-1800/Drama\n",
      "✓ NEWS_P1_OMD_1666_leipzig2.xml → 1650-1700/Newspapers\n",
      "✓ LEGA_P3_OOD_1769_Theresiana.xml → 1750-1800/Legal\n",
      "✓ SCIE_P1_WMD_1676_ArtzneyBuch.xml → 1650-1700/Scientific\n",
      "✓ NARR_P1_WOD_1689_Miranten.xml → 1650-1700/Narrative\n",
      "✓ NEWS_P3_WOD_1784_freiburg2.xml → 1750-1800/Newspapers\n",
      "✓ HUMA_P3_WOD_1784_Weibs.xml → 1750-1800/Humanities\n",
      "✓ SERM_P1_WOD_1683_TrostPredigt.xml → 1650-1700/Sermons\n",
      "✓ SCIE_P2_OMD_1737_Medica.xml → 1700-1750/Scientific\n",
      "✓ LEGA_P2_WOD_1729_WechselRecht.xml → 1700-1750/Legal\n",
      "✓ DRAM_P1_WOD_1663_Carle.xml → 1650-1700/Drama\n",
      "✓ LEGA_P1_WOD_1698_LandsOrdnung.xml → 1650-1700/Legal\n",
      "✓ SERM_P3_OOD_1782_Erloeser.xml → 1750-1800/Sermons\n",
      "✓ LEGA_P3_WMD_1772_RegimentsVerfassung.xml → 1750-1800/Legal\n",
      "✓ HUMA_P2_NoD_1737_Koenigstein.xml → 1700-1750/Humanities\n",
      "✓ LEGA_P1_WOD_1683_Ulm.xml → 1650-1700/Legal\n",
      "✓ SERM_P3_NoD_1798_LetztePredigt.xml → 1750-1800/Sermons\n",
      "✓ SERM_P3_WOD_1751_DreiKoenig.xml → 1750-1800/Sermons\n",
      "✓ DRAM_P2_WMD_1745_Zuegellose.xml → 1700-1750/Drama\n",
      "✓ SCIE_P1_OOD_1689_PferdKunst.xml → 1650-1700/Scientific\n",
      "✓ NARR_P3_OOD_1796_Quintus.xml → 1750-1800/Narrative\n",
      "✓ NEWS_P1_OMD_1666_leipzig1.xml → 1650-1700/Newspapers\n",
      "✓ NARR_P2_WMD_1750_Teutsche.xml → 1700-1750/Narrative\n",
      "✓ DRAM_P2_OOD_1733_Ciro.xml → 1700-1750/Drama\n",
      "✓ NARR_P2_OOD_1734_TaendlMarckt.xml → 1700-1750/Narrative\n",
      "✓ HUMA_P1_WMD_1699_KetzerHistorie.xml → 1650-1700/Humanities\n",
      "✓ LEGA_P3_OOD_1750_HofRathsOrdnung.xml → 1750-1800/Legal\n",
      "✓ DRAM_P1_NoD_1673_Leonilda.xml → 1650-1700/Drama\n",
      "✓ NEWS_P3_WOD_1784_freiburg1.xml → 1750-1800/Newspapers\n",
      "✓ NARR_P3_WMD_1782_Fragmente.xml → 1750-1800/Narrative\n",
      "✓ NARR_P1_WMD_1664_Levante.xml → 1650-1700/Narrative\n",
      "✓ SERM_P3_OMD_1790_Unruhen.xml → 1750-1800/Sermons\n",
      "✓ SERM_P1_OOD_1663_Amaradvlcis.xml → 1650-1700/Sermons\n",
      "✓ NEWS_P1_WOD_1662_strassburg1.xml → 1650-1700/Newspapers\n",
      "✓ SCIE_P2_WOD_1720_FangSchlaeussen.xml → 1700-1750/Scientific\n",
      "✓ LEGA_P2_NoD_1724_StadtRecht.xml → 1700-1750/Legal\n",
      "✓ SCIE_P1_OMD_1700_BergBau.xml → 1650-1700/Scientific\n",
      "✓ SCIE_P1_OMD_1664_StraussStern.xml → 1650-1700/Scientific\n",
      "✓ SERM_P3_OMD_1756_Trost.xml → 1750-1800/Sermons\n",
      "✓ SCIE_P3_WOD_1787_Botanik.xml → 1750-1800/Scientific\n",
      "✓ SCIE_P2_NoD_1734_Barometer.xml → 1700-1750/Scientific\n",
      "✓ NARR_P2_WOD_1724_JungferRobinsone.xml → 1700-1750/Narrative\n",
      "✓ HUMA_P2_OMD_1725_Hass.xml → 1700-1750/Humanities\n",
      "✓ SERM_P2_WMD_1721_HeilBronnen.xml → 1700-1750/Sermons\n",
      "✓ LEGA_P2_WMD_1724_GesetzBuch.xml → 1700-1750/Legal\n",
      "✓ NEWS_P2_WMD_1701_hanau2.xml → 1700-1750/Newspapers\n",
      "✓ NEWS_P3_NoD_1786_wolfenbuettel1.xml → 1750-1800/Newspapers\n",
      "✓ DRAM_P2_WMD_1742_Bookesbeutel.xml → 1700-1750/Drama\n",
      "✓ SERM_P1_OMD_1680_SursumDeosum.xml → 1650-1700/Sermons\n",
      "✓ NEWS_P3_WMD_1784_mannheim.xml → 1750-1800/Newspapers\n",
      "✓ HUMA_P2_WOD_1744_Pfaltz.xml → 1700-1750/Humanities\n",
      "✓ NEWS_P1_WOD_1685_lindau.xml → 1650-1700/Newspapers\n",
      "✓ NEWS_P1_WOD_1662_strassburg2.xml → 1650-1700/Newspapers\n",
      "✓ SERM_P2_NoD_1715_Klugheit.xml → 1700-1750/Sermons\n",
      "✓ HUMA_P1_WOD_1698_Mythoscopia.xml → 1650-1700/Humanities\n",
      "✓ SERM_P2_WOD_1739_Kranckentrost.xml → 1700-1750/Sermons\n",
      "✓ HUMA_P1_OOD_1689_Crain.xml → 1650-1700/Humanities\n",
      "✓ DRAM_P1_WMD_1670_Comoedianten.xml → 1650-1700/Drama\n",
      "✓ NEWS_P3_WMD_1797_hanau.xml → 1750-1800/Newspapers\n",
      "✓ LEGA_P3_NoD_1757_Rostock.xml → 1750-1800/Legal\n",
      "✓ NARR_P1_NoD_1658_Morgenlaendisch.xml → 1650-1700/Narrative\n",
      "✓ DRAM_P2_OMD_1736_Fischbein.xml → 1700-1750/Drama\n",
      "✓ SERM_P2_NoD_1730_JubelFeste.xml → 1700-1750/Sermons\n",
      "✓ DRAM_P1_NoD_1700_Freyheit.xml → 1650-1700/Drama\n",
      "✓ NEWS_P2_OMD_1724_halle.xml → 1700-1750/Newspapers\n",
      "✓ NEWS_P2_WMD_1701_hanau1.xml → 1700-1750/Newspapers\n",
      "✓ LEGA_P2_OOD_1719_Privilegien.xml → 1700-1750/Legal\n",
      "✓ HUMA_P2_OOD_1707_HundertNarren.xml → 1700-1750/Humanities\n",
      "✓ DRAM_P3_WMD_1787_Verbrechen.xml → 1750-1800/Drama\n",
      "✓ NARR_P3_NoD_1786_Muenchhausen.xml → 1750-1800/Narrative\n",
      "✓ NEWS_P3_NoD_1786_wolfenbuettel2.xml → 1750-1800/Newspapers\n",
      "✓ SERM_P3_WOD_1790_Strassburg.xml → 1750-1800/Sermons\n",
      "✓ DRAM_P3_OMD_1788_Egmont.xml → 1750-1800/Drama\n",
      "✓ NARR_P3_WOD_1771_Usong.xml → 1750-1800/Narrative\n",
      "✓ LEGA_P1_OOD_1659_SchulOrdnung.xml → 1650-1700/Legal\n",
      "✓ SERM_P3_WMD_1774_Gewohnheit.xml → 1750-1800/Sermons\n",
      "✓ DRAM_P3_OMD_1775_LeidendeWeib.xml → 1750-1800/Drama\n",
      "✓ LEGA_P1_OOD_1657_Wildtfang.xml → 1650-1700/Legal\n",
      "✓ NARR_P3_WMD_1783_MoralischeErzaehlungen.xml → 1750-1800/Narrative\n",
      "✓ SCIE_P3_WMD_1777_Logik.xml → 1750-1800/Scientific\n",
      "✓ NARR_P1_WOD_1672_Melcher.xml → 1650-1700/Narrative\n",
      "✓ HUMA_P2_WMD_1737_Curiositaeten.xml → 1700-1750/Humanities\n",
      "✓ DRAM_P2_WOD_1702_Helvetia.xml → 1700-1750/Drama\n",
      "✓ DRAM_P2_OOD_1725_Venceslao.xml → 1700-1750/Drama\n",
      "✓ NARR_P2_WMD_1716_Fleurie.xml → 1700-1750/Narrative\n",
      "✓ NEWS_P2_OMD_1722_leipzig2.xml → 1700-1750/Newspapers\n",
      "✓ HUMA_P3_WOD_1795_Dichtung.xml → 1750-1800/Humanities\n",
      "✓ HUMA_P1_WOD_1686_Betrachtung.xml → 1650-1700/Humanities\n",
      "✓ NEWS_P1_NoD_1698_altona.xml → 1650-1700/Newspapers\n",
      "✓ NEWS_P2_NoD_1740_berlin1.xml → 1700-1750/Newspapers\n",
      "✓ NARR_P1_WMD_1696_Schelmuffsky.xml → 1650-1700/Narrative\n",
      "✓ NEWS_P3_WOD_1798_tuebingen.xml → 1750-1800/Newspapers\n",
      "✓ NEWS_P2_WMD_1750_frankfurt.xml → 1700-1750/Newspapers\n",
      "✓ SCIE_P3_OOD_1786_Polizey.xml → 1750-1800/Scientific\n",
      "✓ DRAM_P2_NoD_1707_SchaeferSpiel.xml → 1700-1750/Drama\n",
      "✓ LEGA_P3_OMD_1767_ProcessOrdnung.xml → 1750-1800/Legal\n",
      "✓ HUMA_P1_OOD_1690_Proteus.xml → 1650-1700/Humanities\n",
      "✓ NEWS_P1_OMD_1683_breslau.xml → 1650-1700/Newspapers\n",
      "✓ DRAM_P1_OMD_1661_Cleopatra.xml → 1650-1700/Drama\n",
      "✓ SERM_P1_NoD_1677_LeichSermon.xml → 1650-1700/Sermons\n",
      "✓ HUMA_P3_NoD_1762_Kreuzzuege.xml → 1750-1800/Humanities\n",
      "✓ NEWS_P2_NoD_1702_hamburg.xml → 1700-1750/Newspapers\n",
      "✓ DRAM_P1_WMD_1662_Tomyris.xml → 1650-1700/Drama\n",
      "✓ LEGA_P3_OMD_1777_MuehlenOrdnung.xml → 1750-1800/Legal\n",
      "✓ SERM_P1_OMD_1672_Advent.xml → 1650-1700/Sermons\n",
      "✓ DRAM_P3_WOD_1762_Evander.xml → 1750-1800/Drama\n",
      "✓ SCIE_P1_WOD_1693_Kranckheit.xml → 1650-1700/Scientific\n",
      "✓ LEGA_P1_NoD_1657_Luebeck.xml → 1650-1700/Legal\n",
      "✓ NEWS_P2_OMD_1722_leipzig1.xml → 1700-1750/Newspapers\n",
      "✓ SCIE_P3_WMD_1781_Forstwissenschaft.xml → 1750-1800/Scientific\n",
      "✓ NEWS_P2_NoD_1740_berlin2.xml → 1700-1750/Newspapers\n",
      "✓ DRAM_P1_OOD_1682_Abraham.xml → 1650-1700/Drama\n",
      "✓ SERM_P2_OMD_1734_Evangelisch.xml → 1700-1750/Sermons\n",
      "✓ LEGA_P3_WMD_1756_StaatsArchiv.xml → 1750-1800/Legal\n",
      "✓ NEWS_P3_WMD_1793_mainz.xml → 1750-1800/Newspapers\n",
      "✓ SCIE_P3_WOD_1780_Instrument.xml → 1750-1800/Scientific\n",
      "✓ SERM_P2_NoD_1715_Seeligkeit.xml → 1700-1750/Sermons\n",
      "✓ HUMA_P1_WOD_1662_Musurgia.xml → 1650-1700/Humanities\n",
      "✓ NARR_P3_WOD_1766_Agathon.xml → 1750-1800/Narrative\n",
      "✓ NARR_P1_OMD_1689_Arminius.xml → 1650-1700/Narrative\n",
      "✓ NARR_P1_OOD_1669_Aramena.xml → 1650-1700/Narrative\n",
      "✓ NEWS_P1_OOD_1679_nuernberg1.xml → 1650-1700/Newspapers\n",
      "✓ SERM_P3_WMD_1780_LottoSucht.xml → 1750-1800/Sermons\n",
      "✓ SERM_P3_OOD_1751_Elisabetha.xml → 1750-1800/Sermons\n",
      "✓ SERM_P3_NoD_1765_Trauerrede.xml → 1750-1800/Sermons\n",
      "✓ DRAM_P2_NoD_1711_Croesus.xml → 1700-1750/Drama\n",
      "✓ NEWS_P1_OMD_1684_breslau.xml → 1650-1700/Newspapers\n",
      "✓ SCIE_P3_NoD_1775_Chemie.xml → 1750-1800/Scientific\n",
      "✓ DRAM_P1_OMD_1683_Masaniello.xml → 1650-1700/Drama\n",
      "✓ SCIE_P1_WMD_1687_ArtzneyKunst.xml → 1650-1700/Scientific\n",
      "✓ SCIE_P2_NoD_1736_Anweisung.xml → 1700-1750/Scientific\n",
      "✓ SERM_P1_WMD_1699_Solms.xml → 1650-1700/Sermons\n",
      "✓ SCIE_P2_OOD_1745_Mathematicus.xml → 1700-1750/Scientific\n",
      "✓ LEGA_P3_WOD_1791_Muenze.xml → 1750-1800/Legal\n",
      "✓ NARR_P3_NoD_1796_Siebenkaes.xml → 1750-1800/Narrative\n",
      "✓ HUMA_P1_WMD_1674_BilderSchatz.xml → 1650-1700/Humanities\n",
      "✓ SCIE_P2_WMD_1714_Kleinod.xml → 1700-1750/Scientific\n",
      "✓ HUMA_P2_OOD_1704_WasserKunst.xml → 1700-1750/Humanities\n",
      "✓ SCIE_P1_WOD_1663_KunstSpiegel.xml → 1650-1700/Scientific\n",
      "✓ LEGA_P3_OMD_1784_Erbstatuten.xml → 1750-1800/Legal\n",
      "✓ SCIE_P1_OMD_1672_Handwercke.xml → 1650-1700/Scientific\n",
      "✓ LEGA_P3_NoD_1796_Landtage.xml → 1750-1800/Legal\n",
      "✓ NARR_P2_OMD_1738_LebensBeschreibung.xml → 1700-1750/Narrative\n",
      "✓ NARR_P3_OMD_1776_Zerbin.xml → 1750-1800/Narrative\n",
      "✓ DRAM_P3_OMD_1774_Hofmeister.xml → 1750-1800/Drama\n",
      "✓ NEWS_P1_OOD_1679_nuernberg2.xml → 1650-1700/Newspapers\n",
      "✓ SCIE_P2_WMD_1702_Armuth.xml → 1700-1750/Scientific\n",
      "✓ SCIE_P3_OMD_1781_Chymie.xml → 1750-1800/Scientific\n",
      "✓ HUMA_P1_NoD_1674_NaturalienKammer.xml → 1650-1700/Humanities\n",
      "✓ NARR_P1_OOD_1667_Simplicissimus.xml → 1650-1700/Narrative\n",
      "✓ SCIE_P3_OOD_1780_ZeichenInstruments.xml → 1750-1800/Scientific\n",
      "✓ LEGA_P2_WOD_1711_HalsGericht.xml → 1700-1750/Legal\n",
      "✓ SERM_P2_OOD_1709_Orgel.xml → 1700-1750/Sermons\n",
      "✓ SCIE_P2_OOD_1705_WerckSchul.xml → 1700-1750/Scientific\n",
      "✓ DRAM_P3_NoD_1776_Zwillinge.xml → 1750-1800/Drama\n",
      "✓ LEGA_P2_WMD_1733_Heyrathen.xml → 1700-1750/Legal\n",
      "✓ NEWS_P3_OOD_1780_wien.xml → 1750-1800/Newspapers\n",
      "✓ LEGA_P1_NoD_1673_BergOrdnung.xml → 1650-1700/Legal\n",
      "✓ DRAM_P1_WMD_1668_ChristRuehmendes.xml → 1650-1700/Drama\n",
      "✓ LEGA_P2_OMD_1723_JurisMilitaris.xml → 1700-1750/Legal\n",
      "✓ DRAM_P3_OOD_1798_Donauweibchen.xml → 1750-1800/Drama\n",
      "✓ NEWS_P3_WOD_1781_heilbronn.xml → 1750-1800/Newspapers\n",
      "✓ SCIE_P2_WOD_1708_WunderbarenWelt.xml → 1700-1750/Scientific\n",
      "✓ NEWS_P2_WOD_1722_zuerich.xml → 1700-1750/Newspapers\n",
      "✓ NEWS_P3_OOD_1791_bayreuth.xml → 1750-1800/Newspapers\n",
      "✓ NARR_P2_OOD_1703_Narrennest.xml → 1700-1750/Narrative\n",
      "✓ HUMA_P3_WMD_1789_Italien.xml → 1750-1800/Humanities\n",
      "✓ HUMA_P2_OOD_1731_AntiquitaetenSchatz.xml → 1700-1750/Humanities\n",
      "✓ SERM_P1_NoD_1666_Erbteil.xml → 1650-1700/Sermons\n",
      "✓ SERM_P1_WOD_1654_Eytelkeit.xml → 1650-1700/Sermons\n",
      "✓ NEWS_P3_OMD_1789_gotha.xml → 1750-1800/Newspapers\n",
      "✓ NARR_P2_WMD_1742_RedlicheMann.xml → 1700-1750/Narrative\n",
      "✓ SERM_P2_WOD_1730_SeelenLiecht.xml → 1700-1750/Sermons\n",
      "✓ LEGA_P2_WMD_1720_VatterMord.xml → 1700-1750/Legal\n",
      "✓ NEWS_P1_OOD_1659_muenchen2.xml → 1650-1700/Newspapers\n",
      "✓ DRAM_P3_WMD_1780_Hausvater.xml → 1750-1800/Drama\n",
      "✓ NEWS_P1_OOD_1659_muenchmerc.xml → 1650-1700/Newspapers\n",
      "✓ NEWS_P3_OOD_1790_erlangen.xml → 1750-1800/Newspapers\n",
      "✓ HUMA_P1_OMD_1680_Bericht.xml → 1650-1700/Humanities\n",
      "✓ LEGA_P2_NoD_1700_Braunschweig.xml → 1700-1750/Legal\n",
      "✓ HUMA_P3_OOD_1792_Alterthuemer.xml → 1750-1800/Humanities\n",
      "✓ SCIE_P1_WMD_1680_Epidemica.xml → 1650-1700/Scientific\n",
      "✓ NEWS_P1_WMD_1662_koeln.xml → 1650-1700/Newspapers\n",
      "✓ LEGA_P1_WMD_1698_BergkRecht.xml → 1650-1700/Legal\n",
      "✓ NARR_P3_OOD_1787_Aglais.xml → 1750-1800/Narrative\n",
      "✓ NARR_P3_OMD_1782_Volksmaerchen.xml → 1750-1800/Narrative\n",
      "✓ HUMA_P3_NoD_1772_Ursprung.xml → 1750-1800/Humanities\n",
      "✓ LEGA_P1_OOD_1700_GesetzNuernberg.xml → 1650-1700/Legal\n",
      "✓ SCIE_P2_NoD_1744_Cometen.xml → 1700-1750/Scientific\n",
      "✓ NARR_P2_OOD_1715_HeldenGeschichte.xml → 1700-1750/Narrative\n",
      "✓ NEWS_P1_WOD_1681_zuerich.xml → 1650-1700/Newspapers\n",
      "✓ SERM_P2_OOD_1700_FeyerTag.xml → 1700-1750/Sermons\n",
      "✓ HUMA_P3_OOD_1774_Emil.xml → 1750-1800/Humanities\n",
      "✓ NEWS_P2_OOD_1712_wien.xml → 1700-1750/Newspapers\n",
      "✓ NEWS_P2_OOD_1713_wien.xml → 1700-1750/Newspapers\n",
      "✓ NARR_P2_WOD_1744_Fabeln.xml → 1700-1750/Narrative\n",
      "✓ NEWS_P3_OMD_1784_gotha.xml → 1750-1800/Newspapers\n",
      "✓ LEGA_P3_OOD_1752_Gerichtbarkeit.xml → 1750-1800/Legal\n",
      "✓ NARR_P3_OMD_1774_Werther.xml → 1750-1800/Narrative\n",
      "✓ DRAM_P1_OOD_1682_LiebesSig.xml → 1650-1700/Drama\n",
      "✓ LEGA_P1_OMD_1680_Dreszden.xml → 1650-1700/Legal\n",
      "✓ NEWS_P1_OOD_1659_muenchen1.xml → 1650-1700/Newspapers\n",
      "✓ NEWS_P3_NoD_1796_stettin.xml → 1750-1800/Newspapers\n",
      "✓ SCIE_P2_OOD_1722_NordScheines.xml → 1700-1750/Scientific\n",
      "✓ DRAM_P2_OOD_1749_Schaeferinsel.xml → 1700-1750/Drama\n",
      "✓ LEGA_P3_WOD_1769_ZunftOrdnungen.xml → 1750-1800/Legal\n",
      "✓ HUMA_P3_NoD_1788_Menschen.xml → 1750-1800/Humanities\n",
      "✓ NEWS_P3_OMD_1790_gotha.xml → 1750-1800/Newspapers\n",
      "✓ SCIE_P1_WOD_1665_Cometen.xml → 1650-1700/Scientific\n",
      "✓ NEWS_P1_NoD_1666_berlin2.xml → 1650-1700/Newspapers\n",
      "✓ NARR_P2_NoD_1706_SatyrischerRoman.xml → 1700-1750/Narrative\n",
      "✓ DRAM_P1_WOD_1699_LiebesStreit.xml → 1650-1700/Drama\n",
      "✓ SCIE_P3_OMD_1781_Akademie.xml → 1750-1800/Scientific\n",
      "✓ NARR_P3_WOD_1797_Hyperion.xml → 1750-1800/Narrative\n",
      "✓ HUMA_P2_WMD_1739_Stollberg.xml → 1700-1750/Humanities\n",
      "✓ NARR_P1_WMD_1696_DerEdelmann.xml → 1650-1700/Narrative\n",
      "✓ SCIE_P1_NoD_1684_Durchfall.xml → 1650-1700/Scientific\n",
      "✓ SCIE_P3_OMD_1778_MineralogischeGeographie.xml → 1750-1800/Scientific\n",
      "✓ SCIE_P2_OMD_1717_Materialist.xml → 1700-1750/Scientific\n",
      "✓ SERM_P1_OOD_1686_KlagseufftzendesAch.xml → 1650-1700/Sermons\n",
      "✓ HUMA_P2_NoD_1720_Remarques.xml → 1700-1750/Humanities\n",
      "✓ NEWS_P1_WMD_1671_frankfurt1.xml → 1650-1700/Newspapers\n",
      "✓ HUMA_P2_WOD_1740_Poesie.xml → 1700-1750/Humanities\n",
      "✓ SERM_P2_WOD_1708_Zotten.xml → 1700-1750/Sermons\n",
      "✓ NEWS_P3_NoD_1798_danzig.xml → 1750-1800/Newspapers\n",
      "✓ NEWS_P1_NoD_1666_berlin1.xml → 1650-1700/Newspapers\n",
      "✓ LEGA_P3_WMD_1792_Reichshofrath.xml → 1750-1800/Legal\n",
      "✓ SERM_P1_NoD_1690_WilleGottes.xml → 1650-1700/Sermons\n",
      "✓ SERM_P1_WMD_1662_Funeralia.xml → 1650-1700/Sermons\n",
      "✓ SCIE_P1_NoD_1672_Prognosticis.xml → 1650-1700/Scientific\n",
      "✓ NARR_P1_NoD_1682_Mandorell.xml → 1650-1700/Narrative\n",
      "✓ SERM_P2_WMD_1743_TrostPredigt.xml → 1700-1750/Sermons\n",
      "✓ DRAM_P2_WMD_1743_DieGeistlichen.xml → 1700-1750/Drama\n",
      "✓ SERM_P3_NoD_1770_Gottesdienst.xml → 1750-1800/Sermons\n",
      "✓ HUMA_P2_WMD_1748_Samuel.xml → 1700-1750/Humanities\n",
      "✓ SERM_P2_OOD_1728_Verstellte.xml → 1700-1750/Sermons\n",
      "✓ NEWS_P2_OOD_1744_graz.xml → 1700-1750/Newspapers\n",
      "✓ NEWS_P1_WMD_1699_koeln.xml → 1650-1700/Newspapers\n",
      "✓ LEGA_P1_WMD_1700_LandRecht.xml → 1650-1700/Legal\n",
      "✓ DRAM_P3_WMD_1773_Goetz.xml → 1750-1800/Drama\n",
      "✓ NEWS_P1_NoD_1673_hamburg.xml → 1650-1700/Newspapers\n",
      "✓ SCIE_P1_OOD_1665_Feldmessen.xml → 1650-1700/Scientific\n",
      "✓ NARR_P2_NoD_1709_OstIndien.xml → 1700-1750/Narrative\n",
      "✓ HUMA_P3_WMD_1777_Homburg.xml → 1750-1800/Humanities\n",
      "✓ DRAM_P2_OMD_1732_Cato.xml → 1700-1750/Drama\n",
      "✓ NARR_P1_WOD_1682_Feuermaeuer.xml → 1650-1700/Narrative\n",
      "✓ NEWS_P1_WMD_1671_frankfurt2.xml → 1650-1700/Newspapers\n",
      "✓ SERM_P3_WMD_1780_Feuersbrunst.xml → 1750-1800/Sermons\n",
      "✓ SERM_P1_OOD_1660_EinweihungsPredigt.xml → 1650-1700/Sermons\n",
      "✓ LEGA_P1_WMD_1694_RathsSatzung.xml → 1650-1700/Legal\n",
      "✓ LEGA_P2_OMD_1710_ReichsArchiv.xml → 1700-1750/Legal\n",
      "✓ HUMA_P2_WOD_1741_Antiquitaeten.xml → 1700-1750/Humanities\n",
      "✓ NARR_P3_NoD_1790_AntonReiser.xml → 1750-1800/Narrative\n",
      "✓ DRAM_P2_OMD_1747_Schwestern.xml → 1700-1750/Drama\n",
      "✓ NARR_P1_OMD_1700_Banise.xml → 1650-1700/Narrative\n",
      "✓ NEWS_P2_OMD_1744_erfurt2.xml → 1700-1750/Newspapers\n",
      "✓ NEWS_P2_WMD_1701_frankfurt.xml → 1700-1750/Newspapers\n",
      "✓ DRAM_P3_WOD_1781_Raeuber.xml → 1750-1800/Drama\n",
      "✓ NEWS_P1_WOD_1689_lindau.xml → 1650-1700/Newspapers\n",
      "✓ SCIE_P2_OMD_1731_HarnRuhr.xml → 1700-1750/Scientific\n",
      "✓ HUMA_P2_NoD_1739_MusicalischInterval.xml → 1700-1750/Humanities\n",
      "✓ LEGA_P1_NoD_1695_Duesseldorff.xml → 1650-1700/Legal\n",
      "✓ HUMA_P3_OMD_1777_Dichtkunst.xml → 1750-1800/Humanities\n",
      "✓ DRAM_P2_WOD_1737_Verehrung.xml → 1700-1750/Drama\n",
      "✓ LEGA_P3_WOD_1787_Cameralrecht.xml → 1750-1800/Legal\n",
      "✓ NEWS_P2_WOD_1749_loerrach2.xml → 1700-1750/Newspapers\n",
      "✓ DRAM_P1_NoD_1699_Euridice.xml → 1650-1700/Drama\n",
      "✓ NARR_P2_WOD_1746_Muetze.xml → 1700-1750/Narrative\n",
      "✓ NEWS_P1_OMD_1687_leipzig.xml → 1650-1700/Newspapers\n",
      "✓ HUMA_P3_OMD_1798_Sondershausen.xml → 1750-1800/Humanities\n",
      "✓ HUMA_P3_WOD_1768_Roemer.xml → 1750-1800/Humanities\n",
      "✓ HUMA_P3_OOD_1778_Pons.xml → 1750-1800/Humanities\n",
      "✓ NEWS_P2_WOD_1723_augsburg1.xml → 1700-1750/Newspapers\n",
      "✓ NARR_P1_OOD_1682_Winternaechte.xml → 1650-1700/Narrative\n",
      "✓ SCIE_P3_OOD_1788_Chimie.xml → 1750-1800/Scientific\n",
      "✓ NEWS_P2_OMD_1744_erfurt1.xml → 1700-1750/Newspapers\n",
      "✓ DRAM_P3_OOD_1782_Serail.xml → 1750-1800/Drama\n",
      "✓ HUMA_P2_OMD_1717_DienstMaegde.xml → 1700-1750/Humanities\n",
      "✓ LEGA_P1_OMD_1659_Hexen.xml → 1650-1700/Legal\n",
      "✓ DRAM_P3_WOD_1783_Elfride.xml → 1750-1800/Drama\n",
      "✓ LEGA_P1_OMD_1674_BergOrdnung.xml → 1650-1700/Legal\n",
      "✓ NEWS_P1_WMD_1663_koeln.xml → 1650-1700/Newspapers\n",
      "✓ SCIE_P3_NoD_1761_Menschlich.xml → 1750-1800/Scientific\n",
      "✓ LEGA_P2_OOD_1709_Promptuarii.xml → 1700-1750/Legal\n",
      "✓ NEWS_P2_WOD_1749_loerrach1.xml → 1700-1750/Newspapers\n",
      "✓ NARR_P2_OMD_1708_Affecten.xml → 1700-1750/Narrative\n",
      "✓ SCIE_P3_WMD_1753_ProbierSteins.xml → 1750-1800/Scientific\n",
      "✓ HUMA_P3_OMD_1774_Roman.xml → 1750-1800/Humanities\n",
      "✓ NARR_P3_WMD_1775_Bacchidon.xml → 1750-1800/Narrative\n",
      "✓ DRAM_P2_NoD_1749_AlteJungfer.xml → 1700-1750/Drama\n",
      "✓ SERM_P2_OMD_1706_GedaechtnisPredigt.xml → 1700-1750/Sermons\n",
      "✓ NEWS_P2_WOD_1723_augsburg2.xml → 1700-1750/Newspapers\n",
      "✓ HUMA_P2_OMD_1729_Biedermann.xml → 1700-1750/Humanities\n",
      "\n",
      "📊 Report saved to: /Users/rohan/Downloads/2544/organized_germanc/organization_report.txt\n",
      "\n",
      "🎉 Organization complete!\n",
      "📁 Organized 336 files\n",
      "❌ 0 errors\n",
      "📊 Check organization_report.txt for details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "class GerManCOrganizer:\n",
    "    def __init__(self, source_dir, output_dir):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        \n",
    "        # Genre mapping from filename prefixes\n",
    "        self.genres = {\n",
    "            'DRAM': 'Drama',\n",
    "            'HUMA': 'Humanities', \n",
    "            'LEGA': 'Legal',\n",
    "            'NARR': 'Narrative',\n",
    "            'NEWS': 'Newspapers',\n",
    "            'SCIE': 'Scientific',\n",
    "            'SERM': 'Sermons'\n",
    "        }\n",
    "        \n",
    "        # Time period bins\n",
    "        self.periods = {\n",
    "            'P1': '1650-1700',\n",
    "            'P2': '1700-1750', \n",
    "            'P3': '1750-1800'\n",
    "        }\n",
    "        \n",
    "    def extract_file_info(self, filename):\n",
    "        \"\"\"Extract genre, period, and year from filename\"\"\"\n",
    "        # Pattern: GENRE_PERIOD_REGION_YEAR_TITLE.xml\n",
    "        # Example: DRAM_P1_NoD_1673_Leonilda.xml\n",
    "        \n",
    "        pattern = r'([A-Z]{4})_([P][1-3])_([A-Za-z]+)_(\\d{4})_(.+)\\.xml'\n",
    "        match = re.match(pattern, filename)\n",
    "        \n",
    "        if not match:\n",
    "            return None\n",
    "            \n",
    "        genre_code, period_code, region, year, title = match.groups()\n",
    "        \n",
    "        return {\n",
    "            'genre': self.genres.get(genre_code, 'Unknown'),\n",
    "            'genre_code': genre_code,\n",
    "            'period': self.periods.get(period_code, 'Unknown'),\n",
    "            'period_code': period_code,\n",
    "            'region': region,\n",
    "            'year': int(year),\n",
    "            'title': title,\n",
    "            'filename': filename\n",
    "        }\n",
    "    \n",
    "    def organize_files(self):\n",
    "        \"\"\"Main function to organize all LING-GATE files\"\"\"\n",
    "        \n",
    "        # Create output directory structure\n",
    "        self.create_directory_structure()\n",
    "        \n",
    "        # Stats tracking\n",
    "        stats = defaultdict(lambda: defaultdict(int))\n",
    "        processed_files = []\n",
    "        error_files = []\n",
    "        \n",
    "        # Process each XML file\n",
    "        for xml_file in self.source_dir.glob(\"*.xml\"):\n",
    "            try:\n",
    "                file_info = self.extract_file_info(xml_file.name)\n",
    "                \n",
    "                if file_info:\n",
    "                    # Create destination path\n",
    "                    dest_dir = self.output_dir / file_info['period'] / file_info['genre']\n",
    "                    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    # Copy file to organized location\n",
    "                    dest_path = dest_dir / xml_file.name\n",
    "                    shutil.copy2(xml_file, dest_path)\n",
    "                    \n",
    "                    # Update stats\n",
    "                    stats[file_info['period']][file_info['genre']] += 1\n",
    "                    processed_files.append(file_info)\n",
    "                    \n",
    "                    print(f\"✓ {xml_file.name} → {file_info['period']}/{file_info['genre']}\")\n",
    "                    \n",
    "                else:\n",
    "                    error_files.append(xml_file.name)\n",
    "                    print(f\"✗ Could not parse: {xml_file.name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_files.append(xml_file.name)\n",
    "                print(f\"✗ Error processing {xml_file.name}: {e}\")\n",
    "        \n",
    "        # Generate summary report\n",
    "        self.generate_report(stats, processed_files, error_files)\n",
    "        \n",
    "        return stats, processed_files, error_files\n",
    "    \n",
    "    def create_directory_structure(self):\n",
    "        \"\"\"Create organized directory structure\"\"\"\n",
    "        for period in self.periods.values():\n",
    "            for genre in self.genres.values():\n",
    "                dir_path = self.output_dir / period / genre\n",
    "                dir_path.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "    def generate_report(self, stats, processed_files, error_files):\n",
    "        \"\"\"Generate organization summary report\"\"\"\n",
    "        \n",
    "        report_path = self.output_dir / \"organization_report.txt\"\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"GerManC Corpus Organization Report\\n\")\n",
    "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            f.write(f\"Total files processed: {len(processed_files)}\\n\")\n",
    "            f.write(f\"Total files with errors: {len(error_files)}\\n\\n\")\n",
    "            \n",
    "            # Files by period and genre\n",
    "            f.write(\"Files by Period and Genre:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            for period in sorted(stats.keys()):\n",
    "                f.write(f\"\\n{period}:\\n\")\n",
    "                for genre in sorted(stats[period].keys()):\n",
    "                    count = stats[period][genre]\n",
    "                    f.write(f\"  {genre}: {count} files\\n\")\n",
    "            \n",
    "            # Year distribution\n",
    "            f.write(\"\\n\\nYear Distribution:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            years = [info['year'] for info in processed_files]\n",
    "            if years:\n",
    "                f.write(f\"Earliest: {min(years)}\\n\")\n",
    "                f.write(f\"Latest: {max(years)}\\n\")\n",
    "                f.write(f\"Range: {max(years) - min(years)} years\\n\")\n",
    "            \n",
    "            # Error files\n",
    "            if error_files:\n",
    "                f.write(\"\\n\\nFiles with errors:\\n\")\n",
    "                f.write(\"-\" * 20 + \"\\n\")\n",
    "                for error_file in error_files:\n",
    "                    f.write(f\"  {error_file}\\n\")\n",
    "        \n",
    "        print(f\"\\n📊 Report saved to: {report_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage\"\"\"\n",
    "    \n",
    "    # Set your paths here\n",
    "    source_directory = \"/Users/rohan/Downloads/2544/LING-GATE/\"  # Where your XML files are\n",
    "    output_directory = \"/Users/rohan/Downloads/2544/organized_germanc\"                # Where to create organized structure\n",
    "    \n",
    "    # Create organizer and run\n",
    "    organizer = GerManCOrganizer(source_directory, output_directory)\n",
    "    stats, processed, errors = organizer.organize_files()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n🎉 Organization complete!\")\n",
    "    print(f\"📁 Organized {len(processed)} files\")\n",
    "    print(f\"❌ {len(errors)} errors\")\n",
    "    print(f\"📊 Check organization_report.txt for details\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2b13a-0b0c-4f60-ac2d-03f0e3d6993d",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c64419-3ec4-40ad-bdd5-d81699cfc4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GATE XML files:   0%| | 0/336 [00:00<?, ?file/s, file=HUMA_P2_NoD_1732025-06-15 23:50:24,998 - INFO - ✓ Processed: HUMA_P2_NoD_1737_Koenigstein.xml (2339 tokens)\n",
      "Processing GATE XML files:   0%| | 1/336 [00:00<01:54,  2.92file/s, file=HUMA_P22025-06-15 23:50:25,333 - INFO - ✓ Processed: HUMA_P2_OMD_1725_Hass.xml (2305 tokens)\n",
      "Processing GATE XML files:   1%| | 2/336 [00:00<01:52,  2.96file/s, file=HUMA_P22025-06-15 23:50:25,742 - INFO - ✓ Processed: HUMA_P2_WOD_1744_Pfaltz.xml (2331 tokens)\n",
      "Processing GATE XML files:   1%| | 3/336 [00:01<02:03,  2.70file/s, file=HUMA_P22025-06-15 23:50:26,233 - INFO - ✓ Processed: HUMA_P2_OOD_1707_HundertNarren.xml (2483 tokens)\n",
      "Processing GATE XML files:   1%| | 4/336 [00:01<02:18,  2.39file/s, file=HUMA_P22025-06-15 23:50:26,626 - INFO - ✓ Processed: HUMA_P2_WMD_1737_Curiositaeten.xml (2449 tokens)\n",
      "Processing GATE XML files:   1%| | 5/336 [00:01<02:15,  2.44file/s, file=HUMA_P22025-06-15 23:50:26,989 - INFO - ✓ Processed: HUMA_P2_OOD_1704_WasserKunst.xml (2355 tokens)\n",
      "Processing GATE XML files:   2%| | 6/336 [00:02<02:09,  2.54file/s, file=HUMA_P22025-06-15 23:50:27,357 - INFO - ✓ Processed: HUMA_P2_OOD_1731_AntiquitaetenSchatz.xml (2395 tokens)\n",
      "Processing GATE XML files:   2%| | 7/336 [00:02<02:06,  2.60file/s, file=HUMA_P22025-06-15 23:50:27,734 - INFO - ✓ Processed: HUMA_P2_WMD_1739_Stollberg.xml (2309 tokens)\n",
      "Processing GATE XML files:   2%| | 8/336 [00:03<02:05,  2.62file/s, file=HUMA_P22025-06-15 23:50:28,106 - INFO - ✓ Processed: HUMA_P2_NoD_1720_Remarques.xml (2339 tokens)\n",
      "Processing GATE XML files:   3%| | 9/336 [00:03<02:04,  2.64file/s, file=HUMA_P22025-06-15 23:50:28,484 - INFO - ✓ Processed: HUMA_P2_WOD_1740_Poesie.xml (2477 tokens)\n",
      "Processing GATE XML files:   3%| | 10/336 [00:03<02:03,  2.64file/s, file=HUMA_P2025-06-15 23:50:28,832 - INFO - ✓ Processed: HUMA_P2_WMD_1748_Samuel.xml (2319 tokens)\n",
      "Processing GATE XML files:   3%| | 11/336 [00:04<02:00,  2.71file/s, file=HUMA_P2025-06-15 23:50:29,248 - INFO - ✓ Processed: HUMA_P2_WOD_1741_Antiquitaeten.xml (2422 tokens)\n",
      "Processing GATE XML files:   4%| | 12/336 [00:04<02:04,  2.61file/s, file=HUMA_P2025-06-15 23:50:29,623 - INFO - ✓ Processed: HUMA_P2_NoD_1739_MusicalischInterval.xml (2384 tokens)\n",
      "Processing GATE XML files:   4%| | 13/336 [00:04<02:03,  2.62file/s, file=HUMA_P2025-06-15 23:50:29,980 - INFO - ✓ Processed: HUMA_P2_OMD_1717_DienstMaegde.xml (2329 tokens)\n",
      "Processing GATE XML files:   4%| | 14/336 [00:05<02:00,  2.68file/s, file=HUMA_P2025-06-15 23:50:30,365 - INFO - ✓ Processed: HUMA_P2_OMD_1729_Biedermann.xml (2447 tokens)\n",
      "Processing GATE XML files:   4%| | 15/336 [00:05<02:00,  2.65file/s, file=SERM_P2025-06-15 23:50:30,774 - INFO - ✓ Processed: SERM_P2_OMD_1715_Beerdigung.xml (2543 tokens)\n",
      "Processing GATE XML files:   5%| | 16/336 [00:06<02:03,  2.59file/s, file=SERM_P2025-06-15 23:50:31,152 - INFO - ✓ Processed: SERM_P2_WMD_1702_Leben.xml (2345 tokens)\n",
      "Processing GATE XML files:   5%| | 17/336 [00:06<02:02,  2.60file/s, file=SERM_P2025-06-15 23:50:31,623 - INFO - ✓ Processed: SERM_P2_WMD_1721_HeilBronnen.xml (2369 tokens)\n",
      "Processing GATE XML files:   5%| | 18/336 [00:06<02:10,  2.44file/s, file=SERM_P2025-06-15 23:50:32,009 - INFO - ✓ Processed: SERM_P2_NoD_1715_Klugheit.xml (2449 tokens)\n",
      "Processing GATE XML files:   6%| | 19/336 [00:07<02:07,  2.48file/s, file=SERM_P2025-06-15 23:50:32,444 - INFO - ✓ Processed: SERM_P2_WOD_1739_Kranckentrost.xml (2344 tokens)\n",
      "Processing GATE XML files:   6%| | 20/336 [00:07<02:10,  2.42file/s, file=SERM_P2025-06-15 23:50:32,836 - INFO - ✓ Processed: SERM_P2_NoD_1730_JubelFeste.xml (2457 tokens)\n",
      "Processing GATE XML files:   6%| | 21/336 [00:08<02:08,  2.46file/s, file=SERM_P2025-06-15 23:50:33,211 - INFO - ✓ Processed: SERM_P2_OMD_1734_Evangelisch.xml (2286 tokens)\n",
      "Processing GATE XML files:   7%| | 22/336 [00:08<02:04,  2.52file/s, file=SERM_P2025-06-15 23:50:33,577 - INFO - ✓ Processed: SERM_P2_NoD_1715_Seeligkeit.xml (2209 tokens)\n",
      "Processing GATE XML files:   7%| | 23/336 [00:08<02:01,  2.58file/s, file=SERM_P2025-06-15 23:50:33,961 - INFO - ✓ Processed: SERM_P2_OOD_1709_Orgel.xml (2358 tokens)\n",
      "Processing GATE XML files:   7%| | 24/336 [00:09<02:00,  2.59file/s, file=SERM_P2025-06-15 23:50:34,542 - INFO - ✓ Processed: SERM_P2_WOD_1730_SeelenLiecht.xml (2826 tokens)\n",
      "Processing GATE XML files:   7%| | 25/336 [00:09<02:18,  2.25file/s, file=SERM_P2025-06-15 23:50:34,977 - INFO - ✓ Processed: SERM_P2_OOD_1700_FeyerTag.xml (2491 tokens)\n",
      "Processing GATE XML files:   8%| | 26/336 [00:10<02:16,  2.26file/s, file=SERM_P2025-06-15 23:50:35,384 - INFO - ✓ Processed: SERM_P2_WOD_1708_Zotten.xml (2448 tokens)\n",
      "Processing GATE XML files:   8%| | 27/336 [00:10<02:13,  2.32file/s, file=SERM_P2025-06-15 23:50:35,774 - INFO - ✓ Processed: SERM_P2_WMD_1743_TrostPredigt.xml (2398 tokens)\n",
      "Processing GATE XML files:   8%| | 28/336 [00:11<02:09,  2.39file/s, file=SERM_P2025-06-15 23:50:36,188 - INFO - ✓ Processed: SERM_P2_OOD_1728_Verstellte.xml (2492 tokens)\n",
      "Processing GATE XML files:   9%| | 29/336 [00:11<02:08,  2.40file/s, file=SERM_P2025-06-15 23:50:36,580 - INFO - ✓ Processed: SERM_P2_OMD_1706_GedaechtnisPredigt.xml (2262 tokens)\n",
      "Processing GATE XML files:   9%| | 30/336 [00:11<02:05,  2.44file/s, file=NARR_P2025-06-15 23:50:36,998 - INFO - ✓ Processed: NARR_P2_NoD_1715_Africa.xml (2389 tokens)\n",
      "Processing GATE XML files:   9%| | 31/336 [00:12<02:05,  2.42file/s, file=NARR_P2025-06-15 23:50:37,405 - INFO - ✓ Processed: NARR_P2_OMD_1731_Seefahrer.xml (2347 tokens)\n",
      "Processing GATE XML files:  10%| | 32/336 [00:12<02:04,  2.44file/s, file=NARR_P2025-06-15 23:50:37,788 - INFO - ✓ Processed: NARR_P2_WMD_1750_Teutsche.xml (2391 tokens)\n",
      "Processing GATE XML files:  10%| | 33/336 [00:13<02:01,  2.48file/s, file=NARR_P2025-06-15 23:50:38,180 - INFO - ✓ Processed: NARR_P2_OOD_1734_TaendlMarckt.xml (2308 tokens)\n",
      "Processing GATE XML files:  10%| | 34/336 [00:13<02:00,  2.50file/s, file=NARR_P2025-06-15 23:50:38,553 - INFO - ✓ Processed: NARR_P2_WOD_1724_JungferRobinsone.xml (2416 tokens)\n",
      "Processing GATE XML files:  10%| | 35/336 [00:13<01:57,  2.55file/s, file=NARR_P2025-06-15 23:50:38,930 - INFO - ✓ Processed: NARR_P2_WMD_1716_Fleurie.xml (2282 tokens)\n",
      "Processing GATE XML files:  11%| | 36/336 [00:14<01:56,  2.58file/s, file=NARR_P2025-06-15 23:50:39,320 - INFO - ✓ Processed: NARR_P2_OMD_1738_LebensBeschreibung.xml (2416 tokens)\n",
      "Processing GATE XML files:  11%| | 37/336 [00:14<01:56,  2.58file/s, file=NARR_P2025-06-15 23:50:39,815 - INFO - ✓ Processed: NARR_P2_OOD_1703_Narrennest.xml (2348 tokens)\n",
      "Processing GATE XML files:  11%| | 38/336 [00:15<02:05,  2.38file/s, file=NARR_P2025-06-15 23:50:40,237 - INFO - ✓ Processed: NARR_P2_WMD_1742_RedlicheMann.xml (2524 tokens)\n",
      "Processing GATE XML files:  12%| | 39/336 [00:15<02:04,  2.38file/s, file=NARR_P2025-06-15 23:50:40,606 - INFO - ✓ Processed: NARR_P2_OOD_1715_HeldenGeschichte.xml (2338 tokens)\n",
      "Processing GATE XML files:  12%| | 40/336 [00:15<01:59,  2.47file/s, file=NARR_P2025-06-15 23:50:41,036 - INFO - ✓ Processed: NARR_P2_WOD_1744_Fabeln.xml (2521 tokens)\n",
      "Processing GATE XML files:  12%| | 41/336 [00:16<02:01,  2.42file/s, file=NARR_P2025-06-15 23:50:41,446 - INFO - ✓ Processed: NARR_P2_NoD_1706_SatyrischerRoman.xml (2378 tokens)\n",
      "Processing GATE XML files:  12%|▏| 42/336 [00:16<02:01,  2.43file/s, file=NARR_P2025-06-15 23:50:41,861 - INFO - ✓ Processed: NARR_P2_NoD_1709_OstIndien.xml (2442 tokens)\n",
      "Processing GATE XML files:  13%|▏| 43/336 [00:17<02:00,  2.42file/s, file=NARR_P2025-06-15 23:50:42,261 - INFO - ✓ Processed: NARR_P2_WOD_1746_Muetze.xml (2404 tokens)\n",
      "Processing GATE XML files:  13%|▏| 44/336 [00:17<01:59,  2.44file/s, file=NARR_P2025-06-15 23:50:42,727 - INFO - ✓ Processed: NARR_P2_OMD_1708_Affecten.xml (2438 tokens)\n",
      "Processing GATE XML files:  13%|▏| 45/336 [00:18<02:03,  2.35file/s, file=NEWS_P2025-06-15 23:50:42,981 - INFO - ✓ Processed: NEWS_P2_OOD_1702_muenchen2.xml (1375 tokens)\n",
      "Processing GATE XML files:  14%|▏| 46/336 [00:18<01:48,  2.67file/s, file=NEWS_P2025-06-15 23:50:43,395 - INFO - ✓ Processed: NEWS_P2_NoD_1735_berlin.xml (2273 tokens)\n",
      "Processing GATE XML files:  14%|▏| 47/336 [00:18<01:51,  2.59file/s, file=NEWS_P2025-06-15 23:50:43,631 - INFO - ✓ Processed: NEWS_P2_OOD_1702_muenchen1.xml (1564 tokens)\n",
      "Processing GATE XML files:  14%|▏| 48/336 [00:18<01:38,  2.93file/s, file=NEWS_P2025-06-15 23:50:43,736 - INFO - ✓ Processed: NEWS_P2_WMD_1701_hanau2.xml (794 tokens)\n",
      "Processing GATE XML files:  15%|▏| 49/336 [00:19<01:17,  3.70file/s, file=NEWS_P2025-06-15 23:50:44,170 - INFO - ✓ Processed: NEWS_P2_OMD_1724_halle.xml (2286 tokens)\n",
      "Processing GATE XML files:  15%|▏| 50/336 [00:19<01:31,  3.13file/s, file=NEWS_P2025-06-15 23:50:44,465 - INFO - ✓ Processed: NEWS_P2_WMD_1701_hanau1.xml (1735 tokens)\n",
      "Processing GATE XML files:  15%|▏| 51/336 [00:19<01:28,  3.20file/s, file=NEWS_P2025-06-15 23:50:44,504 - INFO - ✓ Processed: NEWS_P2_OMD_1722_leipzig2.xml (327 tokens)\n",
      "Processing GATE XML files:  15%|▏| 51/336 [00:19<01:28,  3.20file/s, file=NEWS_P2025-06-15 23:50:44,845 - INFO - ✓ Processed: NEWS_P2_NoD_1740_berlin1.xml (2001 tokens)\n",
      "Processing GATE XML files:  16%|▏| 53/336 [00:20<01:12,  3.91file/s, file=NEWS_P2025-06-15 23:50:45,459 - INFO - ✓ Processed: NEWS_P2_WMD_1750_frankfurt.xml (2765 tokens)\n",
      "Processing GATE XML files:  16%|▏| 54/336 [00:20<01:37,  2.90file/s, file=NEWS_P2025-06-15 23:50:45,945 - INFO - ✓ Processed: NEWS_P2_NoD_1702_hamburg.xml (2685 tokens)\n",
      "Processing GATE XML files:  16%|▏| 55/336 [00:21<01:47,  2.62file/s, file=NEWS_P2025-06-15 23:50:46,332 - INFO - ✓ Processed: NEWS_P2_OMD_1722_leipzig1.xml (2185 tokens)\n",
      "Processing GATE XML files:  17%|▏| 56/336 [00:21<01:47,  2.61file/s, file=NEWS_P2025-06-15 23:50:46,390 - INFO - ✓ Processed: NEWS_P2_NoD_1740_berlin2.xml (510 tokens)\n",
      "Processing GATE XML files:  17%|▏| 56/336 [00:21<01:47,  2.61file/s, file=NEWS_P2025-06-15 23:50:46,831 - INFO - ✓ Processed: NEWS_P2_WOD_1722_zuerich.xml (2453 tokens)\n",
      "Processing GATE XML files:  17%|▏| 58/336 [00:22<01:30,  3.08file/s, file=NEWS_P2025-06-15 23:50:47,199 - INFO - ✓ Processed: NEWS_P2_OOD_1712_wien.xml (1962 tokens)\n",
      "Processing GATE XML files:  18%|▏| 59/336 [00:22<01:32,  2.99file/s, file=NEWS_P2025-06-15 23:50:47,261 - INFO - ✓ Processed: NEWS_P2_OOD_1713_wien.xml (552 tokens)\n",
      "Processing GATE XML files:  18%|▏| 59/336 [00:22<01:32,  2.99file/s, file=NEWS_P2025-06-15 23:50:47,720 - INFO - ✓ Processed: NEWS_P2_OOD_1744_graz.xml (2517 tokens)\n",
      "Processing GATE XML files:  18%|▏| 61/336 [00:23<01:23,  3.28file/s, file=NEWS_P2025-06-15 23:50:47,836 - INFO - ✓ Processed: NEWS_P2_OMD_1744_erfurt2.xml (524 tokens)\n",
      "Processing GATE XML files:  18%|▏| 62/336 [00:23<01:11,  3.81file/s, file=NEWS_P2025-06-15 23:50:48,248 - INFO - ✓ Processed: NEWS_P2_WMD_1701_frankfurt.xml (2301 tokens)\n",
      "Processing GATE XML files:  19%|▏| 63/336 [00:23<01:21,  3.35file/s, file=NEWS_P2025-06-15 23:50:48,326 - INFO - ✓ Processed: NEWS_P2_WOD_1749_loerrach2.xml (624 tokens)\n",
      "Processing GATE XML files:  19%|▏| 63/336 [00:23<01:21,  3.35file/s, file=NEWS_P2025-06-15 23:50:48,529 - INFO - ✓ Processed: NEWS_P2_WOD_1723_augsburg1.xml (1380 tokens)\n",
      "Processing GATE XML files:  19%|▏| 65/336 [00:23<01:03,  4.27file/s, file=NEWS_P2025-06-15 23:50:48,929 - INFO - ✓ Processed: NEWS_P2_OMD_1744_erfurt1.xml (2184 tokens)\n",
      "Processing GATE XML files:  20%|▏| 66/336 [00:24<01:13,  3.68file/s, file=NEWS_P2025-06-15 23:50:49,284 - INFO - ✓ Processed: NEWS_P2_WOD_1749_loerrach1.xml (2058 tokens)\n",
      "Processing GATE XML files:  20%|▏| 67/336 [00:24<01:18,  3.43file/s, file=NEWS_P2025-06-15 23:50:49,404 - INFO - ✓ Processed: NEWS_P2_WOD_1723_augsburg2.xml (876 tokens)\n",
      "Processing GATE XML files:  20%|▏| 68/336 [00:24<01:06,  4.04file/s, file=LEGA_P2025-06-15 23:50:49,796 - INFO - ✓ Processed: LEGA_P2_WOD_1738_Constantz.xml (2405 tokens)\n",
      "Processing GATE XML files:  21%|▏| 69/336 [00:25<01:16,  3.49file/s, file=LEGA_P2025-06-15 23:50:50,209 - INFO - ✓ Processed: LEGA_P2_NoD_1707_Reglement.xml (2467 tokens)\n",
      "Processing GATE XML files:  21%|▏| 70/336 [00:25<01:25,  3.11file/s, file=LEGA_P2025-06-15 23:50:50,711 - INFO - ✓ Processed: LEGA_P2_OOD_1704_OrdnungNuernberg.xml (2504 tokens)\n",
      "Processing GATE XML files:  21%|▏| 71/336 [00:26<01:38,  2.68file/s, file=LEGA_P2025-06-15 23:50:51,093 - INFO - ✓ Processed: LEGA_P2_OMD_1709_WaeysenOrdnung.xml (2326 tokens)\n",
      "Processing GATE XML files:  21%|▏| 72/336 [00:26<01:39,  2.66file/s, file=LEGA_P2025-06-15 23:50:51,529 - INFO - ✓ Processed: LEGA_P2_WOD_1729_WechselRecht.xml (2263 tokens)\n",
      "Processing GATE XML files:  22%|▏| 73/336 [00:26<01:43,  2.54file/s, file=LEGA_P2025-06-15 23:50:51,881 - INFO - ✓ Processed: LEGA_P2_NoD_1724_StadtRecht.xml (2412 tokens)\n",
      "Processing GATE XML files:  22%|▏| 74/336 [00:27<01:39,  2.62file/s, file=LEGA_P2025-06-15 23:50:52,298 - INFO - ✓ Processed: LEGA_P2_WMD_1724_GesetzBuch.xml (2525 tokens)\n",
      "Processing GATE XML files:  22%|▏| 75/336 [00:27<01:42,  2.55file/s, file=LEGA_P2025-06-15 23:50:52,746 - INFO - ✓ Processed: LEGA_P2_OOD_1719_Privilegien.xml (2424 tokens)\n",
      "Processing GATE XML files:  23%|▏| 76/336 [00:28<01:46,  2.45file/s, file=LEGA_P2025-06-15 23:50:53,265 - INFO - ✓ Processed: LEGA_P2_WOD_1711_HalsGericht.xml (2530 tokens)\n",
      "Processing GATE XML files:  23%|▏| 77/336 [00:28<01:54,  2.26file/s, file=LEGA_P2025-06-15 23:50:53,667 - INFO - ✓ Processed: LEGA_P2_WMD_1733_Heyrathen.xml (2441 tokens)\n",
      "Processing GATE XML files:  23%|▏| 78/336 [00:29<01:50,  2.33file/s, file=LEGA_P2025-06-15 23:50:54,057 - INFO - ✓ Processed: LEGA_P2_OMD_1723_JurisMilitaris.xml (2507 tokens)\n",
      "Processing GATE XML files:  24%|▏| 79/336 [00:29<01:47,  2.39file/s, file=LEGA_P2025-06-15 23:50:54,508 - INFO - ✓ Processed: LEGA_P2_WMD_1720_VatterMord.xml (2512 tokens)\n",
      "Processing GATE XML files:  24%|▏| 80/336 [00:29<01:49,  2.34file/s, file=LEGA_P2025-06-15 23:50:54,904 - INFO - ✓ Processed: LEGA_P2_NoD_1700_Braunschweig.xml (2465 tokens)\n",
      "Processing GATE XML files:  24%|▏| 81/336 [00:30<01:46,  2.39file/s, file=LEGA_P2025-06-15 23:50:55,314 - INFO - ✓ Processed: LEGA_P2_OMD_1710_ReichsArchiv.xml (2391 tokens)\n",
      "Processing GATE XML files:  24%|▏| 82/336 [00:30<01:45,  2.41file/s, file=LEGA_P2025-06-15 23:50:55,950 - INFO - ✓ Processed: LEGA_P2_OOD_1709_Promptuarii.xml (2456 tokens)\n",
      "Processing GATE XML files:  25%|▏| 83/336 [00:31<02:01,  2.08file/s, file=DRAM_P2025-06-15 23:50:56,327 - INFO - ✓ Processed: DRAM_P2_WOD_1748_Hoelle.xml (2361 tokens)\n",
      "Processing GATE XML files:  25%|▎| 84/336 [00:31<01:53,  2.22file/s, file=DRAM_P2025-06-15 23:50:56,784 - INFO - ✓ Processed: DRAM_P2_WMD_1745_Zuegellose.xml (2479 tokens)\n",
      "Processing GATE XML files:  25%|▎| 85/336 [00:32<01:53,  2.21file/s, file=DRAM_P2025-06-15 23:50:57,338 - INFO - ✓ Processed: DRAM_P2_OOD_1733_Ciro.xml (2354 tokens)\n",
      "Processing GATE XML files:  26%|▎| 86/336 [00:32<02:00,  2.07file/s, file=DRAM_P2025-06-15 23:50:57,801 - INFO - ✓ Processed: DRAM_P2_WMD_1742_Bookesbeutel.xml (2249 tokens)\n",
      "Processing GATE XML files:  26%|▎| 87/336 [00:33<01:58,  2.10file/s, file=DRAM_P2025-06-15 23:50:58,179 - INFO - ✓ Processed: DRAM_P2_OMD_1736_Fischbein.xml (2284 tokens)\n",
      "Processing GATE XML files:  26%|▎| 88/336 [00:33<01:50,  2.24file/s, file=DRAM_P2025-06-15 23:50:58,614 - INFO - ✓ Processed: DRAM_P2_WOD_1702_Helvetia.xml (2210 tokens)\n",
      "Processing GATE XML files:  26%|▎| 89/336 [00:33<01:49,  2.25file/s, file=DRAM_P2025-06-15 23:50:59,047 - INFO - ✓ Processed: DRAM_P2_OOD_1725_Venceslao.xml (2403 tokens)\n",
      "Processing GATE XML files:  27%|▎| 90/336 [00:34<01:48,  2.27file/s, file=DRAM_P2025-06-15 23:50:59,465 - INFO - ✓ Processed: DRAM_P2_NoD_1707_SchaeferSpiel.xml (2492 tokens)\n",
      "Processing GATE XML files:  27%|▎| 91/336 [00:34<01:46,  2.31file/s, file=DRAM_P2025-06-15 23:50:59,895 - INFO - ✓ Processed: DRAM_P2_NoD_1711_Croesus.xml (2296 tokens)\n",
      "Processing GATE XML files:  27%|▎| 92/336 [00:35<01:45,  2.31file/s, file=DRAM_P2025-06-15 23:51:00,416 - INFO - ✓ Processed: DRAM_P2_OOD_1749_Schaeferinsel.xml (2752 tokens)\n",
      "Processing GATE XML files:  28%|▎| 93/336 [00:35<01:51,  2.18file/s, file=DRAM_P2025-06-15 23:51:00,862 - INFO - ✓ Processed: DRAM_P2_WMD_1743_DieGeistlichen.xml (2548 tokens)\n",
      "Processing GATE XML files:  28%|▎| 94/336 [00:36<01:50,  2.20file/s, file=DRAM_P2025-06-15 23:51:01,468 - INFO - ✓ Processed: DRAM_P2_OMD_1732_Cato.xml (2867 tokens)\n",
      "Processing GATE XML files:  28%|▎| 95/336 [00:36<02:00,  2.00file/s, file=DRAM_P2025-06-15 23:51:02,084 - INFO - ✓ Processed: DRAM_P2_OMD_1747_Schwestern.xml (3408 tokens)\n",
      "Processing GATE XML files:  29%|▎| 96/336 [00:37<02:08,  1.87file/s, file=DRAM_P2025-06-15 23:51:02,655 - INFO - ✓ Processed: DRAM_P2_WOD_1737_Verehrung.xml (3088 tokens)\n",
      "Processing GATE XML files:  29%|▎| 97/336 [00:38<02:10,  1.83file/s, file=DRAM_P2025-06-15 23:51:03,133 - INFO - ✓ Processed: DRAM_P2_NoD_1749_AlteJungfer.xml (2525 tokens)\n",
      "Processing GATE XML files:  29%|▎| 98/336 [00:38<02:05,  1.90file/s, file=SCIE_P2025-06-15 23:51:03,529 - INFO - ✓ Processed: SCIE_P2_WOD_1741_Erden.xml (2379 tokens)\n",
      "Processing GATE XML files:  29%|▎| 99/336 [00:38<01:55,  2.06file/s, file=SCIE_P2025-06-15 23:51:03,989 - INFO - ✓ Processed: SCIE_P2_WMD_1744_SelbstArtzt.xml (2400 tokens)\n",
      "Processing GATE XML files:  30%|▎| 100/336 [00:39<01:52,  2.09file/s, file=SCIE_2025-06-15 23:51:04,443 - INFO - ✓ Processed: SCIE_P2_OMD_1737_Medica.xml (2321 tokens)\n",
      "Processing GATE XML files:  30%|▎| 101/336 [00:39<01:50,  2.12file/s, file=SCIE_2025-06-15 23:51:04,889 - INFO - ✓ Processed: SCIE_P2_WOD_1720_FangSchlaeussen.xml (2494 tokens)\n",
      "Processing GATE XML files:  30%|▎| 102/336 [00:40<01:48,  2.16file/s, file=SCIE_2025-06-15 23:51:05,302 - INFO - ✓ Processed: SCIE_P2_NoD_1734_Barometer.xml (2460 tokens)\n",
      "Processing GATE XML files:  31%|▎| 103/336 [00:40<01:44,  2.23file/s, file=SCIE_2025-06-15 23:51:05,658 - INFO - ✓ Processed: SCIE_P2_NoD_1736_Anweisung.xml (2345 tokens)\n",
      "Processing GATE XML files:  31%|▎| 104/336 [00:41<01:37,  2.38file/s, file=SCIE_2025-06-15 23:51:06,064 - INFO - ✓ Processed: SCIE_P2_OOD_1745_Mathematicus.xml (2420 tokens)\n",
      "Processing GATE XML files:  31%|▎| 105/336 [00:41<01:36,  2.40file/s, file=SCIE_2025-06-15 23:51:06,475 - INFO - ✓ Processed: SCIE_P2_WMD_1714_Kleinod.xml (2466 tokens)\n",
      "Processing GATE XML files:  32%|▎| 106/336 [00:41<01:35,  2.41file/s, file=SCIE_2025-06-15 23:51:07,004 - INFO - ✓ Processed: SCIE_P2_WMD_1702_Armuth.xml (2726 tokens)\n",
      "Processing GATE XML files:  32%|▎| 107/336 [00:42<01:42,  2.23file/s, file=SCIE_2025-06-15 23:51:07,483 - INFO - ✓ Processed: SCIE_P2_OOD_1705_WerckSchul.xml (2456 tokens)\n",
      "Processing GATE XML files:  32%|▎| 108/336 [00:42<01:44,  2.18file/s, file=SCIE_2025-06-15 23:51:07,912 - INFO - ✓ Processed: SCIE_P2_WOD_1708_WunderbarenWelt.xml (2480 tokens)\n",
      "Processing GATE XML files:  32%|▎| 109/336 [00:43<01:41,  2.23file/s, file=SCIE_2025-06-15 23:51:08,313 - INFO - ✓ Processed: SCIE_P2_NoD_1744_Cometen.xml (2413 tokens)\n",
      "Processing GATE XML files:  33%|▎| 110/336 [00:43<01:38,  2.30file/s, file=SCIE_2025-06-15 23:51:08,769 - INFO - ✓ Processed: SCIE_P2_OOD_1722_NordScheines.xml (2593 tokens)\n",
      "Processing GATE XML files:  33%|▎| 111/336 [00:44<01:39,  2.27file/s, file=SCIE_2025-06-15 23:51:09,158 - INFO - ✓ Processed: SCIE_P2_OMD_1717_Materialist.xml (2416 tokens)\n",
      "Processing GATE XML files:  33%|▎| 112/336 [00:44<01:35,  2.35file/s, file=SCIE_2025-06-15 23:51:09,637 - INFO - ✓ Processed: SCIE_P2_OMD_1731_HarnRuhr.xml (2305 tokens)\n",
      "Processing GATE XML files:  34%|▎| 113/336 [00:44<01:38,  2.26file/s, file=HUMA_2025-06-15 23:51:10,021 - INFO - ✓ Processed: HUMA_P3_WMD_1772_Baukunst.xml (2451 tokens)\n",
      "Processing GATE XML files:  34%|▎| 114/336 [00:45<01:34,  2.36file/s, file=HUMA_2025-06-15 23:51:10,455 - INFO - ✓ Processed: HUMA_P3_WOD_1784_Weibs.xml (2547 tokens)\n",
      "Processing GATE XML files:  34%|▎| 115/336 [00:45<01:34,  2.34file/s, file=HUMA_2025-06-15 23:51:10,876 - INFO - ✓ Processed: HUMA_P3_WOD_1795_Dichtung.xml (2344 tokens)\n",
      "Processing GATE XML files:  35%|▎| 116/336 [00:46<01:33,  2.35file/s, file=HUMA_2025-06-15 23:51:11,335 - INFO - ✓ Processed: HUMA_P3_NoD_1762_Kreuzzuege.xml (2511 tokens)\n",
      "Processing GATE XML files:  35%|▎| 117/336 [00:46<01:35,  2.30file/s, file=HUMA_2025-06-15 23:51:11,805 - INFO - ✓ Processed: HUMA_P3_WMD_1789_Italien.xml (2396 tokens)\n",
      "Processing GATE XML files:  35%|▎| 118/336 [00:47<01:37,  2.24file/s, file=HUMA_2025-06-15 23:51:12,181 - INFO - ✓ Processed: HUMA_P3_OOD_1792_Alterthuemer.xml (2376 tokens)\n",
      "Processing GATE XML files:  35%|▎| 119/336 [00:47<01:32,  2.35file/s, file=HUMA_2025-06-15 23:51:12,669 - INFO - ✓ Processed: HUMA_P3_NoD_1772_Ursprung.xml (2746 tokens)\n",
      "Processing GATE XML files:  36%|▎| 120/336 [00:48<01:35,  2.25file/s, file=HUMA_2025-06-15 23:51:13,050 - INFO - ✓ Processed: HUMA_P3_OOD_1774_Emil.xml (2348 tokens)\n",
      "Processing GATE XML files:  36%|▎| 121/336 [00:48<01:31,  2.35file/s, file=HUMA_2025-06-15 23:51:13,475 - INFO - ✓ Processed: HUMA_P3_NoD_1788_Menschen.xml (2585 tokens)\n",
      "Processing GATE XML files:  36%|▎| 122/336 [00:48<01:30,  2.35file/s, file=HUMA_2025-06-15 23:51:13,877 - INFO - ✓ Processed: HUMA_P3_WMD_1777_Homburg.xml (2382 tokens)\n",
      "Processing GATE XML files:  37%|▎| 123/336 [00:49<01:29,  2.39file/s, file=HUMA_2025-06-15 23:51:14,464 - INFO - ✓ Processed: HUMA_P3_OMD_1777_Dichtkunst.xml (2630 tokens)\n",
      "Processing GATE XML files:  37%|▎| 124/336 [00:49<01:39,  2.13file/s, file=HUMA_2025-06-15 23:51:14,895 - INFO - ✓ Processed: HUMA_P3_OMD_1798_Sondershausen.xml (2553 tokens)\n",
      "Processing GATE XML files:  37%|▎| 125/336 [00:50<01:36,  2.19file/s, file=HUMA_2025-06-15 23:51:15,379 - INFO - ✓ Processed: HUMA_P3_WOD_1768_Roemer.xml (2468 tokens)\n",
      "Processing GATE XML files:  38%|▍| 126/336 [00:50<01:37,  2.15file/s, file=HUMA_2025-06-15 23:51:15,810 - INFO - ✓ Processed: HUMA_P3_OOD_1778_Pons.xml (2465 tokens)\n",
      "Processing GATE XML files:  38%|▍| 127/336 [00:51<01:35,  2.20file/s, file=HUMA_2025-06-15 23:51:16,237 - INFO - ✓ Processed: HUMA_P3_OMD_1774_Roman.xml (2383 tokens)\n",
      "Processing GATE XML files:  38%|▍| 128/336 [00:51<01:32,  2.24file/s, file=SERM_2025-06-15 23:51:16,653 - INFO - ✓ Processed: SERM_P3_OOD_1792_Sonntagen.xml (2554 tokens)\n",
      "Processing GATE XML files:  38%|▍| 129/336 [00:51<01:30,  2.29file/s, file=SERM_2025-06-15 23:51:17,164 - INFO - ✓ Processed: SERM_P3_OMD_1760_Folgen.xml (2367 tokens)\n",
      "Processing GATE XML files:  39%|▍| 130/336 [00:52<01:34,  2.17file/s, file=SERM_2025-06-15 23:51:17,546 - INFO - ✓ Processed: SERM_P3_WOD_1792_Hegel.xml (2320 tokens)\n",
      "Processing GATE XML files:  39%|▍| 131/336 [00:52<01:29,  2.29file/s, file=SERM_2025-06-15 23:51:17,918 - INFO - ✓ Processed: SERM_P3_OOD_1782_Erloeser.xml (2375 tokens)\n",
      "Processing GATE XML files:  39%|▍| 132/336 [00:53<01:25,  2.40file/s, file=SERM_2025-06-15 23:51:18,332 - INFO - ✓ Processed: SERM_P3_NoD_1798_LetztePredigt.xml (2388 tokens)\n",
      "Processing GATE XML files:  40%|▍| 133/336 [00:53<01:24,  2.40file/s, file=SERM_2025-06-15 23:51:18,761 - INFO - ✓ Processed: SERM_P3_WOD_1751_DreiKoenig.xml (2522 tokens)\n",
      "Processing GATE XML files:  40%|▍| 134/336 [00:54<01:24,  2.38file/s, file=SERM_2025-06-15 23:51:19,149 - INFO - ✓ Processed: SERM_P3_OMD_1790_Unruhen.xml (2300 tokens)\n",
      "Processing GATE XML files:  40%|▍| 135/336 [00:54<01:22,  2.44file/s, file=SERM_2025-06-15 23:51:19,666 - INFO - ✓ Processed: SERM_P3_OMD_1756_Trost.xml (2267 tokens)\n",
      "Processing GATE XML files:  40%|▍| 136/336 [00:55<01:28,  2.26file/s, file=SERM_2025-06-15 23:51:20,046 - INFO - ✓ Processed: SERM_P3_WOD_1790_Strassburg.xml (2295 tokens)\n",
      "Processing GATE XML files:  41%|▍| 137/336 [00:55<01:24,  2.36file/s, file=SERM_2025-06-15 23:51:20,408 - INFO - ✓ Processed: SERM_P3_WMD_1774_Gewohnheit.xml (2297 tokens)\n",
      "Processing GATE XML files:  41%|▍| 138/336 [00:55<01:20,  2.47file/s, file=SERM_2025-06-15 23:51:20,812 - INFO - ✓ Processed: SERM_P3_WMD_1780_LottoSucht.xml (2425 tokens)\n",
      "Processing GATE XML files:  41%|▍| 139/336 [00:56<01:19,  2.47file/s, file=SERM_2025-06-15 23:51:21,206 - INFO - ✓ Processed: SERM_P3_OOD_1751_Elisabetha.xml (2253 tokens)\n",
      "Processing GATE XML files:  42%|▍| 140/336 [00:56<01:18,  2.49file/s, file=SERM_2025-06-15 23:51:21,579 - INFO - ✓ Processed: SERM_P3_NoD_1765_Trauerrede.xml (2301 tokens)\n",
      "Processing GATE XML files:  42%|▍| 141/336 [00:56<01:16,  2.54file/s, file=SERM_2025-06-15 23:51:21,941 - INFO - ✓ Processed: SERM_P3_NoD_1770_Gottesdienst.xml (2290 tokens)\n",
      "Processing GATE XML files:  42%|▍| 142/336 [00:57<01:14,  2.61file/s, file=SERM_2025-06-15 23:51:22,431 - INFO - ✓ Processed: SERM_P3_WMD_1780_Feuersbrunst.xml (2366 tokens)\n",
      "Processing GATE XML files:  43%|▍| 143/336 [00:57<01:20,  2.41file/s, file=NARR_2025-06-15 23:51:22,940 - INFO - ✓ Processed: NARR_P3_OOD_1789_PeterProsch.xml (2477 tokens)\n",
      "Processing GATE XML files:  43%|▍| 144/336 [00:58<01:25,  2.25file/s, file=NARR_2025-06-15 23:51:23,388 - INFO - ✓ Processed: NARR_P3_OOD_1796_Quintus.xml (2515 tokens)\n",
      "Processing GATE XML files:  43%|▍| 145/336 [00:58<01:24,  2.25file/s, file=NARR_2025-06-15 23:51:23,821 - INFO - ✓ Processed: NARR_P3_WMD_1782_Fragmente.xml (2496 tokens)\n",
      "Processing GATE XML files:  43%|▍| 146/336 [00:59<01:23,  2.27file/s, file=NARR_2025-06-15 23:51:24,317 - INFO - ✓ Processed: NARR_P3_NoD_1786_Muenchhausen.xml (2460 tokens)\n",
      "Processing GATE XML files:  44%|▍| 147/336 [00:59<01:26,  2.18file/s, file=NARR_2025-06-15 23:51:24,745 - INFO - ✓ Processed: NARR_P3_WOD_1771_Usong.xml (2432 tokens)\n",
      "Processing GATE XML files:  44%|▍| 148/336 [01:00<01:24,  2.23file/s, file=NARR_2025-06-15 23:51:25,314 - INFO - ✓ Processed: NARR_P3_WMD_1783_MoralischeErzaehlungen.xml (2659 tokens)\n",
      "Processing GATE XML files:  44%|▍| 149/336 [01:00<01:30,  2.06file/s, file=NARR_2025-06-15 23:51:25,719 - INFO - ✓ Processed: NARR_P3_WOD_1766_Agathon.xml (2374 tokens)\n",
      "Processing GATE XML files:  45%|▍| 150/336 [01:01<01:25,  2.17file/s, file=NARR_2025-06-15 23:51:26,121 - INFO - ✓ Processed: NARR_P3_NoD_1796_Siebenkaes.xml (2342 tokens)\n",
      "Processing GATE XML files:  45%|▍| 151/336 [01:01<01:21,  2.26file/s, file=NARR_2025-06-15 23:51:26,517 - INFO - ✓ Processed: NARR_P3_OMD_1776_Zerbin.xml (2378 tokens)\n",
      "Processing GATE XML files:  45%|▍| 152/336 [01:01<01:18,  2.33file/s, file=NARR_2025-06-15 23:51:26,982 - INFO - ✓ Processed: NARR_P3_OOD_1787_Aglais.xml (2468 tokens)\n",
      "Processing GATE XML files:  46%|▍| 153/336 [01:02<01:20,  2.27file/s, file=NARR_2025-06-15 23:51:27,436 - INFO - ✓ Processed: NARR_P3_OMD_1782_Volksmaerchen.xml (2638 tokens)\n",
      "Processing GATE XML files:  46%|▍| 154/336 [01:02<01:20,  2.25file/s, file=NARR_2025-06-15 23:51:27,945 - INFO - ✓ Processed: NARR_P3_OMD_1774_Werther.xml (2353 tokens)\n",
      "Processing GATE XML files:  46%|▍| 155/336 [01:03<01:23,  2.16file/s, file=NARR_2025-06-15 23:51:28,393 - INFO - ✓ Processed: NARR_P3_WOD_1797_Hyperion.xml (2613 tokens)\n",
      "Processing GATE XML files:  46%|▍| 156/336 [01:03<01:22,  2.18file/s, file=NARR_2025-06-15 23:51:28,833 - INFO - ✓ Processed: NARR_P3_NoD_1790_AntonReiser.xml (2555 tokens)\n",
      "Processing GATE XML files:  47%|▍| 157/336 [01:04<01:21,  2.21file/s, file=NARR_2025-06-15 23:51:29,274 - INFO - ✓ Processed: NARR_P3_WMD_1775_Bacchidon.xml (2614 tokens)\n",
      "Processing GATE XML files:  47%|▍| 158/336 [01:04<01:20,  2.22file/s, file=NEWS_2025-06-15 23:51:29,688 - INFO - ✓ Processed: NEWS_P3_OMD_1769_erfurt.xml (2114 tokens)\n",
      "Processing GATE XML files:  47%|▍| 159/336 [01:05<01:17,  2.28file/s, file=NEWS_2025-06-15 23:51:29,858 - INFO - ✓ Processed: NEWS_P3_WOD_1784_freiburg2.xml (1114 tokens)\n",
      "Processing GATE XML files:  48%|▍| 160/336 [01:05<01:03,  2.79file/s, file=NEWS_2025-06-15 23:51:30,075 - INFO - ✓ Processed: NEWS_P3_WOD_1784_freiburg1.xml (1429 tokens)\n",
      "Processing GATE XML files:  48%|▍| 161/336 [01:05<00:55,  3.16file/s, file=NEWS_2025-06-15 23:51:30,314 - INFO - ✓ Processed: NEWS_P3_NoD_1786_wolfenbuettel1.xml (1470 tokens)\n",
      "Processing GATE XML files:  48%|▍| 162/336 [01:05<00:50,  3.42file/s, file=NEWS_2025-06-15 23:51:30,841 - INFO - ✓ Processed: NEWS_P3_WMD_1784_mannheim.xml (2295 tokens)\n",
      "Processing GATE XML files:  49%|▍| 163/336 [01:06<01:02,  2.75file/s, file=NEWS_2025-06-15 23:51:31,313 - INFO - ✓ Processed: NEWS_P3_WMD_1797_hanau.xml (2588 tokens)\n",
      "Processing GATE XML files:  49%|▍| 164/336 [01:06<01:08,  2.53file/s, file=NEWS_2025-06-15 23:51:31,508 - INFO - ✓ Processed: NEWS_P3_NoD_1786_wolfenbuettel2.xml (1120 tokens)\n",
      "Processing GATE XML files:  49%|▍| 165/336 [01:06<00:57,  2.98file/s, file=NEWS_2025-06-15 23:51:31,943 - INFO - ✓ Processed: NEWS_P3_WOD_1798_tuebingen.xml (2448 tokens)\n",
      "Processing GATE XML files:  49%|▍| 166/336 [01:07<01:02,  2.74file/s, file=NEWS_2025-06-15 23:51:32,353 - INFO - ✓ Processed: NEWS_P3_WMD_1793_mainz.xml (2492 tokens)\n",
      "Processing GATE XML files:  50%|▍| 167/336 [01:07<01:03,  2.64file/s, file=NEWS_2025-06-15 23:51:32,834 - INFO - ✓ Processed: NEWS_P3_OOD_1780_wien.xml (2590 tokens)\n",
      "Processing GATE XML files:  50%|▌| 168/336 [01:08<01:08,  2.44file/s, file=NEWS_2025-06-15 23:51:33,419 - INFO - ✓ Processed: NEWS_P3_WOD_1781_heilbronn.xml (2574 tokens)\n",
      "Processing GATE XML files:  50%|▌| 169/336 [01:08<01:17,  2.16file/s, file=NEWS_2025-06-15 23:51:33,887 - INFO - ✓ Processed: NEWS_P3_OOD_1791_bayreuth.xml (2559 tokens)\n",
      "Processing GATE XML files:  51%|▌| 170/336 [01:09<01:16,  2.16file/s, file=NEWS_2025-06-15 23:51:34,192 - INFO - ✓ Processed: NEWS_P3_OMD_1789_gotha.xml (1685 tokens)\n",
      "Processing GATE XML files:  51%|▌| 171/336 [01:09<01:08,  2.40file/s, file=NEWS_2025-06-15 23:51:34,637 - INFO - ✓ Processed: NEWS_P3_OOD_1790_erlangen.xml (2369 tokens)\n",
      "Processing GATE XML files:  51%|▌| 172/336 [01:09<01:09,  2.35file/s, file=NEWS_2025-06-15 23:51:35,203 - INFO - ✓ Processed: NEWS_P3_OMD_1784_gotha.xml (2821 tokens)\n",
      "Processing GATE XML files:  51%|▌| 173/336 [01:10<01:16,  2.14file/s, file=NEWS_2025-06-15 23:51:35,677 - INFO - ✓ Processed: NEWS_P3_NoD_1796_stettin.xml (2566 tokens)\n",
      "Processing GATE XML files:  52%|▌| 174/336 [01:11<01:16,  2.13file/s, file=NEWS_2025-06-15 23:51:35,820 - INFO - ✓ Processed: NEWS_P3_OMD_1790_gotha.xml (1006 tokens)\n",
      "Processing GATE XML files:  52%|▌| 175/336 [01:11<00:59,  2.69file/s, file=NEWS_2025-06-15 23:51:36,422 - INFO - ✓ Processed: NEWS_P3_NoD_1798_danzig.xml (2528 tokens)\n",
      "Processing GATE XML files:  52%|▌| 176/336 [01:11<01:10,  2.27file/s, file=LEGA_2025-06-15 23:51:36,853 - INFO - ✓ Processed: LEGA_P3_NoD_1751_FeuerOrdnung.xml (2345 tokens)\n",
      "Processing GATE XML files:  53%|▌| 177/336 [01:12<01:09,  2.28file/s, file=LEGA_2025-06-15 23:51:37,257 - INFO - ✓ Processed: LEGA_P3_OOD_1769_Theresiana.xml (2384 tokens)\n",
      "Processing GATE XML files:  53%|▌| 178/336 [01:12<01:07,  2.34file/s, file=LEGA_2025-06-15 23:51:37,658 - INFO - ✓ Processed: LEGA_P3_WMD_1772_RegimentsVerfassung.xml (2313 tokens)\n",
      "Processing GATE XML files:  53%|▌| 179/336 [01:13<01:05,  2.38file/s, file=LEGA_2025-06-15 23:51:38,064 - INFO - ✓ Processed: LEGA_P3_OOD_1750_HofRathsOrdnung.xml (2361 tokens)\n",
      "Processing GATE XML files:  54%|▌| 180/336 [01:13<01:04,  2.41file/s, file=LEGA_2025-06-15 23:51:38,483 - INFO - ✓ Processed: LEGA_P3_NoD_1757_Rostock.xml (2462 tokens)\n",
      "Processing GATE XML files:  54%|▌| 181/336 [01:13<01:04,  2.40file/s, file=LEGA_2025-06-15 23:51:39,034 - INFO - ✓ Processed: LEGA_P3_OMD_1767_ProcessOrdnung.xml (2441 tokens)\n",
      "Processing GATE XML files:  54%|▌| 182/336 [01:14<01:10,  2.19file/s, file=LEGA_2025-06-15 23:51:39,460 - INFO - ✓ Processed: LEGA_P3_OMD_1777_MuehlenOrdnung.xml (2414 tokens)\n",
      "Processing GATE XML files:  54%|▌| 183/336 [01:14<01:08,  2.23file/s, file=LEGA_2025-06-15 23:51:39,888 - INFO - ✓ Processed: LEGA_P3_WMD_1756_StaatsArchiv.xml (2367 tokens)\n",
      "Processing GATE XML files:  55%|▌| 184/336 [01:15<01:07,  2.26file/s, file=LEGA_2025-06-15 23:51:40,301 - INFO - ✓ Processed: LEGA_P3_WOD_1791_Muenze.xml (2427 tokens)\n",
      "Processing GATE XML files:  55%|▌| 185/336 [01:15<01:05,  2.31file/s, file=LEGA_2025-06-15 23:51:40,702 - INFO - ✓ Processed: LEGA_P3_OMD_1784_Erbstatuten.xml (2360 tokens)\n",
      "Processing GATE XML files:  55%|▌| 186/336 [01:16<01:03,  2.36file/s, file=LEGA_2025-06-15 23:51:41,107 - INFO - ✓ Processed: LEGA_P3_NoD_1796_Landtage.xml (2279 tokens)\n",
      "Processing GATE XML files:  56%|▌| 187/336 [01:16<01:02,  2.39file/s, file=LEGA_2025-06-15 23:51:41,539 - INFO - ✓ Processed: LEGA_P3_OOD_1752_Gerichtbarkeit.xml (2119 tokens)\n",
      "Processing GATE XML files:  56%|▌| 188/336 [01:16<01:02,  2.37file/s, file=LEGA_2025-06-15 23:51:42,101 - INFO - ✓ Processed: LEGA_P3_WOD_1769_ZunftOrdnungen.xml (2345 tokens)\n",
      "Processing GATE XML files:  56%|▌| 189/336 [01:17<01:08,  2.15file/s, file=LEGA_2025-06-15 23:51:42,488 - INFO - ✓ Processed: LEGA_P3_WMD_1792_Reichshofrath.xml (2267 tokens)\n",
      "Processing GATE XML files:  57%|▌| 190/336 [01:17<01:04,  2.27file/s, file=LEGA_2025-06-15 23:51:42,876 - INFO - ✓ Processed: LEGA_P3_WOD_1787_Cameralrecht.xml (2251 tokens)\n",
      "Processing GATE XML files:  57%|▌| 191/336 [01:18<01:01,  2.35file/s, file=DRAM_2025-06-15 23:51:43,473 - INFO - ✓ Processed: DRAM_P3_NoD_1764_Salomo.xml (3183 tokens)\n",
      "Processing GATE XML files:  57%|▌| 192/336 [01:18<01:08,  2.10file/s, file=DRAM_2025-06-15 23:51:43,883 - INFO - ✓ Processed: DRAM_P3_OOD_1764_Maegera.xml (2398 tokens)\n",
      "Processing GATE XML files:  57%|▌| 193/336 [01:19<01:05,  2.19file/s, file=DRAM_2025-06-15 23:51:44,318 - INFO - ✓ Processed: DRAM_P3_NoD_1767_Minna.xml (2673 tokens)\n",
      "Processing GATE XML files:  58%|▌| 194/336 [01:19<01:03,  2.22file/s, file=DRAM_2025-06-15 23:51:44,833 - INFO - ✓ Processed: DRAM_P3_WMD_1787_Verbrechen.xml (2469 tokens)\n",
      "Processing GATE XML files:  58%|▌| 195/336 [01:20<01:06,  2.13file/s, file=DRAM_2025-06-15 23:51:45,202 - INFO - ✓ Processed: DRAM_P3_OMD_1788_Egmont.xml (2337 tokens)\n",
      "Processing GATE XML files:  58%|▌| 196/336 [01:20<01:01,  2.28file/s, file=DRAM_2025-06-15 23:51:45,727 - INFO - ✓ Processed: DRAM_P3_OMD_1775_LeidendeWeib.xml (2901 tokens)\n",
      "Processing GATE XML files:  59%|▌| 197/336 [01:21<01:04,  2.15file/s, file=DRAM_2025-06-15 23:51:46,170 - INFO - ✓ Processed: DRAM_P3_WOD_1762_Evander.xml (2531 tokens)\n",
      "Processing GATE XML files:  59%|▌| 198/336 [01:21<01:03,  2.18file/s, file=DRAM_2025-06-15 23:51:46,562 - INFO - ✓ Processed: DRAM_P3_OMD_1774_Hofmeister.xml (2320 tokens)\n",
      "Processing GATE XML files:  59%|▌| 199/336 [01:21<01:00,  2.28file/s, file=DRAM_2025-06-15 23:51:47,269 - INFO - ✓ Processed: DRAM_P3_NoD_1776_Zwillinge.xml (2965 tokens)\n",
      "Processing GATE XML files:  60%|▌| 200/336 [01:22<01:10,  1.93file/s, file=DRAM_2025-06-15 23:51:47,643 - INFO - ✓ Processed: DRAM_P3_OOD_1798_Donauweibchen.xml (2242 tokens)\n",
      "Processing GATE XML files:  60%|▌| 201/336 [01:22<01:04,  2.10file/s, file=DRAM_2025-06-15 23:51:48,053 - INFO - ✓ Processed: DRAM_P3_WMD_1780_Hausvater.xml (2353 tokens)\n",
      "Processing GATE XML files:  60%|▌| 202/336 [01:23<01:01,  2.19file/s, file=DRAM_2025-06-15 23:51:48,581 - INFO - ✓ Processed: DRAM_P3_WMD_1773_Goetz.xml (2832 tokens)\n",
      "Processing GATE XML files:  60%|▌| 203/336 [01:23<01:03,  2.09file/s, file=DRAM_2025-06-15 23:51:49,051 - INFO - ✓ Processed: DRAM_P3_WOD_1781_Raeuber.xml (2653 tokens)\n",
      "Processing GATE XML files:  61%|▌| 204/336 [01:24<01:02,  2.10file/s, file=DRAM_2025-06-15 23:51:49,604 - INFO - ✓ Processed: DRAM_P3_OOD_1782_Serail.xml (2740 tokens)\n",
      "Processing GATE XML files:  61%|▌| 205/336 [01:24<01:05,  2.01file/s, file=DRAM_2025-06-15 23:51:50,224 - INFO - ✓ Processed: DRAM_P3_WOD_1783_Elfride.xml (2757 tokens)\n",
      "Processing GATE XML files:  61%|▌| 206/336 [01:25<01:09,  1.87file/s, file=SCIE_2025-06-15 23:51:50,611 - INFO - ✓ Processed: SCIE_P3_NoD_1799_Gasarten.xml (2509 tokens)\n",
      "Processing GATE XML files:  62%|▌| 207/336 [01:25<01:03,  2.04file/s, file=SCIE_2025-06-15 23:51:51,019 - INFO - ✓ Processed: SCIE_P3_WOD_1774_Hygrometrie.xml (2421 tokens)\n",
      "Processing GATE XML files:  62%|▌| 208/336 [01:26<00:59,  2.15file/s, file=SCIE_2025-06-15 23:51:51,410 - INFO - ✓ Processed: SCIE_P3_WOD_1787_Botanik.xml (2307 tokens)\n",
      "Processing GATE XML files:  62%|▌| 209/336 [01:26<00:56,  2.26file/s, file=SCIE_2025-06-15 23:51:51,834 - INFO - ✓ Processed: SCIE_P3_WMD_1777_Logik.xml (2278 tokens)\n",
      "Processing GATE XML files:  62%|▋| 210/336 [01:27<00:55,  2.29file/s, file=SCIE_2025-06-15 23:51:52,275 - INFO - ✓ Processed: SCIE_P3_OOD_1786_Polizey.xml (2431 tokens)\n",
      "Processing GATE XML files:  63%|▋| 211/336 [01:27<00:54,  2.28file/s, file=SCIE_2025-06-15 23:51:52,702 - INFO - ✓ Processed: SCIE_P3_WMD_1781_Forstwissenschaft.xml (2524 tokens)\n",
      "Processing GATE XML files:  63%|▋| 212/336 [01:28<00:53,  2.30file/s, file=SCIE_2025-06-15 23:51:53,153 - INFO - ✓ Processed: SCIE_P3_WOD_1780_Instrument.xml (2288 tokens)\n",
      "Processing GATE XML files:  63%|▋| 213/336 [01:28<00:54,  2.27file/s, file=SCIE_2025-06-15 23:51:53,510 - INFO - ✓ Processed: SCIE_P3_NoD_1775_Chemie.xml (2187 tokens)\n",
      "Processing GATE XML files:  64%|▋| 214/336 [01:28<00:50,  2.41file/s, file=SCIE_2025-06-15 23:51:53,907 - INFO - ✓ Processed: SCIE_P3_OMD_1781_Chymie.xml (2215 tokens)\n",
      "Processing GATE XML files:  64%|▋| 215/336 [01:29<00:49,  2.44file/s, file=SCIE_2025-06-15 23:51:54,354 - INFO - ✓ Processed: SCIE_P3_OOD_1780_ZeichenInstruments.xml (2531 tokens)\n",
      "Processing GATE XML files:  64%|▋| 216/336 [01:29<00:50,  2.38file/s, file=SCIE_2025-06-15 23:51:54,784 - INFO - ✓ Processed: SCIE_P3_OMD_1781_Akademie.xml (2632 tokens)\n",
      "Processing GATE XML files:  65%|▋| 217/336 [01:30<00:50,  2.36file/s, file=SCIE_2025-06-15 23:51:55,209 - INFO - ✓ Processed: SCIE_P3_OMD_1778_MineralogischeGeographie.xml (2472 tokens)\n",
      "Processing GATE XML files:  65%|▋| 218/336 [01:30<00:50,  2.36file/s, file=SCIE_2025-06-15 23:51:55,720 - INFO - ✓ Processed: SCIE_P3_OOD_1788_Chimie.xml (2476 tokens)\n",
      "Processing GATE XML files:  65%|▋| 219/336 [01:31<00:52,  2.22file/s, file=SCIE_2025-06-15 23:51:56,091 - INFO - ✓ Processed: SCIE_P3_NoD_1761_Menschlich.xml (2193 tokens)\n",
      "Processing GATE XML files:  65%|▋| 220/336 [01:31<00:49,  2.35file/s, file=SCIE_2025-06-15 23:51:56,523 - INFO - ✓ Processed: SCIE_P3_WMD_1753_ProbierSteins.xml (2525 tokens)\n",
      "Processing GATE XML files:  66%|▋| 221/336 [01:31<00:49,  2.34file/s, file=HUMA_2025-06-15 23:51:56,966 - INFO - ✓ Processed: HUMA_P1_NoD_1667_Ratseburg.xml (2567 tokens)\n",
      "Processing GATE XML files:  66%|▋| 222/336 [01:32<00:49,  2.31file/s, file=HUMA_2025-06-15 23:51:57,350 - INFO - ✓ Processed: HUMA_P1_WMD_1692_Christus.xml (2432 tokens)\n",
      "Processing GATE XML files:  66%|▋| 223/336 [01:32<00:47,  2.39file/s, file=HUMA_2025-06-15 23:51:57,751 - INFO - ✓ Processed: HUMA_P1_OOD_1680_MercksWienn.xml (2408 tokens)\n",
      "Processing GATE XML files:  67%|▋| 224/336 [01:33<00:46,  2.42file/s, file=HUMA_2025-06-15 23:51:58,431 - INFO - ✓ Processed: HUMA_P1_NoD_1663_HaubtSprache.xml (2862 tokens)\n",
      "Processing GATE XML files:  67%|▋| 225/336 [01:33<00:54,  2.03file/s, file=HUMA_2025-06-15 23:51:58,871 - INFO - ✓ Processed: HUMA_P1_OMD_1654_Rosetum.xml (2474 tokens)\n",
      "Processing GATE XML files:  67%|▋| 226/336 [01:34<00:52,  2.10file/s, file=HUMA_2025-06-15 23:51:59,243 - INFO - ✓ Processed: HUMA_P1_OMD_1685_ChristenStat.xml (2270 tokens)\n",
      "Processing GATE XML files:  68%|▋| 227/336 [01:34<00:48,  2.24file/s, file=HUMA_2025-06-15 23:51:59,660 - INFO - ✓ Processed: HUMA_P1_WMD_1699_KetzerHistorie.xml (2303 tokens)\n",
      "Processing GATE XML files:  68%|▋| 228/336 [01:35<00:47,  2.29file/s, file=HUMA_2025-06-15 23:52:00,097 - INFO - ✓ Processed: HUMA_P1_WOD_1698_Mythoscopia.xml (2500 tokens)\n",
      "Processing GATE XML files:  68%|▋| 229/336 [01:35<00:46,  2.29file/s, file=HUMA_2025-06-15 23:52:00,522 - INFO - ✓ Processed: HUMA_P1_OOD_1689_Crain.xml (2507 tokens)\n",
      "Processing GATE XML files:  68%|▋| 230/336 [01:35<00:45,  2.31file/s, file=HUMA_2025-06-15 23:52:01,056 - INFO - ✓ Processed: HUMA_P1_WOD_1686_Betrachtung.xml (2327 tokens)\n",
      "Processing GATE XML files:  69%|▋| 231/336 [01:36<00:48,  2.16file/s, file=HUMA_2025-06-15 23:52:01,474 - INFO - ✓ Processed: HUMA_P1_OOD_1690_Proteus.xml (2479 tokens)\n",
      "Processing GATE XML files:  69%|▋| 232/336 [01:36<00:46,  2.22file/s, file=HUMA_2025-06-15 23:52:01,877 - INFO - ✓ Processed: HUMA_P1_WOD_1662_Musurgia.xml (2394 tokens)\n",
      "Processing GATE XML files:  69%|▋| 233/336 [01:37<00:44,  2.29file/s, file=HUMA_2025-06-15 23:52:02,306 - INFO - ✓ Processed: HUMA_P1_WMD_1674_BilderSchatz.xml (2365 tokens)\n",
      "Processing GATE XML files:  70%|▋| 234/336 [01:37<00:44,  2.30file/s, file=HUMA_2025-06-15 23:52:02,739 - INFO - ✓ Processed: HUMA_P1_NoD_1674_NaturalienKammer.xml (2488 tokens)\n",
      "Processing GATE XML files:  70%|▋| 235/336 [01:38<00:43,  2.31file/s, file=HUMA_2025-06-15 23:52:03,151 - INFO - ✓ Processed: HUMA_P1_OMD_1680_Bericht.xml (2416 tokens)\n",
      "Processing GATE XML files:  70%|▋| 236/336 [01:38<00:42,  2.34file/s, file=SERM_2025-06-15 23:52:03,582 - INFO - ✓ Processed: SERM_P1_WOD_1660_LeichPredigt.xml (2497 tokens)\n",
      "Processing GATE XML files:  71%|▋| 237/336 [01:38<00:42,  2.34file/s, file=SERM_2025-06-15 23:52:04,174 - INFO - ✓ Processed: SERM_P1_OMD_1680_Balcken.xml (2406 tokens)\n",
      "Processing GATE XML files:  71%|▋| 238/336 [01:39<00:46,  2.09file/s, file=SERM_2025-06-15 23:52:04,632 - INFO - ✓ Processed: SERM_P1_WMD_1674_Trost.xml (2359 tokens)\n",
      "Processing GATE XML files:  71%|▋| 239/336 [01:39<00:45,  2.12file/s, file=SERM_2025-06-15 23:52:05,058 - INFO - ✓ Processed: SERM_P1_WOD_1683_TrostPredigt.xml (2405 tokens)\n",
      "Processing GATE XML files:  71%|▋| 240/336 [01:40<00:43,  2.18file/s, file=SERM_2025-06-15 23:52:05,538 - INFO - ✓ Processed: SERM_P1_OOD_1663_Amaradvlcis.xml (2577 tokens)\n",
      "Processing GATE XML files:  72%|▋| 241/336 [01:40<00:44,  2.15file/s, file=SERM_2025-06-15 23:52:06,070 - INFO - ✓ Processed: SERM_P1_OMD_1680_SursumDeosum.xml (2471 tokens)\n",
      "Processing GATE XML files:  72%|▋| 242/336 [01:41<00:45,  2.06file/s, file=SERM_2025-06-15 23:52:06,471 - INFO - ✓ Processed: SERM_P1_NoD_1677_LeichSermon.xml (2334 tokens)\n",
      "Processing GATE XML files:  72%|▋| 243/336 [01:41<00:42,  2.18file/s, file=SERM_2025-06-15 23:52:07,085 - INFO - ✓ Processed: SERM_P1_OMD_1672_Advent.xml (2360 tokens)\n",
      "Processing GATE XML files:  73%|▋| 244/336 [01:42<00:46,  1.98file/s, file=SERM_2025-06-15 23:52:07,524 - INFO - ✓ Processed: SERM_P1_WMD_1699_Solms.xml (2355 tokens)\n",
      "Processing GATE XML files:  73%|▋| 245/336 [01:42<00:44,  2.06file/s, file=SERM_2025-06-15 23:52:07,902 - INFO - ✓ Processed: SERM_P1_NoD_1666_Erbteil.xml (2274 tokens)\n",
      "Processing GATE XML files:  73%|▋| 246/336 [01:43<00:40,  2.21file/s, file=SERM_2025-06-15 23:52:08,320 - INFO - ✓ Processed: SERM_P1_WOD_1654_Eytelkeit.xml (2367 tokens)\n",
      "Processing GATE XML files:  74%|▋| 247/336 [01:43<00:39,  2.26file/s, file=SERM_2025-06-15 23:52:08,744 - INFO - ✓ Processed: SERM_P1_OOD_1686_KlagseufftzendesAch.xml (2447 tokens)\n",
      "Processing GATE XML files:  74%|▋| 248/336 [01:44<00:38,  2.29file/s, file=SERM_2025-06-15 23:52:09,208 - INFO - ✓ Processed: SERM_P1_NoD_1690_WilleGottes.xml (2491 tokens)\n",
      "Processing GATE XML files:  74%|▋| 249/336 [01:44<00:38,  2.25file/s, file=SERM_2025-06-15 23:52:09,820 - INFO - ✓ Processed: SERM_P1_WMD_1662_Funeralia.xml (2447 tokens)\n",
      "Processing GATE XML files:  74%|▋| 250/336 [01:45<00:42,  2.02file/s, file=SERM_2025-06-15 23:52:10,243 - INFO - ✓ Processed: SERM_P1_OOD_1660_EinweihungsPredigt.xml (2457 tokens)\n",
      "Processing GATE XML files:  75%|▋| 251/336 [01:45<00:40,  2.11file/s, file=NARR_2025-06-15 23:52:10,641 - INFO - ✓ Processed: NARR_P1_OMD_1671_Ruebezahl.xml (2517 tokens)\n",
      "Processing GATE XML files:  75%|▊| 252/336 [01:45<00:37,  2.22file/s, file=NARR_2025-06-15 23:52:11,069 - INFO - ✓ Processed: NARR_P1_NoD_1659_Herkules.xml (2340 tokens)\n",
      "Processing GATE XML files:  75%|▊| 253/336 [01:46<00:36,  2.25file/s, file=NARR_2025-06-15 23:52:11,500 - INFO - ✓ Processed: NARR_P1_WOD_1689_Miranten.xml (2508 tokens)\n",
      "Processing GATE XML files:  76%|▊| 254/336 [01:46<00:36,  2.27file/s, file=NARR_2025-06-15 23:52:11,901 - INFO - ✓ Processed: NARR_P1_WMD_1664_Levante.xml (2396 tokens)\n",
      "Processing GATE XML files:  76%|▊| 255/336 [01:47<00:34,  2.33file/s, file=NARR_2025-06-15 23:52:12,289 - INFO - ✓ Processed: NARR_P1_NoD_1658_Morgenlaendisch.xml (2303 tokens)\n",
      "Processing GATE XML files:  76%|▊| 256/336 [01:47<00:33,  2.40file/s, file=NARR_2025-06-15 23:52:12,860 - INFO - ✓ Processed: NARR_P1_WOD_1672_Melcher.xml (2355 tokens)\n",
      "Processing GATE XML files:  76%|▊| 257/336 [01:48<00:36,  2.16file/s, file=NARR_2025-06-15 23:52:13,245 - INFO - ✓ Processed: NARR_P1_WMD_1696_Schelmuffsky.xml (2302 tokens)\n",
      "Processing GATE XML files:  77%|▊| 258/336 [01:48<00:34,  2.28file/s, file=NARR_2025-06-15 23:52:13,619 - INFO - ✓ Processed: NARR_P1_OMD_1689_Arminius.xml (2235 tokens)\n",
      "Processing GATE XML files:  77%|▊| 259/336 [01:48<00:32,  2.38file/s, file=NARR_2025-06-15 23:52:14,028 - INFO - ✓ Processed: NARR_P1_OOD_1669_Aramena.xml (2507 tokens)\n",
      "Processing GATE XML files:  77%|▊| 260/336 [01:49<00:31,  2.40file/s, file=NARR_2025-06-15 23:52:14,468 - INFO - ✓ Processed: NARR_P1_OOD_1667_Simplicissimus.xml (2385 tokens)\n",
      "Processing GATE XML files:  78%|▊| 261/336 [01:49<00:31,  2.36file/s, file=NARR_2025-06-15 23:52:14,889 - INFO - ✓ Processed: NARR_P1_WMD_1696_DerEdelmann.xml (2448 tokens)\n",
      "Processing GATE XML files:  78%|▊| 262/336 [01:50<00:31,  2.36file/s, file=NARR_2025-06-15 23:52:15,450 - INFO - ✓ Processed: NARR_P1_NoD_1682_Mandorell.xml (2382 tokens)\n",
      "Processing GATE XML files:  78%|▊| 263/336 [01:50<00:33,  2.15file/s, file=NARR_2025-06-15 23:52:15,836 - INFO - ✓ Processed: NARR_P1_WOD_1682_Feuermaeuer.xml (2377 tokens)\n",
      "Processing GATE XML files:  79%|▊| 264/336 [01:51<00:31,  2.27file/s, file=NARR_2025-06-15 23:52:16,233 - INFO - ✓ Processed: NARR_P1_OMD_1700_Banise.xml (2309 tokens)\n",
      "Processing GATE XML files:  79%|▊| 265/336 [01:51<00:30,  2.34file/s, file=NARR_2025-06-15 23:52:16,652 - INFO - ✓ Processed: NARR_P1_OOD_1682_Winternaechte.xml (2509 tokens)\n",
      "Processing GATE XML files:  79%|▊| 266/336 [01:51<00:29,  2.35file/s, file=NEWS_2025-06-15 23:52:16,924 - INFO - ✓ Processed: NEWS_P1_OOD_1684_muenchmerc.xml (1719 tokens)\n",
      "Processing GATE XML files:  79%|▊| 267/336 [01:52<00:26,  2.64file/s, file=NEWS_2025-06-15 23:52:17,104 - INFO - ✓ Processed: NEWS_P1_OMD_1666_leipzig2.xml (1272 tokens)\n",
      "Processing GATE XML files:  80%|▊| 268/336 [01:52<00:21,  3.13file/s, file=NEWS_2025-06-15 23:52:17,344 - INFO - ✓ Processed: NEWS_P1_OMD_1666_leipzig1.xml (1286 tokens)\n",
      "Processing GATE XML files:  80%|▊| 269/336 [01:52<00:19,  3.38file/s, file=NEWS_2025-06-15 23:52:17,562 - INFO - ✓ Processed: NEWS_P1_WOD_1662_strassburg1.xml (1384 tokens)\n",
      "Processing GATE XML files:  80%|▊| 270/336 [01:52<00:17,  3.67file/s, file=NEWS_2025-06-15 23:52:17,915 - INFO - ✓ Processed: NEWS_P1_WOD_1685_lindau.xml (1392 tokens)\n",
      "Processing GATE XML files:  81%|▊| 271/336 [01:53<00:19,  3.37file/s, file=NEWS_2025-06-15 23:52:18,108 - INFO - ✓ Processed: NEWS_P1_WOD_1662_strassburg2.xml (1215 tokens)\n",
      "Processing GATE XML files:  81%|▊| 272/336 [01:53<00:16,  3.77file/s, file=NEWS_2025-06-15 23:52:18,552 - INFO - ✓ Processed: NEWS_P1_NoD_1698_altona.xml (2248 tokens)\n",
      "Processing GATE XML files:  81%|▊| 273/336 [01:53<00:20,  3.13file/s, file=NEWS_2025-06-15 23:52:18,674 - INFO - ✓ Processed: NEWS_P1_OMD_1683_breslau.xml (956 tokens)\n",
      "Processing GATE XML files:  82%|▊| 274/336 [01:54<00:16,  3.85file/s, file=NEWS_2025-06-15 23:52:18,915 - INFO - ✓ Processed: NEWS_P1_OOD_1679_nuernberg1.xml (1466 tokens)\n",
      "Processing GATE XML files:  82%|▊| 275/336 [01:54<00:15,  3.94file/s, file=NEWS_2025-06-15 23:52:19,191 - INFO - ✓ Processed: NEWS_P1_OMD_1684_breslau.xml (1690 tokens)\n",
      "Processing GATE XML files:  82%|▊| 276/336 [01:54<00:15,  3.84file/s, file=NEWS_2025-06-15 23:52:19,448 - INFO - ✓ Processed: NEWS_P1_OOD_1679_nuernberg2.xml (1339 tokens)\n",
      "Processing GATE XML files:  82%|▊| 277/336 [01:54<00:15,  3.85file/s, file=NEWS_2025-06-15 23:52:19,605 - INFO - ✓ Processed: NEWS_P1_OOD_1659_muenchen2.xml (1042 tokens)\n",
      "Processing GATE XML files:  83%|▊| 278/336 [01:54<00:13,  4.37file/s, file=NEWS_2025-06-15 23:52:19,739 - INFO - ✓ Processed: NEWS_P1_OOD_1659_muenchmerc.xml (919 tokens)\n",
      "Processing GATE XML files:  83%|▊| 279/336 [01:55<00:11,  4.99file/s, file=NEWS_2025-06-15 23:52:19,980 - INFO - ✓ Processed: NEWS_P1_WMD_1662_koeln.xml (1489 tokens)\n",
      "Processing GATE XML files:  83%|▊| 280/336 [01:55<00:11,  4.70file/s, file=NEWS_2025-06-15 23:52:20,581 - INFO - ✓ Processed: NEWS_P1_WOD_1681_zuerich.xml (2353 tokens)\n",
      "Processing GATE XML files:  84%|▊| 281/336 [01:55<00:18,  3.04file/s, file=NEWS_2025-06-15 23:52:20,774 - INFO - ✓ Processed: NEWS_P1_OOD_1659_muenchen1.xml (1360 tokens)\n",
      "Processing GATE XML files:  84%|▊| 282/336 [01:56<00:15,  3.47file/s, file=NEWS_2025-06-15 23:52:20,917 - INFO - ✓ Processed: NEWS_P1_NoD_1666_berlin2.xml (1087 tokens)\n",
      "Processing GATE XML files:  84%|▊| 283/336 [01:56<00:12,  4.09file/s, file=NEWS_2025-06-15 23:52:21,261 - INFO - ✓ Processed: NEWS_P1_WMD_1671_frankfurt1.xml (2023 tokens)\n",
      "Processing GATE XML files:  85%|▊| 284/336 [01:56<00:14,  3.64file/s, file=NEWS_2025-06-15 23:52:21,469 - INFO - ✓ Processed: NEWS_P1_NoD_1666_berlin1.xml (1135 tokens)\n",
      "Processing GATE XML files:  85%|▊| 285/336 [01:56<00:12,  3.93file/s, file=NEWS_2025-06-15 23:52:21,940 - INFO - ✓ Processed: NEWS_P1_WMD_1699_koeln.xml (2502 tokens)\n",
      "Processing GATE XML files:  85%|▊| 286/336 [01:57<00:15,  3.13file/s, file=NEWS_2025-06-15 23:52:22,342 - INFO - ✓ Processed: NEWS_P1_NoD_1673_hamburg.xml (2315 tokens)\n",
      "Processing GATE XML files:  85%|▊| 287/336 [01:57<00:16,  2.91file/s, file=NEWS_2025-06-15 23:52:22,457 - INFO - ✓ Processed: NEWS_P1_WMD_1671_frankfurt2.xml (840 tokens)\n",
      "Processing GATE XML files:  86%|▊| 288/336 [01:57<00:13,  3.63file/s, file=NEWS_2025-06-15 23:52:22,886 - INFO - ✓ Processed: NEWS_P1_WOD_1689_lindau.xml (1559 tokens)\n",
      "Processing GATE XML files:  86%|▊| 289/336 [01:58<00:15,  3.11file/s, file=NEWS_2025-06-15 23:52:23,277 - INFO - ✓ Processed: NEWS_P1_OMD_1687_leipzig.xml (2299 tokens)\n",
      "Processing GATE XML files:  86%|▊| 290/336 [01:58<00:15,  2.92file/s, file=NEWS_2025-06-15 23:52:23,406 - INFO - ✓ Processed: NEWS_P1_WMD_1663_koeln.xml (1010 tokens)\n",
      "Processing GATE XML files:  87%|▊| 291/336 [01:58<00:12,  3.59file/s, file=LEGA_2025-06-15 23:52:23,844 - INFO - ✓ Processed: LEGA_P1_WOD_1654_HoffgerichtsOrdnung.xml (2619 tokens)\n",
      "Processing GATE XML files:  87%|▊| 292/336 [01:59<00:14,  3.06file/s, file=LEGA_2025-06-15 23:52:24,220 - INFO - ✓ Processed: LEGA_P1_WOD_1698_LandsOrdnung.xml (2230 tokens)\n",
      "Processing GATE XML files:  87%|▊| 293/336 [01:59<00:14,  2.93file/s, file=LEGA_2025-06-15 23:52:24,628 - INFO - ✓ Processed: LEGA_P1_WOD_1683_Ulm.xml (2437 tokens)\n",
      "Processing GATE XML files:  88%|▉| 294/336 [01:59<00:15,  2.77file/s, file=LEGA_2025-06-15 23:52:25,048 - INFO - ✓ Processed: LEGA_P1_OOD_1659_SchulOrdnung.xml (2458 tokens)\n",
      "Processing GATE XML files:  88%|▉| 295/336 [02:00<00:15,  2.64file/s, file=LEGA_2025-06-15 23:52:25,517 - INFO - ✓ Processed: LEGA_P1_OOD_1657_Wildtfang.xml (2370 tokens)\n",
      "Processing GATE XML files:  88%|▉| 296/336 [02:00<00:16,  2.46file/s, file=LEGA_2025-06-15 23:52:25,977 - INFO - ✓ Processed: LEGA_P1_NoD_1657_Luebeck.xml (2472 tokens)\n",
      "Processing GATE XML files:  88%|▉| 297/336 [02:01<00:16,  2.37file/s, file=LEGA_2025-06-15 23:52:26,403 - INFO - ✓ Processed: LEGA_P1_NoD_1673_BergOrdnung.xml (2519 tokens)\n",
      "Processing GATE XML files:  89%|▉| 298/336 [02:01<00:16,  2.36file/s, file=LEGA_2025-06-15 23:52:26,803 - INFO - ✓ Processed: LEGA_P1_WMD_1698_BergkRecht.xml (2365 tokens)\n",
      "Processing GATE XML files:  89%|▉| 299/336 [02:02<00:15,  2.40file/s, file=LEGA_2025-06-15 23:52:27,260 - INFO - ✓ Processed: LEGA_P1_OOD_1700_GesetzNuernberg.xml (2532 tokens)\n",
      "Processing GATE XML files:  89%|▉| 300/336 [02:02<00:15,  2.33file/s, file=LEGA_2025-06-15 23:52:27,669 - INFO - ✓ Processed: LEGA_P1_OMD_1680_Dreszden.xml (2397 tokens)\n",
      "Processing GATE XML files:  90%|▉| 301/336 [02:03<00:14,  2.37file/s, file=LEGA_2025-06-15 23:52:28,259 - INFO - ✓ Processed: LEGA_P1_WMD_1700_LandRecht.xml (2593 tokens)\n",
      "Processing GATE XML files:  90%|▉| 302/336 [02:03<00:16,  2.12file/s, file=LEGA_2025-06-15 23:52:28,675 - INFO - ✓ Processed: LEGA_P1_WMD_1694_RathsSatzung.xml (2365 tokens)\n",
      "Processing GATE XML files:  90%|▉| 303/336 [02:04<00:15,  2.19file/s, file=LEGA_2025-06-15 23:52:29,126 - INFO - ✓ Processed: LEGA_P1_NoD_1695_Duesseldorff.xml (2409 tokens)\n",
      "Processing GATE XML files:  90%|▉| 304/336 [02:04<00:14,  2.20file/s, file=LEGA_2025-06-15 23:52:29,672 - INFO - ✓ Processed: LEGA_P1_OMD_1659_Hexen.xml (3081 tokens)\n",
      "Processing GATE XML files:  91%|▉| 305/336 [02:05<00:14,  2.08file/s, file=LEGA_2025-06-15 23:52:30,074 - INFO - ✓ Processed: LEGA_P1_OMD_1674_BergOrdnung.xml (2442 tokens)\n",
      "Processing GATE XML files:  91%|▉| 306/336 [02:05<00:13,  2.18file/s, file=DRAM_2025-06-15 23:52:30,527 - INFO - ✓ Processed: DRAM_P1_OOD_1675_Pirrus.xml (2556 tokens)\n",
      "Processing GATE XML files:  91%|▉| 307/336 [02:05<00:13,  2.19file/s, file=DRAM_2025-06-15 23:52:31,268 - INFO - ✓ Processed: DRAM_P1_OMD_1657_Cardenio.xml (2895 tokens)\n",
      "Processing GATE XML files:  92%|▉| 308/336 [02:06<00:15,  1.85file/s, file=DRAM_2025-06-15 23:52:31,629 - INFO - ✓ Processed: DRAM_P1_WOD_1687_Joseph.xml (2265 tokens)\n",
      "Processing GATE XML files:  92%|▉| 309/336 [02:06<00:13,  2.05file/s, file=DRAM_2025-06-15 23:52:32,128 - INFO - ✓ Processed: DRAM_P1_WOD_1663_Carle.xml (2790 tokens)\n",
      "Processing GATE XML files:  92%|▉| 310/336 [02:07<00:12,  2.04file/s, file=DRAM_2025-06-15 23:52:32,617 - INFO - ✓ Processed: DRAM_P1_NoD_1673_Leonilda.xml (2697 tokens)\n",
      "Processing GATE XML files:  93%|▉| 311/336 [02:07<00:12,  2.04file/s, file=DRAM_2025-06-15 23:52:33,050 - INFO - ✓ Processed: DRAM_P1_WMD_1670_Comoedianten.xml (2345 tokens)\n",
      "Processing GATE XML files:  93%|▉| 312/336 [02:08<00:11,  2.11file/s, file=DRAM_2025-06-15 23:52:33,481 - INFO - ✓ Processed: DRAM_P1_NoD_1700_Freyheit.xml (2398 tokens)\n",
      "Processing GATE XML files:  93%|▉| 313/336 [02:08<00:10,  2.17file/s, file=DRAM_2025-06-15 23:52:34,144 - INFO - ✓ Processed: DRAM_P1_OMD_1661_Cleopatra.xml (2949 tokens)\n",
      "Processing GATE XML files:  93%|▉| 314/336 [02:09<00:11,  1.92file/s, file=DRAM_2025-06-15 23:52:34,547 - INFO - ✓ Processed: DRAM_P1_WMD_1662_Tomyris.xml (2319 tokens)\n",
      "Processing GATE XML files:  94%|▉| 315/336 [02:09<00:10,  2.06file/s, file=DRAM_2025-06-15 23:52:35,158 - INFO - ✓ Processed: DRAM_P1_OOD_1682_Abraham.xml (3000 tokens)\n",
      "Processing GATE XML files:  94%|▉| 316/336 [02:10<00:10,  1.91file/s, file=DRAM_2025-06-15 23:52:35,615 - INFO - ✓ Processed: DRAM_P1_OMD_1683_Masaniello.xml (2481 tokens)\n",
      "Processing GATE XML files:  94%|▉| 317/336 [02:10<00:09,  1.99file/s, file=DRAM_2025-06-15 23:52:36,046 - INFO - ✓ Processed: DRAM_P1_WMD_1668_ChristRuehmendes.xml (2331 tokens)\n",
      "Processing GATE XML files:  95%|▉| 318/336 [02:11<00:08,  2.08file/s, file=DRAM_2025-06-15 23:52:36,658 - INFO - ✓ Processed: DRAM_P1_OOD_1682_LiebesSig.xml (2452 tokens)\n",
      "Processing GATE XML files:  95%|▉| 319/336 [02:12<00:08,  1.92file/s, file=DRAM_2025-06-15 23:52:37,175 - INFO - ✓ Processed: DRAM_P1_WOD_1699_LiebesStreit.xml (2562 tokens)\n",
      "Processing GATE XML files:  95%|▉| 320/336 [02:12<00:08,  1.92file/s, file=DRAM_2025-06-15 23:52:37,614 - INFO - ✓ Processed: DRAM_P1_NoD_1699_Euridice.xml (2509 tokens)\n",
      "Processing GATE XML files:  96%|▉| 321/336 [02:12<00:07,  2.02file/s, file=SCIE_2025-06-15 23:52:38,046 - INFO - ✓ Processed: SCIE_P1_OOD_1681_CometenGespoetts.xml (2535 tokens)\n",
      "Processing GATE XML files:  96%|▉| 322/336 [02:13<00:06,  2.10file/s, file=SCIE_2025-06-15 23:52:38,488 - INFO - ✓ Processed: SCIE_P1_NoD_1680_ConsiliumMedicum.xml (2595 tokens)\n",
      "Processing GATE XML files:  96%|▉| 323/336 [02:13<00:06,  2.15file/s, file=SCIE_2025-06-15 23:52:38,922 - INFO - ✓ Processed: SCIE_P1_WMD_1676_ArtzneyBuch.xml (2453 tokens)\n",
      "Processing GATE XML files:  96%|▉| 324/336 [02:14<00:05,  2.19file/s, file=SCIE_2025-06-15 23:52:39,548 - INFO - ✓ Processed: SCIE_P1_OOD_1689_PferdKunst.xml (2616 tokens)\n",
      "Processing GATE XML files:  97%|▉| 325/336 [02:14<00:05,  1.97file/s, file=SCIE_2025-06-15 23:52:39,985 - INFO - ✓ Processed: SCIE_P1_OMD_1700_BergBau.xml (2565 tokens)\n",
      "Processing GATE XML files:  97%|▉| 326/336 [02:15<00:04,  2.06file/s, file=SCIE_2025-06-15 23:52:40,371 - INFO - ✓ Processed: SCIE_P1_OMD_1664_StraussStern.xml (2142 tokens)\n",
      "Processing GATE XML files:  97%|▉| 327/336 [02:15<00:04,  2.19file/s, file=SCIE_2025-06-15 23:52:40,783 - INFO - ✓ Processed: SCIE_P1_WOD_1693_Kranckheit.xml (2412 tokens)\n",
      "Processing GATE XML files:  98%|▉| 328/336 [02:16<00:03,  2.26file/s, file=SCIE_2025-06-15 23:52:41,200 - INFO - ✓ Processed: SCIE_P1_WMD_1687_ArtzneyKunst.xml (2390 tokens)\n",
      "Processing GATE XML files:  98%|▉| 329/336 [02:16<00:03,  2.30file/s, file=SCIE_2025-06-15 23:52:41,670 - INFO - ✓ Processed: SCIE_P1_WOD_1663_KunstSpiegel.xml (2521 tokens)\n",
      "Processing GATE XML files:  98%|▉| 330/336 [02:17<00:02,  2.24file/s, file=SCIE_2025-06-15 23:52:42,275 - INFO - ✓ Processed: SCIE_P1_OMD_1672_Handwercke.xml (2620 tokens)\n",
      "Processing GATE XML files:  99%|▉| 331/336 [02:17<00:02,  2.03file/s, file=SCIE_2025-06-15 23:52:42,667 - INFO - ✓ Processed: SCIE_P1_WMD_1680_Epidemica.xml (2367 tokens)\n",
      "Processing GATE XML files:  99%|▉| 332/336 [02:18<00:01,  2.16file/s, file=SCIE_2025-06-15 23:52:43,093 - INFO - ✓ Processed: SCIE_P1_WOD_1665_Cometen.xml (2456 tokens)\n",
      "Processing GATE XML files:  99%|▉| 333/336 [02:18<00:01,  2.21file/s, file=SCIE_2025-06-15 23:52:43,337 - INFO - ✓ Processed: SCIE_P1_NoD_1684_Durchfall.xml (1561 tokens)\n",
      "Processing GATE XML files:  99%|▉| 334/336 [02:18<00:00,  2.57file/s, file=SCIE_2025-06-15 23:52:43,724 - INFO - ✓ Processed: SCIE_P1_NoD_1672_Prognosticis.xml (2321 tokens)\n",
      "Processing GATE XML files: 100%|▉| 335/336 [02:19<00:00,  2.57file/s, file=SCIE_2025-06-15 23:52:44,222 - INFO - ✓ Processed: SCIE_P1_OOD_1665_Feldmessen.xml (2593 tokens)\n",
      "Processing GATE XML files: 100%|█| 336/336 [02:19<00:00,  2.41file/s, file=SCIE_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Saved to /Users/rohan/Downloads/2544/processed_germanc/\n",
      "   - documents.json (336 docs)\n",
      "   - tokens.csv (774373 tokens)\n",
      "   - linguistic_features.json\n",
      "   - statistics.json\n",
      "\n",
      "🎉 GATE XML Processing complete!\n",
      "📄 Total files: 336\n",
      "✅ Processed: 336\n",
      "❌ XML errors: 0\n",
      "⚠️ Processing errors: 0\n",
      "📭 Empty files: 0\n",
      "🔤 Total tokens: 774373\n",
      "📚 Total documents: 336\n",
      "🔄 Spelling variants: 115929\n",
      "📜 Archaic spellings: 117413 (15.2%)\n",
      "\n",
      "📊 GATE XML Preprocessing complete!\n",
      "Ready for Phase 3: Database creation\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GerManC GATE XML Preprocessor - Fixed Version\n",
    "Properly handles GATE XML format with TextWithNodes and Annotation elements.\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class GateXMLPreprocessor:\n",
    "    def __init__(self, organized_dir, output_dir):\n",
    "        self.organized_dir = Path(organized_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize data containers\n",
    "        self.documents = []\n",
    "        self.tokens = []\n",
    "        self.linguistic_features = defaultdict(list)\n",
    "        \n",
    "        # Initialize logging\n",
    "        self.setup_logging()\n",
    "        self.stats = {\n",
    "            'total_files': 0,\n",
    "            'processed': 0,\n",
    "            'xml_errors': 0,\n",
    "            'processing_errors': 0,\n",
    "            'empty_files': 0\n",
    "        }\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging for error tracking\"\"\"\n",
    "        log_file = self.output_dir / f\"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file, encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def process_all_files(self):\n",
    "        \"\"\"Process all organized XML files\"\"\"\n",
    "        # Collect all XML files\n",
    "        xml_files = []\n",
    "        for period_dir in self.organized_dir.iterdir():\n",
    "            if period_dir.is_dir():\n",
    "                period = period_dir.name\n",
    "                for genre_dir in period_dir.iterdir():\n",
    "                    if genre_dir.is_dir():\n",
    "                        genre = genre_dir.name\n",
    "                        for xml_file in genre_dir.glob(\"*.xml\"):\n",
    "                            xml_files.append((xml_file, period, genre))\n",
    "        \n",
    "        self.stats['total_files'] = len(xml_files)\n",
    "        \n",
    "        # Process with progress bar\n",
    "        with tqdm(xml_files, desc=\"Processing GATE XML files\", unit=\"file\") as pbar:\n",
    "            for xml_file, period, genre in pbar:\n",
    "                pbar.set_postfix(file=xml_file.name[:30])\n",
    "                \n",
    "                try:\n",
    "                    doc_data = self.process_gate_xml(xml_file, period, genre)\n",
    "                    if doc_data and doc_data['tokens']:\n",
    "                        self.documents.append(doc_data)\n",
    "                        self.stats['processed'] += 1\n",
    "                        self.logger.info(f\"✓ Processed: {xml_file.name} ({len(doc_data['tokens'])} tokens)\")\n",
    "                    else:\n",
    "                        self.stats['empty_files'] += 1\n",
    "                        self.logger.warning(f\"No tokens found in: {xml_file.name}\")\n",
    "                        \n",
    "                except ET.ParseError as e:\n",
    "                    self.stats['xml_errors'] += 1\n",
    "                    self.logger.error(f\"XML Error in {xml_file.name}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.stats['processing_errors'] += 1\n",
    "                    self.logger.error(f\"Processing Error in {xml_file.name}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Save processed data\n",
    "        self.save_processed_data()\n",
    "        \n",
    "        # Print summary\n",
    "        self.print_summary()\n",
    "        \n",
    "        return self.stats\n",
    "    \n",
    "    def process_gate_xml(self, xml_path, period, genre):\n",
    "        \"\"\"Process GATE XML file with TextWithNodes structure\"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Extract document metadata\n",
    "            doc_id = xml_path.stem\n",
    "            doc_data = {\n",
    "                'doc_id': doc_id,\n",
    "                'period': period,\n",
    "                'genre': genre,\n",
    "                'filename': xml_path.name,\n",
    "                'year': self.extract_year(xml_path.name),\n",
    "                'region': self.extract_region(xml_path.name),\n",
    "                'text': '',\n",
    "                'tokens': [],\n",
    "                'sentences': [],\n",
    "                'token_count': 0,\n",
    "                'unique_words': set()\n",
    "            }\n",
    "            \n",
    "            # Extract text with nodes\n",
    "            text_with_nodes = root.find('.//TextWithNodes')\n",
    "            if text_with_nodes is None:\n",
    "                self.logger.warning(f\"No TextWithNodes found in {xml_path.name}\")\n",
    "                return None\n",
    "            \n",
    "            # Parse text and node positions\n",
    "            text_content, node_positions = self.parse_text_with_nodes(text_with_nodes)\n",
    "            doc_data['text'] = text_content\n",
    "            \n",
    "            # Extract annotations\n",
    "            annotation_set = root.find('.//AnnotationSet')\n",
    "            annotations = {}\n",
    "            if annotation_set is not None:\n",
    "                annotations = self.parse_annotations(annotation_set)\n",
    "            \n",
    "            # Process tokens using annotations and text\n",
    "            tokens, sentences = self.extract_tokens_and_sentences(\n",
    "                text_content, node_positions, annotations, doc_id, period, genre\n",
    "            )\n",
    "            \n",
    "            doc_data['tokens'] = tokens\n",
    "            doc_data['sentences'] = sentences\n",
    "            doc_data['token_count'] = len(tokens)\n",
    "            doc_data['unique_words'] = len(set(token['normalized'].lower() for token in tokens if token['normalized']))\n",
    "            \n",
    "            # Add tokens to global list\n",
    "            self.tokens.extend(tokens)\n",
    "            \n",
    "            return doc_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {xml_path.name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def parse_text_with_nodes(self, text_with_nodes_elem):\n",
    "        \"\"\"Parse TextWithNodes element to extract text and node positions\"\"\"\n",
    "        text_parts = []\n",
    "        node_positions = {}\n",
    "        \n",
    "        # Process all text and node elements\n",
    "        for elem in text_with_nodes_elem:\n",
    "            if elem.tag == 'Node':\n",
    "                node_id = elem.get('id')\n",
    "                if node_id:\n",
    "                    current_pos = len(''.join(text_parts))\n",
    "                    node_positions[int(node_id)] = current_pos\n",
    "            elif elem.text:\n",
    "                text_parts.append(elem.text)\n",
    "            \n",
    "            # Also handle tail text after nodes\n",
    "            if elem.tail:\n",
    "                text_parts.append(elem.tail)\n",
    "        \n",
    "        # Handle text directly in TextWithNodes\n",
    "        if text_with_nodes_elem.text:\n",
    "            text_parts.insert(0, text_with_nodes_elem.text)\n",
    "        \n",
    "        full_text = ''.join(text_parts)\n",
    "        return full_text, node_positions\n",
    "    \n",
    "    def parse_annotations(self, annotation_set):\n",
    "        \"\"\"Parse annotation elements to extract linguistic features\"\"\"\n",
    "        annotations = {}\n",
    "        \n",
    "        for annotation in annotation_set.findall('Annotation'):\n",
    "            ann_id = annotation.get('Id')\n",
    "            ann_type = annotation.get('Type')\n",
    "            start_node = annotation.get('StartNode')\n",
    "            end_node = annotation.get('EndNode')\n",
    "            \n",
    "            if not all([ann_id, ann_type, start_node, end_node]):\n",
    "                continue\n",
    "            \n",
    "            # Extract features\n",
    "            features = {}\n",
    "            for feature in annotation.findall('Feature'):\n",
    "                name_elem = feature.find('Name')\n",
    "                value_elem = feature.find('Value')\n",
    "                \n",
    "                if name_elem is not None and value_elem is not None:\n",
    "                    feature_name = name_elem.text\n",
    "                    feature_value = value_elem.text\n",
    "                    features[feature_name] = feature_value\n",
    "            \n",
    "            annotations[ann_id] = {\n",
    "                'type': ann_type,\n",
    "                'start_node': int(start_node),\n",
    "                'end_node': int(end_node),\n",
    "                'features': features\n",
    "            }\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    def extract_tokens_and_sentences(self, text, node_positions, annotations, doc_id, period, genre):\n",
    "        \"\"\"Extract tokens and sentences from text and annotations\"\"\"\n",
    "        tokens = []\n",
    "        sentences = []\n",
    "        \n",
    "        # Sort annotations by start position\n",
    "        token_annotations = []\n",
    "        for ann_id, ann in annotations.items():\n",
    "            if ann['type'] == 'Token':\n",
    "                token_annotations.append((ann['start_node'], ann['end_node'], ann['features'], ann_id))\n",
    "        \n",
    "        token_annotations.sort(key=lambda x: x[0])  # Sort by start node\n",
    "        \n",
    "        current_sentence = []\n",
    "        sentence_id = 0\n",
    "        token_id = 0\n",
    "        \n",
    "        for start_node, end_node, features, ann_id in token_annotations:\n",
    "            # Get text span using node positions\n",
    "            start_pos = node_positions.get(start_node, 0)\n",
    "            end_pos = node_positions.get(end_node, len(text))\n",
    "            \n",
    "            if start_pos < len(text) and end_pos <= len(text) and start_pos < end_pos:\n",
    "                token_text = text[start_pos:end_pos].strip()\n",
    "                \n",
    "                if not token_text:\n",
    "                    continue\n",
    "                \n",
    "                # Extract token features\n",
    "                token_data = self.create_token_data(\n",
    "                    token_text, features, doc_id, sentence_id, token_id, period, genre\n",
    "                )\n",
    "                \n",
    "                if token_data:\n",
    "                    tokens.append(token_data)\n",
    "                    current_sentence.append(token_data)\n",
    "                    token_id += 1\n",
    "                    \n",
    "                    # Check for sentence boundaries (simple heuristic)\n",
    "                    if self.is_sentence_end(token_text):\n",
    "                        if current_sentence:\n",
    "                            sentence_text = ' '.join(t['original'] for t in current_sentence)\n",
    "                            sentences.append({\n",
    "                                'sentence_id': sentence_id,\n",
    "                                'text': sentence_text,\n",
    "                                'tokens': current_sentence.copy(),\n",
    "                                'token_count': len(current_sentence)\n",
    "                            })\n",
    "                            current_sentence = []\n",
    "                            sentence_id += 1\n",
    "        \n",
    "        # Handle remaining tokens as final sentence\n",
    "        if current_sentence:\n",
    "            sentence_text = ' '.join(t['original'] for t in current_sentence)\n",
    "            sentences.append({\n",
    "                'sentence_id': sentence_id,\n",
    "                'text': sentence_text,\n",
    "                'tokens': current_sentence,\n",
    "                'token_count': len(current_sentence)\n",
    "            })\n",
    "        \n",
    "        return tokens, sentences\n",
    "    \n",
    "    def create_token_data(self, token_text, features, doc_id, sent_id, token_id, period, genre):\n",
    "        \"\"\"Create token data structure from text and features\"\"\"\n",
    "        \n",
    "        # Get linguistic features\n",
    "        original = features.get('string', token_text)\n",
    "        normalized = features.get('norm', original)\n",
    "        lemma = features.get('lemma', normalized)\n",
    "        pos = features.get('pos', '')\n",
    "        morph = features.get('morph', '')\n",
    "        \n",
    "        # Skip very short or empty tokens\n",
    "        if len(original.strip()) == 0:\n",
    "            return None\n",
    "        \n",
    "        token_data = {\n",
    "            'doc_id': doc_id,\n",
    "            'sentence_id': sent_id,\n",
    "            'token_id': token_id,\n",
    "            'period': period,\n",
    "            'genre': genre,\n",
    "            'original': original.strip(),\n",
    "            'normalized': normalized.strip() if normalized else original.strip(),\n",
    "            'lemma': lemma.strip() if lemma else normalized.strip() if normalized else original.strip(),\n",
    "            'pos': pos,\n",
    "            'morphology': morph,\n",
    "            'is_spelling_variant': original.lower() != normalized.lower() if normalized else False,\n",
    "            'word_length': len(original.strip()),\n",
    "            'has_archaic_spelling': self.is_archaic_spelling(original),\n",
    "            'is_punctuation': self.is_punctuation(original)\n",
    "        }\n",
    "        \n",
    "        # Track linguistic changes\n",
    "        if token_data['is_spelling_variant']:\n",
    "            self.linguistic_features['spelling_variants'].append({\n",
    "                'original': original,\n",
    "                'normalized': normalized,\n",
    "                'lemma': lemma,\n",
    "                'period': period,\n",
    "                'genre': genre,\n",
    "                'pos': pos\n",
    "            })\n",
    "        \n",
    "        return token_data\n",
    "    \n",
    "    def is_sentence_end(self, token):\n",
    "        \"\"\"Simple sentence boundary detection\"\"\"\n",
    "        return token.strip() in '.!?;'\n",
    "    \n",
    "    def is_punctuation(self, token):\n",
    "        \"\"\"Check if token is punctuation\"\"\"\n",
    "        return all(c in '.,!?;:()[]{}\"\\'-' for c in token.strip())\n",
    "    \n",
    "    def is_archaic_spelling(self, word):\n",
    "        \"\"\"Identify archaic spelling patterns in Early New High German\"\"\"\n",
    "        if not word or len(word) < 2:\n",
    "            return False\n",
    "            \n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        # Common archaic patterns in Early New High German\n",
    "        archaic_patterns = [\n",
    "            r'.*uo.*',      # uo diphthong (guot -> gut)\n",
    "            r'.*ie.*',      # ie for long i (liebe)\n",
    "            r'.*ey.*',      # ey/ei variations\n",
    "            r'.*ck$',       # archaic endings\n",
    "            r'^v[aeiou]',   # v- beginnings (vmb -> um)\n",
    "            r'.*th.*',      # th spellings (thun -> tun)\n",
    "            r'.*umb$',      # umb endings (vmb -> um)\n",
    "            r'.*tz$',       # tz endings\n",
    "            r'.*ff.*',      # double f\n",
    "            r'.*ss.*',      # double s patterns\n",
    "            r'.*ů.*',       # archaic u with circle\n",
    "            r'.*ä.*',       # archaic a-umlaut forms\n",
    "            r'.*ö.*',       # archaic o-umlaut forms\n",
    "            r'.*ü.*',       # archaic u-umlaut forms\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, word_lower) for pattern in archaic_patterns)\n",
    "    \n",
    "    def extract_year(self, filename):\n",
    "        \"\"\"Extract year from filename\"\"\"\n",
    "        match = re.search(r'_(\\d{4})_', filename)\n",
    "        return int(match.group(1)) if match else None\n",
    "    \n",
    "    def extract_region(self, filename):\n",
    "        \"\"\"Extract region code from filename\"\"\"\n",
    "        match = re.search(r'_([A-Za-z]+)_\\d{4}_', filename)\n",
    "        return match.group(1) if match else None\n",
    "    \n",
    "    def save_processed_data(self):\n",
    "        \"\"\"Save all processed data in multiple formats\"\"\"\n",
    "        \n",
    "        # 1. Save documents metadata as JSON\n",
    "        docs_file = self.output_dir / \"documents.json\"\n",
    "        with open(docs_file, 'w', encoding='utf-8') as f:\n",
    "            # Convert sets to lists for JSON serialization\n",
    "            docs_for_json = []\n",
    "            for doc in self.documents:\n",
    "                doc_copy = doc.copy()\n",
    "                if 'unique_words' in doc_copy and isinstance(doc_copy['unique_words'], set):\n",
    "                    doc_copy['unique_words'] = list(doc_copy['unique_words'])\n",
    "                docs_for_json.append(doc_copy)\n",
    "            json.dump(docs_for_json, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 2. Save tokens as CSV for analysis\n",
    "        if self.tokens:\n",
    "            tokens_df = pd.DataFrame(self.tokens)\n",
    "            tokens_file = self.output_dir / \"tokens.csv\"\n",
    "            tokens_df.to_csv(tokens_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        # 3. Save linguistic features\n",
    "        features_file = self.output_dir / \"linguistic_features.json\"\n",
    "        with open(features_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dict(self.linguistic_features), f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # 4. Save summary statistics\n",
    "        self.save_statistics()\n",
    "        \n",
    "        print(f\"📁 Saved to {self.output_dir}/\")\n",
    "        print(f\"   - documents.json ({len(self.documents)} docs)\")\n",
    "        print(f\"   - tokens.csv ({len(self.tokens)} tokens)\")\n",
    "        print(f\"   - linguistic_features.json\")\n",
    "        print(f\"   - statistics.json\")\n",
    "    \n",
    "    def save_statistics(self):\n",
    "        \"\"\"Generate and save corpus statistics\"\"\"\n",
    "        \n",
    "        if not self.documents:\n",
    "            return\n",
    "        \n",
    "        # Period distribution\n",
    "        period_stats = defaultdict(int)\n",
    "        for doc in self.documents:\n",
    "            period_stats[doc['period']] += 1\n",
    "        \n",
    "        # Genre distribution\n",
    "        genre_stats = defaultdict(int)\n",
    "        for doc in self.documents:\n",
    "            genre_stats[doc['genre']] += 1\n",
    "        \n",
    "        # Year distribution\n",
    "        year_stats = defaultdict(int)\n",
    "        for doc in self.documents:\n",
    "            if doc.get('year'):\n",
    "                year_stats[doc['year']] += 1\n",
    "        \n",
    "        # Spelling variants by period\n",
    "        variants_by_period = defaultdict(int)\n",
    "        for variant in self.linguistic_features['spelling_variants']:\n",
    "            variants_by_period[variant['period']] += 1\n",
    "        \n",
    "        # Token statistics\n",
    "        pos_distribution = defaultdict(int)\n",
    "        archaic_count = 0\n",
    "        \n",
    "        for token in self.tokens:\n",
    "            if token.get('pos'):\n",
    "                pos_distribution[token['pos']] += 1\n",
    "            if token.get('has_archaic_spelling'):\n",
    "                archaic_count += 1\n",
    "        \n",
    "        stats = {\n",
    "            'processing_summary': self.stats,\n",
    "            'corpus_stats': {\n",
    "                'total_documents': len(self.documents),\n",
    "                'total_tokens': len(self.tokens),\n",
    "                'total_sentences': sum(len(doc.get('sentences', [])) for doc in self.documents),\n",
    "                'average_tokens_per_doc': len(self.tokens) / len(self.documents) if self.documents else 0,\n",
    "                'average_sentences_per_doc': sum(len(doc.get('sentences', [])) for doc in self.documents) / len(self.documents) if self.documents else 0\n",
    "            },\n",
    "            'temporal_distribution': {\n",
    "                'period_distribution': dict(period_stats),\n",
    "                'year_distribution': dict(sorted(year_stats.items()))\n",
    "            },\n",
    "            'genre_distribution': dict(genre_stats),\n",
    "            'linguistic_features': {\n",
    "                'pos_distribution': dict(pos_distribution),\n",
    "                'spelling_variants_total': len(self.linguistic_features['spelling_variants']),\n",
    "                'variants_by_period': dict(variants_by_period),\n",
    "                'archaic_spelling_count': archaic_count,\n",
    "                'spelling_variant_rate': len(self.linguistic_features['spelling_variants']) / len(self.tokens) if self.tokens else 0,\n",
    "                'archaic_spelling_rate': archaic_count / len(self.tokens) if self.tokens else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stats_file = self.output_dir / \"statistics.json\"\n",
    "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(f\"\\n🎉 GATE XML Processing complete!\")\n",
    "        print(f\"📄 Total files: {self.stats['total_files']}\")\n",
    "        print(f\"✅ Processed: {self.stats['processed']}\")\n",
    "        print(f\"❌ XML errors: {self.stats['xml_errors']}\")\n",
    "        print(f\"⚠️ Processing errors: {self.stats['processing_errors']}\")\n",
    "        print(f\"📭 Empty files: {self.stats['empty_files']}\")\n",
    "        print(f\"🔤 Total tokens: {len(self.tokens)}\")\n",
    "        print(f\"📚 Total documents: {len(self.documents)}\")\n",
    "        \n",
    "        if self.linguistic_features['spelling_variants']:\n",
    "            print(f\"🔄 Spelling variants: {len(self.linguistic_features['spelling_variants'])}\")\n",
    "            \n",
    "        if self.tokens:\n",
    "            archaic_count = sum(1 for t in self.tokens if t.get('has_archaic_spelling'))\n",
    "            print(f\"📜 Archaic spellings: {archaic_count} ({archaic_count/len(self.tokens)*100:.1f}%)\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run GATE XML preprocessing\"\"\"\n",
    "    # Update these paths to match your setup\n",
    "    organized_dir = \"/Users/rohan/Downloads/2544/organized_germanc\"  # From Phase 1\n",
    "    output_dir = \"/Users/rohan/Downloads/2544/processed_germanc\"     # Phase 2 output\n",
    "    \n",
    "    processor = GateXMLPreprocessor(organized_dir, output_dir)\n",
    "    stats = processor.process_all_files()\n",
    "    \n",
    "    print(f\"\\n📊 GATE XML Preprocessing complete!\")\n",
    "    print(f\"Ready for Phase 3: Database creation\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb2608-5934-4043-9b33-4c092d087915",
   "metadata": {},
   "source": [
    "# Preprocessing Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7982c74b-9248-4337-a928-0c7dc69574e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 GerManC Preprocessing Validation\n",
      "==================================\n",
      "🔍 Running GerManC Preprocessing Validation Suite...\n",
      "============================================================\n",
      "\n",
      "📅 Validating Temporal Data...\n",
      "   ✓ Periods found: ['1750-1800', '1650-1700', '1700-1750']\n",
      "   ✓ Year range: 1654 - 1799\n",
      "   ✓ Documents with years: 336/336\n",
      "\n",
      "🔤 Validating Spelling Variants...\n",
      "   📝 Testing known spelling changes...\n",
      "      - ck_changes: 4900 examples\n",
      "      - th_to_t: 7573 examples\n",
      "      - uo_to_u: 10 examples\n",
      "      - v_to_u: 1884 examples\n",
      "   ✓ Total variants: 115929\n",
      "   ✓ Valid variants: 115929\n",
      "   ✓ Variant rate: 14.97%\n",
      "   ✓ Archaic patterns found: 22508\n",
      "\n",
      "🏷️ Validating Linguistic Features...\n",
      "   ✓ POS tags found: 62\n",
      "   ✓ Major POS categories: 6/7\n",
      "   ✓ Morphological features: Yes\n",
      "\n",
      "📝 Validating Text Quality...\n",
      "   ✓ Empty documents: 0\n",
      "   ✓ Short documents: 0\n",
      "   ✓ Average token length: 4.8\n",
      "\n",
      "📊 Validating Data Completeness...\n",
      "   ✓ Total tokens: 774373\n",
      "   ✓ Missing essential fields: []\n",
      "   ✓ Genres found: ['Drama', 'Newspapers', 'Humanities', 'Narrative', 'Legal', 'Scientific', 'Sermons']\n",
      "\n",
      "🤖 Validating RAG Readiness...\n",
      "   📊 RAG Readiness Score: 100/100\n",
      "   ✅ READY for RAG pipeline\n",
      "\n",
      "📋 Generating Validation Report...\n",
      "   📄 Report saved to: /Users/rohan/Downloads/2544/processed_germanc/validation_report.json\n",
      "\n",
      "============================================================\n",
      "🎯 VALIDATION SUMMARY\n",
      "============================================================\n",
      "✅ NO CRITICAL ERRORS - Data ready for RAG pipeline!\n",
      "\n",
      "🔍 Testing Sample RAG Queries...\n",
      "   ✅ 'How did 'thun' become 'tun'?' - Can answer with 7632 examples\n",
      "   ✅ 'When did German spelling change?' - Can answer with 7632 examples\n",
      "   ✅ 'What words changed between 1600-1800?' - Can answer with 7632 examples\n",
      "   ✅ 'Show evolution of verb endings' - Can answer with 300 examples\n",
      "   ✅ 'Compare religious vs scientific language' - Can answer with 774373 examples\n",
      "\n",
      "🎉 Validation Complete!\n",
      "Check validation_report.json for detailed results\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GerManC Preprocessing Validation Suite\n",
    "Ensures preprocessing extracts correct data for RAG pipeline\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "\n",
    "class GerManCValidator:\n",
    "    def __init__(self, processed_dir):\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.validation_results = {}\n",
    "        self.critical_errors = []\n",
    "        self.warnings = []\n",
    "        \n",
    "        # Load processed data\n",
    "        self.load_processed_data()\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def load_processed_data(self):\n",
    "        \"\"\"Load all processed data files\"\"\"\n",
    "        try:\n",
    "            # Load documents\n",
    "            with open(self.processed_dir / \"documents.json\", 'r', encoding='utf-8') as f:\n",
    "                self.documents = json.load(f)\n",
    "            \n",
    "            # Load tokens\n",
    "            self.tokens_df = pd.read_csv(self.processed_dir / \"tokens.csv\")\n",
    "            \n",
    "            # Load linguistic features\n",
    "            with open(self.processed_dir / \"linguistic_features.json\", 'r', encoding='utf-8') as f:\n",
    "                self.linguistic_features = json.load(f)\n",
    "            \n",
    "            # Load statistics\n",
    "            with open(self.processed_dir / \"statistics.json\", 'r', encoding='utf-8') as f:\n",
    "                self.statistics = json.load(f)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.critical_errors.append(f\"Failed to load processed data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_full_validation(self):\n",
    "        \"\"\"Run complete validation suite\"\"\"\n",
    "        print(\"🔍 Running GerManC Preprocessing Validation Suite...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Critical validations for RAG pipeline\n",
    "        self.validate_temporal_data()\n",
    "        self.validate_spelling_variants()\n",
    "        self.validate_linguistic_features()\n",
    "        self.validate_text_quality()\n",
    "        self.validate_data_completeness()\n",
    "        self.validate_rag_readiness()\n",
    "        \n",
    "        # Generate validation report\n",
    "        self.generate_validation_report()\n",
    "        \n",
    "        # Create sample queries to test data\n",
    "        self.test_sample_queries()\n",
    "        \n",
    "        return self.validation_results\n",
    "    \n",
    "    def validate_temporal_data(self):\n",
    "        \"\"\"Validate temporal distribution and dating\"\"\"\n",
    "        print(\"\\n📅 Validating Temporal Data...\")\n",
    "        \n",
    "        temporal_issues = []\n",
    "        \n",
    "        # Check if we have documents across time periods\n",
    "        periods = set()\n",
    "        years = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            if doc.get('period'):\n",
    "                periods.add(doc['period'])\n",
    "            if doc.get('year') and doc['year'] > 1500 and doc['year'] < 2000:\n",
    "                years.append(doc['year'])\n",
    "            elif doc.get('year'):\n",
    "                temporal_issues.append(f\"Suspicious year: {doc['year']} in {doc['filename']}\")\n",
    "        \n",
    "        # Validate temporal coverage\n",
    "        if len(periods) < 2:\n",
    "            self.critical_errors.append(\"Insufficient temporal periods - need multiple periods for evolution tracking\")\n",
    "        \n",
    "        if len(years) < len(self.documents) * 0.8:\n",
    "            self.warnings.append(\"Many documents missing year information\")\n",
    "        \n",
    "        # Check temporal distribution\n",
    "        year_range = max(years) - min(years) if years else 0\n",
    "        \n",
    "        self.validation_results['temporal'] = {\n",
    "            'periods_found': list(periods),\n",
    "            'year_range': year_range,\n",
    "            'docs_with_years': len(years),\n",
    "            'temporal_coverage': year_range > 200,  # Should span centuries\n",
    "            'issues': temporal_issues\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Periods found: {list(periods)}\")\n",
    "        print(f\"   ✓ Year range: {min(years) if years else 'N/A'} - {max(years) if years else 'N/A'}\")\n",
    "        print(f\"   ✓ Documents with years: {len(years)}/{len(self.documents)}\")\n",
    "    \n",
    "    def validate_spelling_variants(self):\n",
    "        \"\"\"Validate spelling variant extraction - CRITICAL for RAG\"\"\"\n",
    "        print(\"\\n🔤 Validating Spelling Variants...\")\n",
    "        \n",
    "        spelling_issues = []\n",
    "        variants = self.linguistic_features.get('spelling_variants', [])\n",
    "        \n",
    "        if not variants:\n",
    "            self.critical_errors.append(\"NO SPELLING VARIANTS FOUND - Critical for language evolution tracking!\")\n",
    "            return\n",
    "        \n",
    "        # Check for meaningful variants\n",
    "        valid_variants = 0\n",
    "        archaic_patterns = 0\n",
    "        \n",
    "        for variant in variants:\n",
    "            original = variant.get('original', '')\n",
    "            normalized = variant.get('normalized', '')\n",
    "            \n",
    "            if original and normalized and original != normalized:\n",
    "                valid_variants += 1\n",
    "                \n",
    "                # Check for known archaic patterns\n",
    "                if self.has_archaic_patterns(original):\n",
    "                    archaic_patterns += 1\n",
    "            else:\n",
    "                spelling_issues.append(f\"Invalid variant: {original} -> {normalized}\")\n",
    "        \n",
    "        # Validate variant quality\n",
    "        variant_rate = len(variants) / len(self.tokens_df) if not self.tokens_df.empty else 0\n",
    "        \n",
    "        if variant_rate < 0.05:  # Less than 5% variants seems low for historical texts\n",
    "            self.warnings.append(f\"Low spelling variant rate ({variant_rate:.2%}) - expected higher for historical texts\")\n",
    "        \n",
    "        # Test specific known changes\n",
    "        self.test_known_spelling_changes(variants)\n",
    "        \n",
    "        self.validation_results['spelling_variants'] = {\n",
    "            'total_variants': len(variants),\n",
    "            'valid_variants': valid_variants,\n",
    "            'archaic_patterns': archaic_patterns,\n",
    "            'variant_rate': variant_rate,\n",
    "            'issues': spelling_issues\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Total variants: {len(variants)}\")\n",
    "        print(f\"   ✓ Valid variants: {valid_variants}\")\n",
    "        print(f\"   ✓ Variant rate: {variant_rate:.2%}\")\n",
    "        print(f\"   ✓ Archaic patterns found: {archaic_patterns}\")\n",
    "    \n",
    "    def has_archaic_patterns(self, word):\n",
    "        \"\"\"Check for known archaic German patterns\"\"\"\n",
    "        archaic_patterns = [\n",
    "            r'.*th.*',      # thun -> tun\n",
    "            r'.*uo.*',      # guot -> gut\n",
    "            r'.*ey.*',      # archaic diphthongs\n",
    "            r'.*ck$',       # archaic endings\n",
    "            r'^v[aeiou]',   # vmb -> um\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, word.lower()) for pattern in archaic_patterns)\n",
    "    \n",
    "    def test_known_spelling_changes(self, variants):\n",
    "        \"\"\"Test for specific known German spelling changes\"\"\"\n",
    "        print(\"   📝 Testing known spelling changes...\")\n",
    "        \n",
    "        known_changes = {\n",
    "            'th_to_t': (r'.*th.*', r'.*t.*'),      # thun -> tun\n",
    "            'uo_to_u': (r'.*uo.*', r'.*u.*'),      # guot -> gut  \n",
    "            'v_to_u': (r'^v.*', r'^u.*'),          # vmb -> um\n",
    "            'ck_changes': (r'.*ck.*', r'.*k.*'),   # various ck changes\n",
    "        }\n",
    "        \n",
    "        found_changes = defaultdict(int)\n",
    "        \n",
    "        for variant in variants:\n",
    "            original = variant.get('original', '').lower()\n",
    "            normalized = variant.get('normalized', '').lower()\n",
    "            \n",
    "            for change_type, (old_pattern, new_pattern) in known_changes.items():\n",
    "                if re.search(old_pattern, original) and re.search(new_pattern, normalized):\n",
    "                    found_changes[change_type] += 1\n",
    "        \n",
    "        for change_type, count in found_changes.items():\n",
    "            print(f\"      - {change_type}: {count} examples\")\n",
    "        \n",
    "        if not found_changes:\n",
    "            self.warnings.append(\"No known spelling change patterns detected\")\n",
    "    \n",
    "    def validate_linguistic_features(self):\n",
    "        \"\"\"Validate POS tags and morphological features\"\"\"\n",
    "        print(\"\\n🏷️ Validating Linguistic Features...\")\n",
    "        \n",
    "        linguistic_issues = []\n",
    "        \n",
    "        # Check POS tag distribution\n",
    "        pos_tags = self.tokens_df['pos'].value_counts() if 'pos' in self.tokens_df.columns else pd.Series()\n",
    "        \n",
    "        # Expected major POS categories for German\n",
    "        expected_pos = ['NN', 'ART', 'VVFIN', 'ADV', 'ADJA', 'APPR', 'PRON']\n",
    "        found_major_pos = 0\n",
    "        \n",
    "        if not pos_tags.empty:\n",
    "            found_major_pos = sum(1 for pos in expected_pos if any(pos in tag for tag in pos_tags.index))\n",
    "        \n",
    "        if found_major_pos < 4:\n",
    "            self.warnings.append(\"Few major POS categories found - check POS tagging\")\n",
    "        \n",
    "        # Check morphological features\n",
    "        morph_features = self.tokens_df['morphology'].dropna() if 'morphology' in self.tokens_df.columns else pd.Series()\n",
    "        \n",
    "        self.validation_results['linguistic_features'] = {\n",
    "            'pos_tag_count': len(pos_tags),\n",
    "            'major_pos_found': found_major_pos,\n",
    "            'morph_features_present': len(morph_features) > 0,\n",
    "            'pos_distribution': dict(pos_tags.head(10)) if not pos_tags.empty else {},\n",
    "            'issues': linguistic_issues\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ POS tags found: {len(pos_tags)}\")\n",
    "        print(f\"   ✓ Major POS categories: {found_major_pos}/7\")\n",
    "        print(f\"   ✓ Morphological features: {'Yes' if len(morph_features) > 0 else 'No'}\")\n",
    "    \n",
    "    def validate_text_quality(self):\n",
    "        \"\"\"Validate text extraction quality\"\"\"\n",
    "        print(\"\\n📝 Validating Text Quality...\")\n",
    "        \n",
    "        quality_issues = []\n",
    "        avg_token_length = 0\n",
    "        \n",
    "        # Check for empty or very short texts\n",
    "        short_docs = 0\n",
    "        empty_docs = 0\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            token_count = doc.get('token_count', 0)\n",
    "            if token_count == 0:\n",
    "                empty_docs += 1\n",
    "            elif token_count < 50:  # Very short documents\n",
    "                short_docs += 1\n",
    "        \n",
    "        # Check for reasonable sentence lengths\n",
    "        if not self.tokens_df.empty:\n",
    "            avg_token_length = self.tokens_df['word_length'].mean() if 'word_length' in self.tokens_df.columns else 0\n",
    "            \n",
    "            if avg_token_length > 0 and (avg_token_length < 3 or avg_token_length > 15):\n",
    "                quality_issues.append(f\"Unusual average word length: {avg_token_length:.1f}\")\n",
    "        \n",
    "        self.validation_results['text_quality'] = {\n",
    "            'empty_documents': empty_docs,\n",
    "            'short_documents': short_docs,\n",
    "            'average_token_length': avg_token_length,\n",
    "            'issues': quality_issues\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Empty documents: {empty_docs}\")\n",
    "        print(f\"   ✓ Short documents: {short_docs}\")\n",
    "        if avg_token_length > 0:\n",
    "            print(f\"   ✓ Average token length: {avg_token_length:.1f}\")\n",
    "        else:\n",
    "            print(\"   ⚠ No word length data available\")\n",
    "    \n",
    "    def validate_data_completeness(self):\n",
    "        \"\"\"Validate data completeness for RAG pipeline\"\"\"\n",
    "        print(\"\\n📊 Validating Data Completeness...\")\n",
    "        \n",
    "        completeness_issues = []\n",
    "        \n",
    "        # Check essential fields\n",
    "        essential_fields = ['doc_id', 'period', 'genre', 'original', 'normalized']\n",
    "        missing_fields = []\n",
    "        \n",
    "        if self.tokens_df.empty:\n",
    "            self.critical_errors.append(\"No token data found!\")\n",
    "            return\n",
    "        \n",
    "        for field in essential_fields:\n",
    "            if field not in self.tokens_df.columns:\n",
    "                missing_fields.append(field)\n",
    "            elif self.tokens_df[field].isna().sum() > len(self.tokens_df) * 0.1:  # >10% missing\n",
    "                completeness_issues.append(f\"Many missing values in {field}\")\n",
    "        \n",
    "        if missing_fields:\n",
    "            self.critical_errors.append(f\"Missing essential fields: {missing_fields}\")\n",
    "        \n",
    "        # Check genre distribution\n",
    "        genres = self.tokens_df['genre'].value_counts() if 'genre' in self.tokens_df.columns else pd.Series()\n",
    "        \n",
    "        self.validation_results['completeness'] = {\n",
    "            'total_tokens': len(self.tokens_df),\n",
    "            'missing_fields': missing_fields,\n",
    "            'genre_distribution': dict(genres) if not genres.empty else {},\n",
    "            'issues': completeness_issues\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Total tokens: {len(self.tokens_df)}\")\n",
    "        print(f\"   ✓ Missing essential fields: {missing_fields}\")\n",
    "        print(f\"   ✓ Genres found: {list(genres.keys()) if not genres.empty else 'None'}\")\n",
    "    \n",
    "    def validate_rag_readiness(self):\n",
    "        \"\"\"Validate data is ready for RAG pipeline\"\"\"\n",
    "        print(\"\\n🤖 Validating RAG Readiness...\")\n",
    "        \n",
    "        rag_issues = []\n",
    "        readiness_score = 0\n",
    "        \n",
    "        # Check 1: Temporal evolution data\n",
    "        if len(self.validation_results.get('temporal', {}).get('periods_found', [])) >= 2:\n",
    "            readiness_score += 20\n",
    "        else:\n",
    "            rag_issues.append(\"Need multiple time periods for evolution tracking\")\n",
    "        \n",
    "        # Check 2: Spelling variants\n",
    "        if self.validation_results.get('spelling_variants', {}).get('total_variants', 0) > 100:\n",
    "            readiness_score += 25\n",
    "        else:\n",
    "            rag_issues.append(\"Need more spelling variants for language change analysis\")\n",
    "        \n",
    "        # Check 3: Sufficient data volume\n",
    "        if len(self.tokens_df) > 10000:\n",
    "            readiness_score += 20\n",
    "        else:\n",
    "            rag_issues.append(\"Need more tokens for robust analysis\")\n",
    "        \n",
    "        # Check 4: Linguistic features\n",
    "        if self.validation_results.get('linguistic_features', {}).get('pos_tag_count', 0) > 10:\n",
    "            readiness_score += 15\n",
    "        else:\n",
    "            rag_issues.append(\"Need more linguistic features\")\n",
    "        \n",
    "        # Check 5: Data quality\n",
    "        if self.validation_results.get('text_quality', {}).get('empty_documents', 0) < len(self.documents) * 0.1:\n",
    "            readiness_score += 20\n",
    "        else:\n",
    "            rag_issues.append(\"Too many empty/poor quality documents\")\n",
    "        \n",
    "        is_ready = readiness_score >= 80\n",
    "        \n",
    "        self.validation_results['rag_readiness'] = {\n",
    "            'readiness_score': readiness_score,\n",
    "            'is_ready': is_ready,\n",
    "            'issues': rag_issues\n",
    "        }\n",
    "        \n",
    "        print(f\"   📊 RAG Readiness Score: {readiness_score}/100\")\n",
    "        print(f\"   {'✅ READY for RAG pipeline' if is_ready else '❌ NOT READY - fix issues first'}\")\n",
    "    \n",
    "    def test_sample_queries(self):\n",
    "        \"\"\"Test sample queries that the RAG system should handle\"\"\"\n",
    "        print(\"\\n🔍 Testing Sample RAG Queries...\")\n",
    "        \n",
    "        sample_queries = [\n",
    "            \"How did 'thun' become 'tun'?\",\n",
    "            \"When did German spelling change?\",\n",
    "            \"What words changed between 1600-1800?\",\n",
    "            \"Show evolution of verb endings\",\n",
    "            \"Compare religious vs scientific language\"\n",
    "        ]\n",
    "        \n",
    "        query_results = {}\n",
    "        \n",
    "        for query in sample_queries:\n",
    "            # Simulate query processing\n",
    "            relevant_data = self.simulate_query_processing(query)\n",
    "            query_results[query] = relevant_data\n",
    "            \n",
    "            if relevant_data['can_answer']:\n",
    "                print(f\"   ✅ '{query}' - Can answer with {relevant_data['evidence_count']} examples\")\n",
    "            else:\n",
    "                print(f\"   ❌ '{query}' - Insufficient data\")\n",
    "                self.warnings.append(f\"Cannot answer query: {query}\")\n",
    "        \n",
    "        self.validation_results['sample_queries'] = query_results\n",
    "    \n",
    "    def simulate_query_processing(self, query):\n",
    "        \"\"\"Simulate how well we can answer a query with current data\"\"\"\n",
    "        # Simple simulation - check if we have relevant data\n",
    "        \n",
    "        can_answer = True\n",
    "        evidence_count = 0\n",
    "        \n",
    "        # Check for spelling evolution queries\n",
    "        if any(word in query.lower() for word in ['thun', 'tun', 'spelling', 'change']):\n",
    "            variants = self.linguistic_features.get('spelling_variants', [])\n",
    "            evidence_count = len([v for v in variants if 'th' in v.get('original', '').lower()])\n",
    "            can_answer = evidence_count > 5\n",
    "        \n",
    "        # Check for temporal queries\n",
    "        elif any(word in query.lower() for word in ['when', 'between', 'evolution']):\n",
    "            periods = self.validation_results.get('temporal', {}).get('periods_found', [])\n",
    "            can_answer = len(periods) >= 2\n",
    "            evidence_count = len(periods) * 100  # Estimate\n",
    "        \n",
    "        # Check for genre comparison queries\n",
    "        elif any(word in query.lower() for word in ['religious', 'scientific', 'compare']):\n",
    "            genres = self.validation_results.get('completeness', {}).get('genre_distribution', {})\n",
    "            can_answer = len(genres) >= 2\n",
    "            evidence_count = sum(genres.values()) if isinstance(genres, dict) and genres else 0\n",
    "        \n",
    "        else:\n",
    "            # General query\n",
    "            evidence_count = len(self.tokens_df) // 10\n",
    "            can_answer = len(self.tokens_df) > 1000\n",
    "        \n",
    "        return {\n",
    "            'can_answer': can_answer,\n",
    "            'evidence_count': evidence_count\n",
    "        }\n",
    "    \n",
    "    def convert_to_json_serializable(self, obj):\n",
    "        \"\"\"Convert pandas types to JSON serializable types\"\"\"\n",
    "        if hasattr(obj, 'item'):  # pandas scalar\n",
    "            return obj.item()\n",
    "        elif hasattr(obj, 'tolist'):  # pandas array/series\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: self.convert_to_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_json_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def generate_validation_report(self):\n",
    "        \"\"\"Generate comprehensive validation report\"\"\"\n",
    "        print(\"\\n📋 Generating Validation Report...\")\n",
    "        \n",
    "        report_path = self.processed_dir / \"validation_report.json\"\n",
    "        \n",
    "        report = {\n",
    "            'validation_summary': {\n",
    "                'critical_errors': len(self.critical_errors),\n",
    "                'warnings': len(self.warnings),\n",
    "                'overall_status': 'READY' if not self.critical_errors else 'NEEDS_FIXES'\n",
    "            },\n",
    "            'critical_errors': self.critical_errors,\n",
    "            'warnings': self.warnings,\n",
    "            'detailed_results': self.validation_results,\n",
    "            'recommendations': self.generate_recommendations()\n",
    "        }\n",
    "        \n",
    "        # Convert pandas types to JSON serializable\n",
    "        report = self.convert_to_json_serializable(report)\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"   📄 Report saved to: {report_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎯 VALIDATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if self.critical_errors:\n",
    "            print(\"❌ CRITICAL ERRORS:\")\n",
    "            for error in self.critical_errors:\n",
    "                print(f\"   • {error}\")\n",
    "        \n",
    "        if self.warnings:\n",
    "            print(\"\\n⚠️ WARNINGS:\")\n",
    "            for warning in self.warnings:\n",
    "                print(f\"   • {warning}\")\n",
    "        \n",
    "        if not self.critical_errors:\n",
    "            print(\"✅ NO CRITICAL ERRORS - Data ready for RAG pipeline!\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Fix {len(self.critical_errors)} critical errors before proceeding\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate recommendations for fixing issues\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if self.critical_errors:\n",
    "            recommendations.append(\"Fix all critical errors before building RAG system\")\n",
    "        \n",
    "        if self.validation_results.get('spelling_variants', {}).get('total_variants', 0) < 100:\n",
    "            recommendations.append(\"Improve spelling variant extraction - check normalization rules\")\n",
    "        \n",
    "        if len(self.validation_results.get('temporal', {}).get('periods_found', [])) < 3:\n",
    "            recommendations.append(\"Add more temporal periods for better evolution tracking\")\n",
    "        \n",
    "        if self.validation_results.get('rag_readiness', {}).get('readiness_score', 0) < 80:\n",
    "            recommendations.append(\"Address RAG readiness issues before deployment\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run validation on processed GerManC data\"\"\"\n",
    "    processed_dir = \"/Users/rohan/Downloads/2544/processed_germanc\"  # Update path\n",
    "    \n",
    "    print(\"🔍 GerManC Preprocessing Validation\")\n",
    "    print(\"==================================\")\n",
    "    \n",
    "    validator = GerManCValidator(processed_dir)\n",
    "    results = validator.run_full_validation()\n",
    "    \n",
    "    print(\"\\n🎉 Validation Complete!\")\n",
    "    print(\"Check validation_report.json for detailed results\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c033013b-18d1-4381-ae13-07bf005700f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Loading data...\n",
      "✓ 774373 tokens, 336 documents\n",
      "🚀 Phase 2: PREPARE\n",
      "========================================\n",
      "\n",
      "🔄 Creating temporal chunks (size: 800)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|███████████████████| 336/336 [01:09<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 3607 chunks\n",
      "\n",
      "💾 Creating PostgreSQL tables...\n",
      "✓ PostgreSQL tables saved\n",
      "\n",
      "📊 Creating word frequencies table...\n",
      "✓ Word frequencies table saved\n",
      "\n",
      "📊 Creating features database...\n",
      "✓ Features database saved to /Users/rohan/Downloads/2544/prepare_output/linguistic_features_db.csv\n",
      "\n",
      "✅ PREPARE phase completed!\n",
      "📊 3607 chunks created\n",
      "📁 Output: /Users/rohan/Downloads/2544/prepare_output\n",
      "📋 Ready for Phase 3: ACCESS\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Phase 2: PREPARE - Pre-ACCESS Pipeline\n",
    "Transform GerManC tokens into structured temporal chunks ready for PostgreSQL\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Any\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GerManCPrepareProcessor:\n",
    "    def __init__(self, input_dir: str, output_dir: str):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.load_data()\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load processed GerManC data\"\"\"\n",
    "        print(\"📚 Loading data...\")\n",
    "        \n",
    "        self.tokens_df = pd.read_csv(self.input_dir / \"tokens.csv\")\n",
    "        \n",
    "        with open(self.input_dir / \"documents.json\", 'r', encoding='utf-8') as f:\n",
    "            self.documents = json.load(f)\n",
    "        \n",
    "        with open(self.input_dir / \"linguistic_features.json\", 'r', encoding='utf-8') as f:\n",
    "            self.linguistic_features = json.load(f)\n",
    "        \n",
    "        print(f\"✓ {len(self.tokens_df)} tokens, {len(self.documents)} documents\")\n",
    "    \n",
    "    def create_temporal_chunks(self, chunk_size: int = 800) -> List[Dict]:\n",
    "        \"\"\"Create temporal chunks from tokens\"\"\"\n",
    "        print(f\"\\n🔄 Creating temporal chunks (size: {chunk_size})...\")\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        for doc in tqdm(self.documents, desc=\"Processing documents\"):\n",
    "            doc_id = doc['doc_id']\n",
    "            doc_tokens = self.tokens_df[self.tokens_df['doc_id'] == doc_id]\n",
    "            \n",
    "            if len(doc_tokens) == 0:\n",
    "                continue\n",
    "            \n",
    "            doc_chunks = self.chunk_document(doc, doc_tokens, chunk_size)\n",
    "            chunks.extend(doc_chunks)\n",
    "        \n",
    "        print(f\"✓ Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def chunk_document(self, doc: Dict, tokens: pd.DataFrame, chunk_size: int) -> List[Dict]:\n",
    "        \"\"\"Split document into chunks\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        if 'position' in tokens.columns:\n",
    "            tokens = tokens.sort_values('position')\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for _, token in tokens.iterrows():\n",
    "            current_chunk.append(token)\n",
    "            current_size += 1\n",
    "            \n",
    "            if current_size >= chunk_size or self.is_sentence_end(token):\n",
    "                if len(current_chunk) > 50:\n",
    "                    chunk = self.create_chunk(doc, current_chunk)\n",
    "                    chunks.append(chunk)\n",
    "                \n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "        \n",
    "        if len(current_chunk) > 50:\n",
    "            chunk = self.create_chunk(doc, current_chunk)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def is_sentence_end(self, token) -> bool:\n",
    "        \"\"\"Check sentence boundary\"\"\"\n",
    "        original = str(token.get('original', ''))\n",
    "        return original.endswith(('.', '!', '?'))\n",
    "    \n",
    "    def create_chunk(self, doc: Dict, tokens: List) -> Dict:\n",
    "        \"\"\"Create chunk with metadata\"\"\"\n",
    "        original_text = ' '.join([str(token.get('original', '')) for token in tokens])\n",
    "        normalized_text = ' '.join([str(token.get('normalized', '')) for token in tokens])\n",
    "        \n",
    "        spelling_variants = self.extract_spelling_variants(tokens)\n",
    "        linguistic_features = self.calculate_linguistic_features(tokens)\n",
    "        \n",
    "        return {\n",
    "            'chunk_id': f\"{doc['doc_id']}_chunk_{len(tokens)}\",\n",
    "            'doc_id': doc['doc_id'],\n",
    "            'original_text': original_text,\n",
    "            'normalized_text': normalized_text,\n",
    "            'period': doc.get('period', 'unknown'),\n",
    "            'genre': doc.get('genre', 'unknown'),\n",
    "            'year': doc.get('year'),\n",
    "            'filename': doc.get('filename', ''),\n",
    "            'token_count': len(tokens),\n",
    "            'spelling_variants': spelling_variants,\n",
    "            'linguistic_features': linguistic_features\n",
    "        }\n",
    "    \n",
    "    def extract_spelling_variants(self, tokens: List) -> List[Dict]:\n",
    "        \"\"\"Extract spelling variants from tokens\"\"\"\n",
    "        variants = []\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            original = str(token.get('original', ''))\n",
    "            normalized = str(token.get('normalized', ''))\n",
    "            \n",
    "            if original != normalized and len(original) > 2:\n",
    "                variants.append({\n",
    "                    'original': original,\n",
    "                    'normalized': normalized,\n",
    "                    'pos': token.get('pos', ''),\n",
    "                    'position_in_chunk': i\n",
    "                })\n",
    "        \n",
    "        return variants\n",
    "    \n",
    "    def calculate_linguistic_features(self, tokens: List) -> Dict:\n",
    "        \"\"\"Calculate chunk-level linguistic features\"\"\"\n",
    "        pos_counts = Counter()\n",
    "        word_lengths = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            pos = token.get('pos', 'UNKNOWN')\n",
    "            pos_counts[pos] += 1\n",
    "            \n",
    "            original = str(token.get('original', ''))\n",
    "            if original:\n",
    "                word_lengths.append(len(original))\n",
    "        \n",
    "        return {\n",
    "            'pos_distribution': dict(pos_counts),\n",
    "            'avg_word_length': np.mean(word_lengths) if word_lengths else 0,\n",
    "            'total_tokens': len(tokens),\n",
    "            'unique_pos_count': len(pos_counts),\n",
    "            'spelling_variant_count': sum(1 for token in tokens \n",
    "                                        if str(token.get('original', '')) != str(token.get('normalized', '')))\n",
    "        }\n",
    "    \n",
    "    def create_postgresql_tables(self, chunks: List[Dict]):\n",
    "        \"\"\"Create PostgreSQL-ready data structures\"\"\"\n",
    "        print(\"\\n💾 Creating PostgreSQL tables...\")\n",
    "        \n",
    "        # Chunks table\n",
    "        chunks_data = []\n",
    "        for chunk in chunks:\n",
    "            chunks_data.append({\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'doc_id': chunk['doc_id'],\n",
    "                'period': chunk['period'],\n",
    "                'genre': chunk['genre'],\n",
    "                'year': chunk['year'],\n",
    "                'filename': chunk['filename'],\n",
    "                'normalized_text': chunk['normalized_text'],\n",
    "                'original_text': chunk['original_text'],\n",
    "                'token_count': chunk['token_count']\n",
    "            })\n",
    "        \n",
    "        chunks_df = pd.DataFrame(chunks_data)\n",
    "        chunks_path = self.output_dir / \"chunks_table.csv\"\n",
    "        chunks_df.to_csv(chunks_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Spelling variants table\n",
    "        variants_data = []\n",
    "        for chunk in chunks:\n",
    "            for variant in chunk['spelling_variants']:\n",
    "                variants_data.append({\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'period': chunk['period'],\n",
    "                    'genre': chunk['genre'],\n",
    "                    'original': variant['original'],\n",
    "                    'normalized': variant['normalized'],\n",
    "                    'pos': variant['pos'],\n",
    "                    'position_in_chunk': variant['position_in_chunk']\n",
    "                })\n",
    "        \n",
    "        if variants_data:\n",
    "            variants_df = pd.DataFrame(variants_data)\n",
    "            variants_path = self.output_dir / \"spelling_variants_table.csv\"\n",
    "            variants_df.to_csv(variants_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✓ PostgreSQL tables saved\")\n",
    "        return chunks_df\n",
    "    \n",
    "    def create_word_frequencies_table(self, chunks: List[Dict]):\n",
    "        \"\"\"Create word frequencies table for PostgreSQL\"\"\"\n",
    "        print(\"\\n📊 Creating word frequencies table...\")\n",
    "        \n",
    "        word_freq_data = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Count words in normalized text\n",
    "            words = chunk['normalized_text'].lower().split()\n",
    "            word_counts = Counter(words)\n",
    "            \n",
    "            for word, count in word_counts.items():\n",
    "                if len(word) > 2:  # Filter short words\n",
    "                    word_freq_data.append({\n",
    "                        'word': word,\n",
    "                        'period': chunk['period'],\n",
    "                        'genre': chunk['genre'],\n",
    "                        'frequency': count,\n",
    "                        'chunk_id': chunk['chunk_id']\n",
    "                    })\n",
    "        \n",
    "        if word_freq_data:\n",
    "            word_freq_df = pd.DataFrame(word_freq_data)\n",
    "            word_freq_path = self.output_dir / \"word_frequencies_table.csv\"\n",
    "            word_freq_df.to_csv(word_freq_path, index=False, encoding='utf-8')\n",
    "            print(f\"✓ Word frequencies table saved\")\n",
    "            return word_freq_df\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def create_linguistic_features_database(self, chunks: List[Dict]):\n",
    "        \"\"\"Create linguistic features database for PostgreSQL\"\"\"\n",
    "        print(\"\\n📊 Creating features database...\")\n",
    "        \n",
    "        features_data = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_id = chunk['chunk_id']\n",
    "            period = chunk['period']\n",
    "            genre = chunk['genre']\n",
    "            \n",
    "            # POS features\n",
    "            for pos, count in chunk['linguistic_features']['pos_distribution'].items():\n",
    "                features_data.append({\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'period': period,\n",
    "                    'genre': genre,\n",
    "                    'feature_type': 'pos',\n",
    "                    'feature_name': pos,\n",
    "                    'frequency': count,\n",
    "                    'relative_frequency': count / chunk['linguistic_features']['total_tokens']\n",
    "                })\n",
    "            \n",
    "            # Spelling variants\n",
    "            for variant in chunk['spelling_variants']:\n",
    "                features_data.append({\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'period': period,\n",
    "                    'genre': genre,\n",
    "                    'feature_type': 'spelling_variant',\n",
    "                    'feature_name': f\"{variant['original']}→{variant['normalized']}\",\n",
    "                    'frequency': 1,\n",
    "                    'relative_frequency': 1 / chunk['token_count']\n",
    "                })\n",
    "        \n",
    "        # Save as CSV for PostgreSQL import\n",
    "        features_df = pd.DataFrame(features_data)\n",
    "        features_path = self.output_dir / \"linguistic_features_db.csv\"\n",
    "        features_df.to_csv(features_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✓ Features database saved to {features_path}\")\n",
    "        return features_df\n",
    "    \n",
    "    def calculate_statistics(self, chunks: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate corpus statistics\"\"\"\n",
    "        periods = defaultdict(int)\n",
    "        genres = defaultdict(int)\n",
    "        token_counts = []\n",
    "        variant_counts = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            periods[chunk['period']] += 1\n",
    "            genres[chunk['genre']] += 1\n",
    "            token_counts.append(chunk['token_count'])\n",
    "            variant_counts.append(len(chunk['spelling_variants']))\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(chunks),\n",
    "            'period_distribution': dict(periods),\n",
    "            'genre_distribution': dict(genres),\n",
    "            'token_statistics': {\n",
    "                'mean': float(np.mean(token_counts)),\n",
    "                'median': float(np.median(token_counts)),\n",
    "                'total': int(np.sum(token_counts))\n",
    "            },\n",
    "            'variant_statistics': {\n",
    "                'mean_per_chunk': float(np.mean(variant_counts)),\n",
    "                'total_variants': int(np.sum(variant_counts))\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_prepare_phase(self, chunk_size: int = 800):\n",
    "        \"\"\"Execute complete PREPARE phase\"\"\"\n",
    "        print(\"🚀 Phase 2: PREPARE\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # 1. Create temporal chunks\n",
    "        chunks = self.create_temporal_chunks(chunk_size)\n",
    "        \n",
    "        # 2. Create PostgreSQL tables\n",
    "        chunks_df = self.create_postgresql_tables(chunks)\n",
    "        \n",
    "        # 3. Create word frequencies table\n",
    "        word_freq_df = self.create_word_frequencies_table(chunks)\n",
    "        \n",
    "        # 4. Create linguistic features database\n",
    "        features_df = self.create_linguistic_features_database(chunks)\n",
    "        \n",
    "        # 5. Save chunks as JSON\n",
    "        chunks_path = self.output_dir / \"temporal_chunks.json\"\n",
    "        with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        # 6. Calculate and save statistics\n",
    "        stats = self.calculate_statistics(chunks)\n",
    "        stats_path = self.output_dir / \"prepare_statistics.json\"\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n✅ PREPARE phase completed!\")\n",
    "        print(f\"📊 {len(chunks)} chunks created\")\n",
    "        print(f\"📁 Output: {self.output_dir}\")\n",
    "        print(f\"📋 Ready for Phase 3: ACCESS\")\n",
    "        \n",
    "        return chunks, features_df, stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run PREPARE phase\"\"\"\n",
    "    input_dir = \"/Users/rohan/Downloads/2544/processed_germanc\"\n",
    "    output_dir = \"/Users/rohan/Downloads/2544/prepare_output\"\n",
    "    \n",
    "    processor = GerManCPrepareProcessor(input_dir, output_dir)\n",
    "    chunks, features_df, stats = processor.run_prepare_phase()\n",
    "    \n",
    "    return chunks, features_df, stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunks, features_df, stats = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8cc753-f0cb-4c8d-9121-ef50cbf31cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗃️ Phase 3: ACCESS - Database Setup\n",
      "========================================\n",
      "📊 Creating database schema...\n",
      "✓ Database schema created\n",
      "📥 Importing data...\n",
      "✓ Imported 3607 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/default.py:1509: RuntimeWarning: coroutine 'Server.serve' was never awaited\n",
      "  key: (\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imported 48390 spelling variants\n",
      "✓ Imported 193019 word frequencies\n",
      "✓ Imported 131405 linguistic features\n",
      "🚀 Creating indexes...\n",
      "✓ Indexes created\n",
      "✅ Database setup completed\n",
      "\n",
      "✅ Phase 3: ACCESS completed!\n",
      "📊 Database ready with tables:\n",
      "  - chunks\n",
      "  - spelling_variants\n",
      "  - word_frequencies\n",
      "  - linguistic_features\n",
      "\n",
      "🔄 Ready for Phase 4: RAG\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Phase 3: ACCESS - Database Setup & Query Interface\n",
    "Import PREPARE data into PostgreSQL and create REST API\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from pathlib import Path\n",
    "import json\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "\n",
    "class AccessPhaseSetup:\n",
    "    def __init__(self, prepare_output_dir: str, db_config: Dict):\n",
    "        self.prepare_dir = Path(prepare_output_dir)\n",
    "        self.db_config = db_config\n",
    "        self.engine = self.create_db_connection()\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def create_db_connection(self):\n",
    "        \"\"\"Create SQLAlchemy engine\"\"\"\n",
    "        connection_string = f\"postgresql://{self.db_config['user']}:{self.db_config['password']}@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}\"\n",
    "        return create_engine(connection_string)\n",
    "    \n",
    "    def create_database_schema(self):\n",
    "        \"\"\"Create PostgreSQL tables\"\"\"\n",
    "        print(\"📊 Creating database schema...\")\n",
    "        \n",
    "        schema_sql = \"\"\"\n",
    "        -- Chunks table\n",
    "        CREATE TABLE IF NOT EXISTS chunks (\n",
    "            chunk_id VARCHAR(255) PRIMARY KEY,\n",
    "            doc_id VARCHAR(255),\n",
    "            period VARCHAR(10),\n",
    "            genre VARCHAR(100),\n",
    "            year INTEGER,\n",
    "            filename VARCHAR(255),\n",
    "            normalized_text TEXT,\n",
    "            original_text TEXT,\n",
    "            token_count INTEGER,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \n",
    "        -- Spelling variants table\n",
    "        CREATE TABLE IF NOT EXISTS spelling_variants (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            chunk_id VARCHAR(255) REFERENCES chunks(chunk_id),\n",
    "            period VARCHAR(10),\n",
    "            genre VARCHAR(100),\n",
    "            original VARCHAR(255),\n",
    "            normalized VARCHAR(255),\n",
    "            pos VARCHAR(50),\n",
    "            position_in_chunk INTEGER\n",
    "        );\n",
    "        \n",
    "        -- Word frequencies table\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            word VARCHAR(255),\n",
    "            period VARCHAR(10),\n",
    "            genre VARCHAR(100),\n",
    "            frequency INTEGER,\n",
    "            chunk_id VARCHAR(255) REFERENCES chunks(chunk_id)\n",
    "        );\n",
    "        \n",
    "        -- Linguistic features table\n",
    "        CREATE TABLE IF NOT EXISTS linguistic_features (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            chunk_id VARCHAR(255) REFERENCES chunks(chunk_id),\n",
    "            period VARCHAR(10),\n",
    "            genre VARCHAR(100),\n",
    "            feature_type VARCHAR(50),\n",
    "            feature_name VARCHAR(255),\n",
    "            frequency INTEGER,\n",
    "            relative_frequency FLOAT\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(schema_sql))\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"✓ Database schema created\")\n",
    "    \n",
    "    def create_indexes(self):\n",
    "        \"\"\"Create performance indexes\"\"\"\n",
    "        print(\"🚀 Creating indexes...\")\n",
    "        \n",
    "        indexes_sql = \"\"\"\n",
    "        -- Temporal indexes\n",
    "        CREATE INDEX IF NOT EXISTS idx_chunks_period ON chunks(period);\n",
    "        CREATE INDEX IF NOT EXISTS idx_chunks_genre ON chunks(period, genre);\n",
    "        CREATE INDEX IF NOT EXISTS idx_chunks_year ON chunks(year);\n",
    "        \n",
    "        -- Full-text search\n",
    "        CREATE INDEX IF NOT EXISTS idx_chunks_text_gin ON chunks USING gin(to_tsvector('german', normalized_text));\n",
    "        \n",
    "        -- Spelling variants indexes\n",
    "        CREATE INDEX IF NOT EXISTS idx_variants_period ON spelling_variants(period);\n",
    "        CREATE INDEX IF NOT EXISTS idx_variants_original ON spelling_variants(original);\n",
    "        CREATE INDEX IF NOT EXISTS idx_variants_normalized ON spelling_variants(normalized);\n",
    "        \n",
    "        -- Word frequency indexes\n",
    "        CREATE INDEX IF NOT EXISTS idx_word_freq_word ON word_frequencies(word);\n",
    "        CREATE INDEX IF NOT EXISTS idx_word_freq_period ON word_frequencies(word, period);\n",
    "        \n",
    "        -- Features indexes\n",
    "        CREATE INDEX IF NOT EXISTS idx_features_type ON linguistic_features(feature_type);\n",
    "        CREATE INDEX IF NOT EXISTS idx_features_period ON linguistic_features(period, feature_type);\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(indexes_sql))\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"✓ Indexes created\")\n",
    "    \n",
    "    def drop_tables_cascade(self):\n",
    "        \"\"\"Drop existing tables with CASCADE\"\"\"\n",
    "        drop_sql = \"\"\"\n",
    "        DROP TABLE IF EXISTS linguistic_features CASCADE;\n",
    "        DROP TABLE IF EXISTS word_frequencies CASCADE;\n",
    "        DROP TABLE IF EXISTS spelling_variants CASCADE;\n",
    "        DROP TABLE IF EXISTS chunks CASCADE;\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(drop_sql))\n",
    "            conn.commit()\n",
    "    \n",
    "    def import_data_tables(self):\n",
    "        \"\"\"Import CSV data from PREPARE phase\"\"\"\n",
    "        print(\"📥 Importing data...\")\n",
    "        \n",
    "        # Drop existing tables first\n",
    "        self.drop_tables_cascade()\n",
    "        \n",
    "        # Import chunks\n",
    "        chunks_path = self.prepare_dir / \"chunks_table.csv\"\n",
    "        if chunks_path.exists():\n",
    "            chunks_df = pd.read_csv(chunks_path)\n",
    "            chunks_df.to_sql('chunks', self.engine, if_exists='replace', index=False)\n",
    "            print(f\"✓ Imported {len(chunks_df)} chunks\")\n",
    "        \n",
    "        # Import spelling variants\n",
    "        variants_path = self.prepare_dir / \"spelling_variants_table.csv\"\n",
    "        if variants_path.exists():\n",
    "            variants_df = pd.read_csv(variants_path)\n",
    "            variants_df.to_sql('spelling_variants', self.engine, if_exists='replace', index=False)\n",
    "            print(f\"✓ Imported {len(variants_df)} spelling variants\")\n",
    "        \n",
    "        # Import word frequencies\n",
    "        word_freq_path = self.prepare_dir / \"word_frequencies_table.csv\"\n",
    "        if word_freq_path.exists():\n",
    "            word_freq_df = pd.read_csv(word_freq_path)\n",
    "            word_freq_df.to_sql('word_frequencies', self.engine, if_exists='replace', index=False)\n",
    "            print(f\"✓ Imported {len(word_freq_df)} word frequencies\")\n",
    "        \n",
    "        # Import linguistic features\n",
    "        features_path = self.prepare_dir / \"linguistic_features_db.csv\"\n",
    "        if features_path.exists():\n",
    "            features_df = pd.read_csv(features_path)\n",
    "            features_df.to_sql('linguistic_features', self.engine, if_exists='replace', index=False)\n",
    "            print(f\"✓ Imported {len(features_df)} linguistic features\")\n",
    "    \n",
    "    def setup_database(self):\n",
    "        \"\"\"Complete database setup\"\"\"\n",
    "        print(\"🗃️ Phase 3: ACCESS - Database Setup\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        self.create_database_schema()\n",
    "        self.import_data_tables()\n",
    "        self.create_indexes()\n",
    "        \n",
    "        print(\"✅ Database setup completed\")\n",
    "\n",
    "# API Models\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    period: Optional[str] = None\n",
    "    genre: Optional[str] = None\n",
    "    limit: Optional[int] = 100\n",
    "\n",
    "class EvolutionQuery(BaseModel):\n",
    "    word: str\n",
    "    start_period: str\n",
    "    end_period: str\n",
    "\n",
    "# FastAPI Application\n",
    "class GerManCAPI:\n",
    "    def __init__(self, db_config: Dict):\n",
    "        self.db_config = db_config\n",
    "        self.engine = create_engine(\n",
    "            f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "        )\n",
    "        self.app = FastAPI(title=\"GerManC Historical Linguistics API\", version=\"1.0.0\")\n",
    "        self.setup_routes()\n",
    "        self.setup_cors()\n",
    "    \n",
    "    def setup_cors(self):\n",
    "        \"\"\"Setup CORS middleware\"\"\"\n",
    "        self.app.add_middleware(\n",
    "            CORSMiddleware,\n",
    "            allow_origins=[\"*\"],\n",
    "            allow_credentials=True,\n",
    "            allow_methods=[\"*\"],\n",
    "            allow_headers=[\"*\"],\n",
    "        )\n",
    "    \n",
    "    def setup_routes(self):\n",
    "        \"\"\"Setup API routes\"\"\"\n",
    "        \n",
    "        @self.app.get(\"/\")\n",
    "        def root():\n",
    "            return {\"message\": \"GerManC Historical Linguistics API\", \"version\": \"1.0.0\"}\n",
    "        \n",
    "        @self.app.get(\"/evolution/{word}/{start_period}/{end_period}\")\n",
    "        def query_word_evolution(word: str, start_period: str, end_period: str):\n",
    "            \"\"\"Track word evolution across periods\"\"\"\n",
    "            try:\n",
    "                query = text(\"\"\"\n",
    "                    SELECT period, COUNT(*) as frequency, \n",
    "                           ARRAY_AGG(DISTINCT original) as variants\n",
    "                    FROM spelling_variants \n",
    "                    WHERE (original = :word OR normalized = :word)\n",
    "                    AND period BETWEEN :start_period AND :end_period\n",
    "                    GROUP BY period \n",
    "                    ORDER BY period\n",
    "                \"\"\")\n",
    "                \n",
    "                with self.engine.connect() as conn:\n",
    "                    result = conn.execute(query, {\n",
    "                        'word': word,\n",
    "                        'start_period': start_period,\n",
    "                        'end_period': end_period\n",
    "                    })\n",
    "                    \n",
    "                    evolution_data = []\n",
    "                    for row in result:\n",
    "                        evolution_data.append({\n",
    "                            'period': row.period,\n",
    "                            'frequency': row.frequency,\n",
    "                            'variants': row.variants\n",
    "                        })\n",
    "                    \n",
    "                    return {\n",
    "                        'word': word,\n",
    "                        'period_range': f\"{start_period}-{end_period}\",\n",
    "                        'evolution': evolution_data\n",
    "                    }\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "        \n",
    "        @self.app.post(\"/linguistic_analysis\")\n",
    "        def linguistic_analysis(request: QueryRequest):\n",
    "            \"\"\"Perform linguistic pattern analysis\"\"\"\n",
    "            try:\n",
    "                conditions = []\n",
    "                params = {}\n",
    "                \n",
    "                if request.period:\n",
    "                    conditions.append(\"period = :period\")\n",
    "                    params['period'] = request.period\n",
    "                \n",
    "                if request.genre:\n",
    "                    conditions.append(\"genre = :genre\")\n",
    "                    params['genre'] = request.genre\n",
    "                \n",
    "                where_clause = \" AND \".join(conditions) if conditions else \"1=1\"\n",
    "                \n",
    "                query = text(f\"\"\"\n",
    "                    SELECT feature_type, feature_name, \n",
    "                           SUM(frequency) as total_frequency,\n",
    "                           AVG(relative_frequency) as avg_relative_frequency\n",
    "                    FROM linguistic_features \n",
    "                    WHERE {where_clause}\n",
    "                    GROUP BY feature_type, feature_name\n",
    "                    ORDER BY total_frequency DESC\n",
    "                    LIMIT :limit\n",
    "                \"\"\")\n",
    "                \n",
    "                params['limit'] = request.limit\n",
    "                \n",
    "                with self.engine.connect() as conn:\n",
    "                    result = conn.execute(query, params)\n",
    "                    \n",
    "                    analysis_data = []\n",
    "                    for row in result:\n",
    "                        analysis_data.append({\n",
    "                            'feature_type': row.feature_type,\n",
    "                            'feature_name': row.feature_name,\n",
    "                            'total_frequency': row.total_frequency,\n",
    "                            'avg_relative_frequency': float(row.avg_relative_frequency)\n",
    "                        })\n",
    "                    \n",
    "                    return {\n",
    "                        'filters': {\n",
    "                            'period': request.period,\n",
    "                            'genre': request.genre\n",
    "                        },\n",
    "                        'results': analysis_data\n",
    "                    }\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "        \n",
    "        @self.app.get(\"/temporal_patterns\")\n",
    "        def temporal_patterns():\n",
    "            \"\"\"Get temporal distribution patterns\"\"\"\n",
    "            try:\n",
    "                query = text(\"\"\"\n",
    "                    SELECT period, genre, COUNT(*) as chunk_count,\n",
    "                           AVG(token_count) as avg_token_count\n",
    "                    FROM chunks\n",
    "                    GROUP BY period, genre\n",
    "                    ORDER BY period, genre\n",
    "                \"\"\")\n",
    "                \n",
    "                with self.engine.connect() as conn:\n",
    "                    result = conn.execute(query)\n",
    "                    \n",
    "                    patterns = []\n",
    "                    for row in result:\n",
    "                        patterns.append({\n",
    "                            'period': row.period,\n",
    "                            'genre': row.genre,\n",
    "                            'chunk_count': row.chunk_count,\n",
    "                            'avg_token_count': float(row.avg_token_count)\n",
    "                        })\n",
    "                    \n",
    "                    return {'temporal_patterns': patterns}\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "        \n",
    "        @self.app.get(\"/search/{query}\")\n",
    "        def full_text_search(query: str, period: Optional[str] = None, limit: int = 50):\n",
    "            \"\"\"Full-text search in historical texts\"\"\"\n",
    "            try:\n",
    "                conditions = [\"to_tsvector('german', normalized_text) @@ plainto_tsquery('german', :query)\"]\n",
    "                params = {'query': query, 'limit': limit}\n",
    "                \n",
    "                if period:\n",
    "                    conditions.append(\"period = :period\")\n",
    "                    params['period'] = period\n",
    "                \n",
    "                where_clause = \" AND \".join(conditions)\n",
    "                \n",
    "                search_query = text(f\"\"\"\n",
    "                    SELECT chunk_id, period, genre, \n",
    "                           ts_headline('german', normalized_text, plainto_tsquery('german', :query)) as highlighted_text,\n",
    "                           ts_rank(to_tsvector('german', normalized_text), plainto_tsquery('german', :query)) as rank\n",
    "                    FROM chunks\n",
    "                    WHERE {where_clause}\n",
    "                    ORDER BY rank DESC\n",
    "                    LIMIT :limit\n",
    "                \"\"\")\n",
    "                \n",
    "                with self.engine.connect() as conn:\n",
    "                    result = conn.execute(search_query, params)\n",
    "                    \n",
    "                    search_results = []\n",
    "                    for row in result:\n",
    "                        search_results.append({\n",
    "                            'chunk_id': row.chunk_id,\n",
    "                            'period': row.period,\n",
    "                            'genre': row.genre,\n",
    "                            'highlighted_text': row.highlighted_text,\n",
    "                            'relevance_score': float(row.rank)\n",
    "                        })\n",
    "                    \n",
    "                    return {\n",
    "                        'query': query,\n",
    "                        'period_filter': period,\n",
    "                        'results': search_results\n",
    "                    }\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    def run(self, host: str = \"127.0.0.1\", port: int = 8000):\n",
    "        \"\"\"Start API server\"\"\"\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        uvicorn.run(self.app, host=host, port=port)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run Phase 3: ACCESS setup\"\"\"\n",
    "    \n",
    "    # Database configuration\n",
    "    db_config = {\n",
    "        'host': 'localhost',\n",
    "        'port': 5432,\n",
    "        'database': 'germanc_corpus',\n",
    "        'user': 'rohan',  # Adjust as needed\n",
    "        'password': 1996  # Adjust as needed\n",
    "    }\n",
    "    \n",
    "    prepare_output_dir = \"/Users/rohan/Downloads/2544/prepare_output\"\n",
    "    \n",
    "    # Setup database\n",
    "    setup = AccessPhaseSetup(prepare_output_dir, db_config)\n",
    "    setup.setup_database()\n",
    "    \n",
    "    print(\"\\n✅ Phase 3: ACCESS completed!\")\n",
    "    print(\"📊 Database ready with tables:\")\n",
    "    print(\"  - chunks\")\n",
    "    print(\"  - spelling_variants\") \n",
    "    print(\"  - word_frequencies\")\n",
    "    print(\"  - linguistic_features\")\n",
    "    print(\"\\n🔄 Ready for Phase 4: RAG\")\n",
    "    \n",
    "    # Optional: Start API server (comment out if not needed)\n",
    "    \"\"\"\n",
    "    print(\"\\n🚀 Starting API server...\")\n",
    "    api = GerManCAPI(db_config)\n",
    "    \n",
    "    print(\"📡 API Endpoints:\")\n",
    "    print(\"  GET  /evolution/{word}/{start_period}/{end_period}\")\n",
    "    print(\"  POST /linguistic_analysis\")\n",
    "    print(\"  GET  /temporal_patterns\")\n",
    "    print(\"  GET  /search/{query}\")\n",
    "    print(f\"\\n🌐 Server running at: http://localhost:8000\")\n",
    "    print(\"📖 API docs at: http://localhost:8000/docs\")\n",
    "    \n",
    "    api.run()\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248f4ea5-4c01-47f9-9bcf-40eff0967144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface) (0.3.65)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface) (0.33.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.4)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.8.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.30.2->langchain-huggingface) (2025.4.26)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-huggingface) (0.14.0)\n",
      "Downloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c58ef35-155f-4b22-ab8f-b890526dc5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.13-cp39-abi3-macosx_10_12_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.11.0)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (2.8.2)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.0-cp312-cp312-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.73.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.23.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-macosx_10_12_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp312-cp312-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading chromadb-1.0.13-cp39-abi3-macosx_10_12_x86_64.whl (18.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading grpcio-1.73.0-cp312-cp312-macosx_11_0_universal2.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading mmh3-5.1.0-cp312-cp312-macosx_10_13_x86_64.whl (40 kB)\n",
      "Downloading onnxruntime-1.22.0-cp312-cp312-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "Downloading posthog-5.3.0-py3-none-any.whl (103 kB)\n",
      "Downloading pybase64-1.4.1-cp312-cp312-macosx_10_13_x86_64.whl (38 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading hf_xet-1.1.4-cp37-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp312-cp312-macosx_10_13_universal2.whl (200 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Downloading uvloop-0.21.0-cp312-cp312-macosx_10_13_x86_64.whl (821 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.3/821.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-macosx_10_12_x86_64.whl (402 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-macosx_10_13_x86_64.whl (173 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=cbead8676f7504c220572dd84525d67151fed9756e8340d832459fbc88de3547\n",
      "  Stored in directory: /Users/rohan/Library/Caches/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, websockets, uvloop, safetensors, rsa, pyproject_hooks, pybase64, protobuf, orjson, oauthlib, mmh3, importlib-resources, humanfriendly, httptools, hf-xet, grpcio, bcrypt, backoff, watchfiles, torch, requests-oauthlib, posthog, opentelemetry-proto, opentelemetry-api, huggingface-hub, googleapis-common-protos, google-auth, coloredlogs, build, tokenizers, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, langsmith, kubernetes, transformers, opentelemetry-sdk, langchain-core, sentence-transformers, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain, chromadb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chromadb-1.0.13 coloredlogs-15.0.1 durationpy-0.10 flatbuffers-25.2.10 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.73.0 hf-xet-1.1.4 httptools-0.6.4 huggingface-hub-0.33.0 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-33.1.0 langchain-0.3.25 langchain-core-0.3.65 langchain-text-splitters-0.3.8 langsmith-0.3.45 mmh3-5.1.0 oauthlib-3.3.1 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 orjson-3.10.18 posthog-5.3.0 protobuf-5.29.5 pybase64-1.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rsa-4.9.1 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 torch-2.2.2 transformers-4.52.4 uvloop-0.21.0 watchfiles-1.1.0 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers chromadb langchain transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba0ff097-592b-4116-9266-8dbc57c8cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 00:14:34,645 - INFO - Use pytorch device_name: mps\n",
      "2025-06-21 00:14:34,646 - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗃️ Phase 4: RAG Pipeline for German Historical Corpus\n",
      "============================================================\n",
      "🔄 Loading German sentence transformer model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 00:14:38,493 - INFO - Use pytorch device_name: mps\n",
      "2025-06-21 00:14:38,493 - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Loading chunks from PostgreSQL...\n",
      "✅ Loaded 1000 chunks from database\n",
      "📊 Periods: ['1650-1700']\n",
      "📊 Genres: ['Drama' 'Humanities' 'Legal' 'Narrative' 'Newspapers' 'Scientific']\n",
      "🧠 Creating embeddings for text chunks...\n",
      "⚠️  Removed 98 duplicate chunk IDs\n",
      "📦 Found existing collection with 902 documents\n",
      "🔄 Adding 902 new chunks\n",
      "Processing batch 1/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90f86c954f84fb596ec9698a4a3839f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3733b488fa47d2942d5c3f75eab015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b082c3e2b44b599fe0ddf99566c850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f590ca4b5b1493f88b54bbe6916f36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 5/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05f515d73204a35b1f794378646da36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 6/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d7451ea269424da14595cf1ede591c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 7/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4f864b8ade4b31a853f08c887ec5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 8/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1ca7d6dce04e099c76355fb6c48c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 9/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6527878e42742998f152b6c8d407f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 10/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3283e0f16e3e4333b49a89523b3feff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 11/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59418b08e784cb0a85b61a2ef746554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 12/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dadc94023d0430ea0caf9d500bfbba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 13/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291dbeee3e7c4548b436b8e254e8c440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 14/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4897ed5cba41a9af223523335f3986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 15/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b6d3e112e14d3e98a91aecbf12a7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 16/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c461ac7f37b8415ca097ad68c6be8541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 17/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3eb3305e364f64bc55c2da84a75111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 18/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57810c5a9d364a6fa8dc05efb552d05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 19/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75647793eba14c088a5e65fa0f72da01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 20/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e197c99155d4a0fb1e3ff98c4c5e080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 21/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e59b891fab42a998d4dde2f7db19c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 22/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17dfb2eca7441739746049518cb1cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 23/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0946e962084499ca627186265ff7988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 24/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cdc0102b72444c98bfa2a4e4aec964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 25/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ee2fd8dfad4586a475d107b4a28c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 26/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837211dd98aa41d7b33ae0e1e06a516b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 27/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bf1ad96df04517ac0246f6eeaff582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 28/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f8dd9696bf4837ad66754ca18e0f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 29/29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4688d151a241425aa85e5480d9db005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created embeddings for 902 chunks\n",
      "📊 Total documents in collection: 902\n",
      "🔗 Setting up LangChain vectorstore...\n",
      "✅ LangChain vectorstore ready\n",
      "🤖 Setting up QA chain with simple...\n",
      "⚠️  Using simple retrieval mode (no LLM generation)\n",
      "\n",
      "🧪 Testing RAG system...\n",
      "\n",
      "1. Semantic Search Test:\n",
      "   1. Period: 1650-1700\n",
      "      Text: Römischen Reichs Müntz-Ordnung / Anno Fünfzehen hundert Fünfzig neun auffgericht / und hernacher auf...\n",
      "\n",
      "   2. Period: 1650-1700\n",
      "      Text: DAs freimachen soll mit Vorwissen des BergVoigts geschehen / und soll der Freimacher mit ein oder zw...\n",
      "\n",
      "   3. Period: 1650-1700\n",
      "      Text: eine Pfeif von dem Stengel hellebori oder Christwurtz gemacht / Kuriere mit ihrem sono die lymphatic...\n",
      "\n",
      "\n",
      "2. Question Answering Test:\n",
      "Using simple retrieval mode (no LLM generation)\n",
      "   Question: Wie entwickelte sich die deutsche Sprache im Mittelalter?\n",
      "   Answer: [Chunk 1]: Römischen Reichs Müntz-Ordnung / Anno Fünfzehen hundert Fünfzig neun auffgericht / und hernacher auf etlichen Reichs-Tägen / sonderlich aber bei dem in Fünffzehenhundert ein und siebentsiec...\n",
      "\n",
      "\n",
      "3. Language Evolution Test:\n",
      "   Analyzing word: deutsch\n",
      "   1050-1350: 0 contexts found\n",
      "   1350-1650: 0 contexts found\n",
      "   1650-1800: 0 contexts found\n",
      "   1800-1900: 0 contexts found\n",
      "   1900-2000: 0 contexts found\n",
      "\n",
      "📊 System Statistics:\n",
      "   Total chunks in DB: 3607\n",
      "   Total embeddings: 902\n",
      "   Time periods: 3\n",
      "\n",
      "✅ Phase 4: RAG Pipeline completed successfully!\n",
      "\n",
      "🎯 Available capabilities:\n",
      "   - Semantic search across historical German texts\n",
      "   - Question answering about language evolution\n",
      "   - Period-specific analysis\n",
      "   - Language change tracking\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Phase 4: RAG Pipeline for German Historical Language Evolution\n",
    "================================================================\n",
    "\n",
    "This script builds a Retrieval-Augmented Generation (RAG) system that:\n",
    "1. Loads chunks from PostgreSQL database\n",
    "2. Creates embeddings using sentence-transformers\n",
    "3. Stores embeddings in ChromaDB vector database\n",
    "4. Provides semantic search and question-answering capabilities\n",
    "\n",
    "Requirements:\n",
    "- PostgreSQL with German corpus data (from Phase 3)\n",
    "- sentence-transformers, chromadb, langchain\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Database and vector store\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Embeddings and LLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from transformers import pipeline\n",
    "\n",
    "# LangChain components\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GermanRAGPipeline:\n",
    "    \"\"\"RAG Pipeline for German Historical Corpus\"\"\"\n",
    "    \n",
    "    def __init__(self, db_config: Dict[str, Any], vector_db_path: str = \"./chroma_db\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG Pipeline\n",
    "        \n",
    "        Args:\n",
    "            db_config: PostgreSQL database configuration\n",
    "            vector_db_path: Path to ChromaDB vector database\n",
    "        \"\"\"\n",
    "        self.db_config = db_config\n",
    "        self.vector_db_path = Path(vector_db_path)\n",
    "        self.vector_db_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Database connection\n",
    "        self.engine = create_engine(\n",
    "            f\"postgresql://{db_config['user']}:{db_config['password']}@\"\n",
    "            f\"{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "        )\n",
    "        \n",
    "        # Initialize embedding model (German-specific)\n",
    "        print(\"🔄 Loading German sentence transformer model...\")\n",
    "        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.chroma_client = chromadb.PersistentClient(path=str(self.vector_db_path))\n",
    "        self.collection_name = \"german_corpus_chunks\"\n",
    "        \n",
    "        # LangChain embeddings wrapper\n",
    "        self.langchain_embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "        \n",
    "    def load_chunks_from_db(self, limit: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load text chunks from PostgreSQL database\"\"\"\n",
    "        \n",
    "        print(\"📚 Loading chunks from PostgreSQL...\")\n",
    "        \n",
    "        # Based on the actual table structure:\n",
    "        # ['token_count', 'year', 'period', 'genre', 'chunk_id', 'filename', 'normalized_text', 'original_text', 'doc_id']\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            c.chunk_id,\n",
    "            c.normalized_text as text,\n",
    "            c.period,\n",
    "            c.token_count as word_count,\n",
    "            LENGTH(c.normalized_text) as char_count,\n",
    "            c.doc_id as document_id,\n",
    "            ROW_NUMBER() OVER (PARTITION BY c.doc_id ORDER BY c.chunk_id) as chunk_index,\n",
    "            c.year,\n",
    "            c.genre,\n",
    "            c.filename\n",
    "        FROM chunks c\n",
    "        WHERE c.normalized_text IS NOT NULL \n",
    "        AND LENGTH(c.normalized_text) > 50\n",
    "        ORDER BY c.period, c.doc_id, c.chunk_id\n",
    "        \"\"\"\n",
    "        \n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "            \n",
    "        with self.engine.connect() as conn:\n",
    "            df = pd.read_sql(query, conn)\n",
    "            \n",
    "        print(f\"✅ Loaded {len(df)} chunks from database\")\n",
    "        print(f\"📊 Periods: {df['period'].unique()}\")\n",
    "        print(f\"📊 Genres: {df['genre'].unique()}\")\n",
    "        return df\n",
    "    \n",
    "    def create_embeddings(self, chunks_df: pd.DataFrame, batch_size: int = 32) -> None:\n",
    "        \"\"\"Create embeddings for text chunks and store in ChromaDB\"\"\"\n",
    "        \n",
    "        print(\"🧠 Creating embeddings for text chunks...\")\n",
    "        \n",
    "        # Remove duplicates based on chunk_id\n",
    "        original_len = len(chunks_df)\n",
    "        chunks_df = chunks_df.drop_duplicates(subset=['chunk_id'], keep='first')\n",
    "        deduped_len = len(chunks_df)\n",
    "        \n",
    "        if original_len != deduped_len:\n",
    "            print(f\"⚠️  Removed {original_len - deduped_len} duplicate chunk IDs\")\n",
    "        \n",
    "        # Create or get collection\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(self.collection_name)\n",
    "            print(f\"📦 Found existing collection with {collection.count()} documents\")\n",
    "            \n",
    "            # Check if we need to add new chunks\n",
    "            existing_ids = set(collection.get()['ids'])\n",
    "            new_chunks = chunks_df[~chunks_df['chunk_id'].astype(str).isin(existing_ids)]\n",
    "            \n",
    "            if len(new_chunks) == 0:\n",
    "                print(\"✅ All chunks already embedded\")\n",
    "                return\n",
    "            else:\n",
    "                print(f\"🔄 Adding {len(new_chunks)} new chunks\")\n",
    "                chunks_df = new_chunks\n",
    "                \n",
    "        except Exception:\n",
    "            collection = self.chroma_client.create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"German historical corpus text chunks\"}\n",
    "            )\n",
    "            print(\"📦 Created new ChromaDB collection\")\n",
    "        \n",
    "        # Process chunks in batches\n",
    "        total_chunks = len(chunks_df)\n",
    "        \n",
    "        for i in range(0, total_chunks, batch_size):\n",
    "            batch = chunks_df.iloc[i:i+batch_size]\n",
    "            \n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(total_chunks-1)//batch_size + 1}\")\n",
    "            \n",
    "            # Prepare texts and metadata\n",
    "            texts = batch['text'].tolist()\n",
    "            \n",
    "            # Ensure unique IDs by adding row index if needed\n",
    "            chunk_ids = []\n",
    "            for idx, row in batch.iterrows():\n",
    "                base_id = str(row['chunk_id'])\n",
    "                # Add row index to ensure uniqueness\n",
    "                unique_id = f\"{base_id}_{idx}\"\n",
    "                chunk_ids.append(unique_id)\n",
    "            \n",
    "            # Create embeddings\n",
    "            embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadatas = []\n",
    "            for _, row in batch.iterrows():\n",
    "                metadata = {\n",
    "                    'period': str(row['period']),\n",
    "                    'document_id': str(row['document_id']),\n",
    "                    'chunk_index': int(row['chunk_index']),\n",
    "                    'word_count': int(row['word_count']),\n",
    "                    'char_count': int(row['char_count']),\n",
    "                    'year': str(row.get('year', '')),\n",
    "                    'genre': str(row.get('genre', '')),\n",
    "                    'filename': str(row.get('filename', '')),\n",
    "                    'original_chunk_id': str(row['chunk_id'])  # Keep original ID in metadata\n",
    "                }\n",
    "                metadatas.append(metadata)\n",
    "            \n",
    "            # Add to ChromaDB\n",
    "            collection.add(\n",
    "                embeddings=embeddings.tolist(),\n",
    "                documents=texts,\n",
    "                metadatas=metadatas,\n",
    "                ids=chunk_ids\n",
    "            )\n",
    "        \n",
    "        print(f\"✅ Created embeddings for {total_chunks} chunks\")\n",
    "        print(f\"📊 Total documents in collection: {collection.count()}\")\n",
    "    \n",
    "    def setup_langchain_vectorstore(self) -> None:\n",
    "        \"\"\"Setup LangChain Chroma vectorstore for QA\"\"\"\n",
    "        \n",
    "        print(\"🔗 Setting up LangChain vectorstore...\")\n",
    "        \n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=str(self.vector_db_path),\n",
    "            embedding_function=self.langchain_embeddings,\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "        \n",
    "        print(\"✅ LangChain vectorstore ready\")\n",
    "    \n",
    "    def setup_qa_chain(self, llm_provider: str = \"simple\") -> None:\n",
    "        \"\"\"Setup QA chain with LLM\"\"\"\n",
    "        \n",
    "        print(f\"🤖 Setting up QA chain with {llm_provider}...\")\n",
    "        \n",
    "        if llm_provider == \"openai\":\n",
    "            # OpenAI GPT (requires API key)\n",
    "            if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "                raise ValueError(\"OPENAI_API_KEY environment variable required\")\n",
    "            llm = OpenAI(temperature=0.1, max_tokens=1000)\n",
    "            \n",
    "        elif llm_provider == \"simple\":\n",
    "            # Simple text-based retrieval without LLM generation\n",
    "            print(\"⚠️  Using simple retrieval mode (no LLM generation)\")\n",
    "            self.qa_chain = None  # We'll handle this differently\n",
    "            return\n",
    "            \n",
    "        elif llm_provider == \"huggingface\":\n",
    "            # Try a smaller, safer model\n",
    "            from langchain_community.llms import HuggingFacePipeline\n",
    "            \n",
    "            try:\n",
    "                # Use a smaller model that should work\n",
    "                hf_pipeline = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=\"distilgpt2\",  # Smaller, more compatible model\n",
    "                    max_length=256,\n",
    "                    temperature=0.1,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=50256\n",
    "                )\n",
    "                llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  HuggingFace model loading failed: {e}\")\n",
    "                print(\"Falling back to simple retrieval mode\")\n",
    "                self.qa_chain = None\n",
    "                return\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM provider: {llm_provider}\")\n",
    "        \n",
    "        # Create retrieval QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        print(\"✅ QA chain ready\")\n",
    "    \n",
    "    def semantic_search(self, query: str, k: int = 5, period_filter: Optional[str] = None) -> List[Dict]:\n",
    "        \"\"\"Perform semantic search on the corpus\"\"\"\n",
    "        \n",
    "        collection = self.chroma_client.get_collection(self.collection_name)\n",
    "        \n",
    "        # Build where clause for filtering\n",
    "        where_clause = None\n",
    "        if period_filter:\n",
    "            where_clause = {\"period\": period_filter}\n",
    "        \n",
    "        # Perform search\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=k,\n",
    "            where=where_clause\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            result = {\n",
    "                'text': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'distance': results['distances'][0][i] if 'distances' in results else None,\n",
    "                'chunk_id': results['ids'][0][i]\n",
    "            }\n",
    "            formatted_results.append(result)\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def ask_question(self, question: str, period_filter: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Ask a question using the RAG system\"\"\"\n",
    "        \n",
    "        if self.qa_chain is None:\n",
    "            # Simple retrieval mode - return relevant chunks without LLM generation\n",
    "            print(\"Using simple retrieval mode (no LLM generation)\")\n",
    "            \n",
    "            # Perform semantic search\n",
    "            results = self.semantic_search(question, k=5, period_filter=period_filter)\n",
    "            \n",
    "            # Create a simple answer from the most relevant chunks\n",
    "            answer_parts = []\n",
    "            for i, result in enumerate(results[:3]):\n",
    "                answer_parts.append(f\"[Chunk {i+1}]: {result['text'][:200]}...\")\n",
    "            \n",
    "            simple_answer = \"\\n\\n\".join(answer_parts)\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': simple_answer,\n",
    "                'source_documents': [\n",
    "                    {\n",
    "                        'content': result['text'],\n",
    "                        'metadata': result['metadata']\n",
    "                    }\n",
    "                    for result in results\n",
    "                ],\n",
    "                'mode': 'simple_retrieval'\n",
    "            }\n",
    "        \n",
    "        # If period filter specified, create a filtered retriever\n",
    "        if period_filter:\n",
    "            filtered_retriever = self.vectorstore.as_retriever(\n",
    "                search_kwargs={\n",
    "                    \"k\": 5,\n",
    "                    \"filter\": {\"period\": period_filter}\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Create temporary QA chain with filtered retriever\n",
    "            from langchain.chains import RetrievalQA\n",
    "            temp_qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=self.qa_chain.combine_documents_chain.llm_chain.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=filtered_retriever,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            result = temp_qa_chain({\"query\": question})\n",
    "        else:\n",
    "            result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': result['result'],\n",
    "            'source_documents': [\n",
    "                {\n",
    "                    'content': doc.page_content,\n",
    "                    'metadata': doc.metadata\n",
    "                }\n",
    "                for doc in result['source_documents']\n",
    "            ],\n",
    "            'mode': 'llm_generation'\n",
    "        }\n",
    "    \n",
    "    def analyze_language_evolution(self, word: str, periods: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze how a word/concept evolved across time periods\"\"\"\n",
    "        \n",
    "        if not periods:\n",
    "            periods = ['1050-1350', '1350-1650', '1650-1800', '1800-1900', '1900-2000']\n",
    "        \n",
    "        evolution_analysis = {\n",
    "            'word': word,\n",
    "            'periods': {},\n",
    "            'summary': ''\n",
    "        }\n",
    "        \n",
    "        for period in periods:\n",
    "            # Search for word usage in specific period\n",
    "            search_query = f\"Verwendung des Wortes '{word}'\"\n",
    "            results = self.semantic_search(search_query, k=3, period_filter=period)\n",
    "            \n",
    "            evolution_analysis['periods'][period] = {\n",
    "                'examples': results,\n",
    "                'context_count': len(results)\n",
    "            }\n",
    "        \n",
    "        # Generate evolution summary\n",
    "        if self.qa_chain:\n",
    "            summary_question = f\"Wie hat sich das Wort '{word}' in der deutschen Sprache über die Zeit entwickelt?\"\n",
    "            summary = self.ask_question(summary_question)\n",
    "            evolution_analysis['summary'] = summary['answer']\n",
    "        else:\n",
    "            # Simple summary without LLM\n",
    "            total_contexts = sum(len(data['examples']) for data in evolution_analysis['periods'].values())\n",
    "            evolution_analysis['summary'] = f\"Found {total_contexts} contexts for '{word}' across {len(periods)} time periods.\"\n",
    "        \n",
    "        return evolution_analysis\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get corpus and vector database statistics\"\"\"\n",
    "        \n",
    "        # PostgreSQL stats\n",
    "        with self.engine.connect() as conn:\n",
    "            db_stats = {}\n",
    "            \n",
    "            # Chunk statistics (using correct column names)\n",
    "            chunk_stats = conn.execute(text(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_chunks,\n",
    "                    AVG(token_count) as avg_word_count,\n",
    "                    MIN(period) as earliest_period,\n",
    "                    MAX(period) as latest_period,\n",
    "                    COUNT(DISTINCT period) as period_count,\n",
    "                    COUNT(DISTINCT genre) as genre_count\n",
    "                FROM chunks\n",
    "            \"\"\")).fetchone()\n",
    "            \n",
    "            db_stats['chunks'] = dict(chunk_stats._mapping)\n",
    "            \n",
    "            # Period distribution\n",
    "            period_dist = conn.execute(text(\"\"\"\n",
    "                SELECT period, COUNT(*) as chunk_count\n",
    "                FROM chunks\n",
    "                GROUP BY period\n",
    "                ORDER BY period\n",
    "            \"\"\")).fetchall()\n",
    "            \n",
    "            db_stats['period_distribution'] = [dict(row._mapping) for row in period_dist]\n",
    "            \n",
    "            # Genre distribution\n",
    "            genre_dist = conn.execute(text(\"\"\"\n",
    "                SELECT genre, COUNT(*) as chunk_count\n",
    "                FROM chunks\n",
    "                GROUP BY genre\n",
    "                ORDER BY chunk_count DESC\n",
    "            \"\"\")).fetchall()\n",
    "            \n",
    "            db_stats['genre_distribution'] = [dict(row._mapping) for row in genre_dist]\n",
    "        \n",
    "        # ChromaDB stats\n",
    "        try:\n",
    "            collection = self.chroma_client.get_collection(self.collection_name)\n",
    "            vector_stats = {\n",
    "                'total_embeddings': collection.count(),\n",
    "                'collection_name': self.collection_name\n",
    "            }\n",
    "        except Exception:\n",
    "            vector_stats = {'total_embeddings': 0, 'collection_name': 'Not created'}\n",
    "        \n",
    "        return {\n",
    "            'database_stats': db_stats,\n",
    "            'vector_stats': vector_stats,\n",
    "            'model_info': {\n",
    "                'embedding_model': 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                'vector_db_path': str(self.vector_db_path)\n",
    "            }\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run Phase 4: RAG Pipeline setup\"\"\"\n",
    "    \n",
    "    print(\"🗃️ Phase 4: RAG Pipeline for German Historical Corpus\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Database configuration (adjust as needed)\n",
    "    db_config = {\n",
    "        'host': 'localhost',\n",
    "        'port': 5432,\n",
    "        'database': 'germanc_corpus',\n",
    "        'user': 'rohan',\n",
    "        'password': ''\n",
    "    }\n",
    "    \n",
    "    # Initialize RAG pipeline\n",
    "    rag = GermanRAGPipeline(db_config, vector_db_path=\"./german_corpus_vectordb\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load chunks from database\n",
    "        chunks_df = rag.load_chunks_from_db(limit=1000)  # Start with 1000 chunks for testing\n",
    "        \n",
    "        # Step 2: Create embeddings and store in ChromaDB\n",
    "        rag.create_embeddings(chunks_df)\n",
    "        \n",
    "        # Step 3: Setup LangChain components\n",
    "        rag.setup_langchain_vectorstore()\n",
    "        \n",
    "        # Step 4: Setup QA chain (using simple mode to avoid PyTorch issues)\n",
    "        rag.setup_qa_chain(llm_provider=\"simple\")\n",
    "        \n",
    "        # Step 5: Test the system\n",
    "        print(\"\\n🧪 Testing RAG system...\")\n",
    "        \n",
    "        # Test semantic search\n",
    "        print(\"\\n1. Semantic Search Test:\")\n",
    "        search_results = rag.semantic_search(\"deutsche Sprache mittelalter\", k=3)\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            print(f\"   {i}. Period: {result['metadata']['period']}\")\n",
    "            print(f\"      Text: {result['text'][:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        # Test question answering\n",
    "        print(\"\\n2. Question Answering Test:\")\n",
    "        qa_result = rag.ask_question(\"Wie entwickelte sich die deutsche Sprache im Mittelalter?\")\n",
    "        print(f\"   Question: {qa_result['question']}\")\n",
    "        print(f\"   Answer: {qa_result['answer'][:200]}...\")\n",
    "        print()\n",
    "        \n",
    "        # Test language evolution analysis\n",
    "        print(\"\\n3. Language Evolution Test:\")\n",
    "        evolution = rag.analyze_language_evolution(\"deutsch\")\n",
    "        print(f\"   Analyzing word: {evolution['word']}\")\n",
    "        for period, data in evolution['periods'].items():\n",
    "            print(f\"   {period}: {data['context_count']} contexts found\")\n",
    "        \n",
    "        # Show statistics\n",
    "        print(\"\\n📊 System Statistics:\")\n",
    "        stats = rag.get_statistics()\n",
    "        print(f\"   Total chunks in DB: {stats['database_stats']['chunks']['total_chunks']}\")\n",
    "        print(f\"   Total embeddings: {stats['vector_stats']['total_embeddings']}\")\n",
    "        print(f\"   Time periods: {stats['database_stats']['chunks']['period_count']}\")\n",
    "        \n",
    "        print(\"\\n✅ Phase 4: RAG Pipeline completed successfully!\")\n",
    "        print(\"\\n🎯 Available capabilities:\")\n",
    "        print(\"   - Semantic search across historical German texts\")\n",
    "        print(\"   - Question answering about language evolution\")  \n",
    "        print(\"   - Period-specific analysis\")\n",
    "        print(\"   - Language change tracking\")\n",
    "        \n",
    "        return rag\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in RAG pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages first:\n",
    "    \"\"\"\n",
    "    pip install sentence-transformers chromadb langchain langchain-community langchain-huggingface openai transformers torch\n",
    "    \"\"\"\n",
    "    \n",
    "    rag_system = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b72ef9-a497-4894-a167-20864d29febf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
